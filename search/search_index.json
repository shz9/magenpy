{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Modeling and Analysis of Statistical Genetics data in Python (<code>magenpy</code>)","text":"<p>This site contains documentation, tutorials, and examples for using the <code>magenpy</code> package for the purposes of  handling, harmonizing, and computing over genotype data to prepare them for downstream genetics analyses.  The <code>magenpy</code> package provides tools for:</p> <ul> <li>Reading and processing genotype data in <code>plink</code> BED format.</li> <li>Efficient LD matrix construction and storage in Zarr array format.</li> <li>Data structures for harmonizing various GWAS data sources.</li> <li>Includes parsers for commonly used GWAS summary statistics formats.</li> <li>Simulating polygenic traits (continuous and binary) using complex genetic architectures.<ul> <li>Multi-cohort simulation scenarios (beta)</li> <li>Simulations incorporating functional annotations in the genetic architecture (beta)</li> </ul> </li> <li>Interfaces for performing association testing on simulated and real phenotypes.</li> <li>Preliminary support for processing and integrating genomic annotations with other data sources.</li> </ul> <p>If you use <code>magenpy</code> in your research, please cite the following paper:</p> <p>Zabad, S., Gravel, S., &amp; Li, Y. (2023). Fast and accurate Bayesian polygenic risk modeling with variational inference.  The American Journal of Human Genetics, 110(5), 741\u2013761. https://doi.org/10.1016/j.ajhg.2023.03.009</p>"},{"location":"#helpful-links","title":"Helpful links","text":"<ul> <li>API Reference</li> <li>Installation</li> <li>Getting Started</li> <li>Features and Configurations</li> <li>Command Line Scripts</li> <li>Project homepage on <code>GitHub</code></li> <li>Sister package <code>viprs</code></li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions or issues, please feel free to open an issue  on the <code>GitHub</code> repository or contact us directly at:</p> <ul> <li>Shadi Zabad</li> <li>Yue Li</li> <li>Simon Gravel</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>If you use <code>magenpy</code> in your research, please cite the following paper(s):</p> <p>Zabad, S., Gravel, S., &amp; Li, Y. (2023). Fast and accurate Bayesian polygenic risk modeling with variational inference.  The American Journal of Human Genetics, 110(5), 741\u2013761. https://doi.org/10.1016/j.ajhg.2023.03.009</p>"},{"location":"citation/#bibtex-records","title":"BibTeX records","text":"<pre><code>@article{ZABAD2023741,\n    title = {Fast and accurate Bayesian polygenic risk modeling with variational inference},\n    journal = {The American Journal of Human Genetics},\n    volume = {110},\n    number = {5},\n    pages = {741-761},\n    year = {2023},\n    issn = {0002-9297},\n    doi = {https://doi.org/10.1016/j.ajhg.2023.03.009},\n    url = {https://www.sciencedirect.com/science/article/pii/S0002929723000939},\n    author = {Shadi Zabad and Simon Gravel and Yue Li}\n}\n</code></pre>"},{"location":"features/","title":"Features and Configurations","text":""},{"location":"features/#1-complex-trait-simulation","title":"(1) Complex trait simulation","text":"<p><code>magenpy</code> may be used for complex trait simulation employing a variety of different  genetic architectures and phenotype likelihoods. For example, to simulate a quantitative  trait with heritability set to 0.25 and where a random subset of 15% of the variants are causal,  you may invoke the following command:</p> <pre><code>import magenpy as mgp\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),  # Path to 1000G genotype data\n                               pi=[.85, .15],  # Proportion of non-causal and causal variants\n                               h2=0.25)  # Heritability\n# Export simulated phenotype to pandas dataframe:\ng_sim.to_phenotype_table()\n</code></pre> <pre><code>         FID      IID  phenotype\n 0    HG00096  HG00096  -2.185944\n 1    HG00097  HG00097  -1.664984\n 2    HG00099  HG00099  -0.208703\n 3    HG00100  HG00100   0.257040\n 4    HG00101  HG00101  -0.068826\n ..       ...      ...        ...\n 373  NA20815  NA20815  -1.770358\n 374  NA20818  NA20818   1.823890\n 375  NA20819  NA20819   0.835763\n 376  NA20826  NA20826  -0.029256\n 377  NA20828  NA20828  -0.088353\n\n [378 rows x 3 columns]\n</code></pre> <p>To simulate a binary, or case-control, trait, the interface is very similar. First,  you need to specify that the likelihood for the phenotype is binomial (<code>phenotype_likelihood='binomial'</code>), and then  specify the prevalence of the positive cases in the population. For example,  to simulate a case-control trait with heritability of 0.3 and prevalence of 8%, we can invoke the following  command:</p> <pre><code>import magenpy as mgp\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),\n                               phenotype_likelihood='binomial',\n                               prevalence=.08,\n                               h2=0.3)\ng_sim.simulate()\ng_sim.to_phenotype_table()\n</code></pre> <pre><code>          FID      IID  phenotype\n 0    HG00096  HG00096          0\n 1    HG00097  HG00097          0\n 2    HG00099  HG00099          0\n 3    HG00100  HG00100          0\n 4    HG00101  HG00101          0\n ..       ...      ...        ...\n 373  NA20815  NA20815          0\n 374  NA20818  NA20818          0\n 375  NA20819  NA20819          1\n 376  NA20826  NA20826          0\n 377  NA20828  NA20828          0\n\n [378 rows x 3 columns]\n</code></pre>"},{"location":"features/#2-genome-wide-association-testing-gwas","title":"(2) Genome-wide Association Testing (GWAS)","text":"<p><code>magenpy</code> is not a GWAS tool. However, we do support preliminary association  testing functionalities either via closed-form formulas for quantitative traits, or  by providing a <code>python</code> interface to third-party association testing tools, such as <code>plink</code>.  </p> <p>If you are conducting simple tests based on simulated data, an easy way to perform  association testing is to tell the simulator that you'd like to perform GWAS on the  simulated trait, with the <code>perform_gwas=True</code> flag:</p> <pre><code>import magenpy as mgp\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),\n                               pi=[.85, .15],\n                               h2=0.25)\ng_sim.simulate(perform_gwas=True)\n</code></pre> <p>Alternatively, you can conduct association testing on real or  simulated phenotypes using the <code>.perform_gwas()</code> method and exporting the summary statistics to a <code>pandas</code> dataframe with <code>.to_summary_statistics_table()</code>:</p> <pre><code>g_sim.perform_gwas()\ng_sim.to_summary_statistics_table()\n</code></pre> <pre><code>        CHR         SNP       POS A1 A2  ...    N      BETA         Z        SE      PVAL\n 0       22    rs131538  16871137  A  G  ...  378 -0.046662 -0.900937  0.051793  0.367622\n 1       22   rs9605903  17054720  C  T  ...  378  0.063977  1.235253  0.051793  0.216736\n 2       22   rs5746647  17057138  G  T  ...  378  0.057151  1.103454  0.051793  0.269830\n 3       22  rs16980739  17058616  T  C  ...  378 -0.091312 -1.763029  0.051793  0.077896\n 4       22   rs9605923  17065079  A  T  ...  378  0.069368  1.339338  0.051793  0.180461\n ...    ...         ...       ... .. ..  ...  ...       ...       ...       ...       ...\n 15933   22   rs8137951  51165664  A  G  ...  378  0.078817  1.521782  0.051793  0.128064\n 15934   22   rs2301584  51171497  A  G  ...  378  0.076377  1.474658  0.051793  0.140304\n 15935   22   rs3810648  51175626  G  A  ...  378 -0.001448 -0.027952  0.051793  0.977701\n 15936   22   rs2285395  51178090  A  G  ...  378 -0.019057 -0.367949  0.051793  0.712911\n 15937   22  rs28729663  51219006  A  G  ...  378  0.029667  0.572805  0.051793  0.566777\n\n [15938 rows x 11 columns]\n</code></pre> <p>If you wish to use <code>plink2</code> for association testing (highly recommended), ensure that  you tell <code>PhenotypeSimulator</code> (or any <code>GWADataLoader</code>-derived object) to use plink by explicitly  specifying the <code>backend</code> software that you wish to use:</p> <pre><code>import magenpy as mgp\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),\n                               backend='plink', # Set the backend\n                               pi=[.85, .15],\n                               h2=0.25)\ng_sim.simulate(perform_gwas=True)\ng_sim.cleanup() # Clean up temporary files\n</code></pre> <p>When using <code>plink</code>, we sometimes create temporary intermediate files to pass to the software. To clean up  the temporary directories and files, you can invoke the <code>.cleanup()</code> command.</p>"},{"location":"features/#3-calculating-ld-matrices","title":"(3) Calculating LD matrices","text":"<p>One of the main features of the <code>magenpy</code> package is an efficient interface for computing  and storing Linkage Disequilibrium (LD) matrices. LD matrices record the pairwise SNP-by-SNP  Pearson correlation coefficient. In general, LD matrices are computed for each chromosome separately  or may also be computed within LD blocks from, e.g. LDetect. For large autosomal chromosomes,  LD matrices can be huge and may require extra care from the user.</p> <p>In <code>magenpy</code>, LD matrices can be computed using either <code>xarray</code> or <code>plink</code>, depending on the  backend that the user specifies (see Section 5 below). In general, at this moment, we do not recommend using  <code>xarray</code> as a backend for large genotype matrices, as it is less efficient than <code>plink</code>. When using the default  <code>xarray</code> as a backend, we compute the full <code>X'X</code> (X-transpose-X) matrix first, store it on-disk in chunked  <code>Zarr</code> arrays and then perform all sparsification procedures afterwards. When using <code>plink</code> as a  backend, on the other hand, we only compute LD between variants that are generally in close proximity  along the chromosome, so it is generally more efficient. In the end, both will be transformed such that  the LD matrix is stored in sparse <code>Zarr</code> arrays.</p> <p>In either case, to compute an LD matrix using <code>magenpy</code>, you can invoke the <code>.compute_ld()</code> method  of all <code>GWADataLoader</code>-derived objects, as follows:</p> <pre><code># Using xarray:\nimport magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path())\ngdl.compute_ld(estimator='windowed',\n               output_dir='output/ldl/',\n               window_size=100)\ngdl.cleanup()\n</code></pre> <p>This creates a windowed LD matrix where we only measure the correlation between the focal SNP and the nearest 100 variants from either side. As stated above, the LD matrix will be stored on-disk and that is why we must  specify the output directory when we call <code>.compute_ld()</code>. To use <code>plink</code> to compute the LD matrix,  we can invoke a similar command:</p> <pre><code># Using plink:\nimport magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\ngdl.compute_ld(estimator='windowed',\n               output_dir='output/ld/',\n               cm_window_size=3.)\ngdl.cleanup()\n</code></pre> <p>In this case, we are computing a windowed LD matrix where we only measure the correlation between  SNPs that are at most 3 centi Morgan (cM) apart along the chromosome. For this small 1000G dataset, computing  the LD matrix takes about a minute. The LD matrices in Zarr format will be written to the path  specified in <code>output_dir</code>, so ensure that this argument is set to the desired directory. </p> <p>To facilitate working with LD matrices stored in <code>Zarr</code> format, we created a data structure in python called <code>LDMatrix</code>,  which acts as an intermediary and provides various features. For example, to compute LD scores  using this LD matrix, you can invoke the command <code>.compute_ld_scores()</code> on it:</p> <pre><code>gdl.ld[22].compute_ld_scores()\n</code></pre> <pre><code>array([1.60969673, 1.84471792, 1.59205322, ..., 3.3126724 , 3.42234106,\n       2.97252452])\n</code></pre> <p>You can also get a table that lists the properties of the SNPs included in the LD matrix:</p> <pre><code>gdl.ld[22].to_snp_table()\n</code></pre> <pre><code>        CHR         SNP       POS A1       MAF\n 0       22   rs9605903  17054720  C  0.260736\n 1       22   rs5746647  17057138  G  0.060327\n 2       22  rs16980739  17058616  T  0.131902\n 3       22   rs9605927  17067005  C  0.033742\n 4       22   rs5746664  17074622  A  0.066462\n ...    ...         ...       ... ..       ...\n 14880   22   rs8137951  51165664  A  0.284254\n 14881   22   rs2301584  51171497  A  0.183027\n 14882   22   rs3810648  51175626  G  0.065440\n 14883   22   rs2285395  51178090  A  0.061350\n 14884   22  rs28729663  51219006  A  0.159509\n\n [14885 rows x 5 columns]\n</code></pre>"},{"location":"features/#ld-estimators-and-their-properties","title":"LD estimators and their properties","text":"<p><code>magenpy</code> supports computing LD matrices using 4 different estimators that are commonly used  in statistical genetics applications.  For a more thorough description of the estimators and their properties, consult our manuscript  and the citations therein. The LD estimators are:</p> <p>1) <code>windowed</code> (recommended): The windowed estimator computes the pairwise correlation coefficient between SNPs that are      within a pre-defined distance along the chromosome from each other. In many statistical genetics applications, the     recommended distance is between 1 and 3 centi Morgan (cM). As of <code>magenpy&gt;=0.0.2</code>, now you can customize     the distance based on three criteria: (1) A window size based on the number neighboring variants, (2)     distance threshold in kilobases (kb), and (3) distance threshold in centi Morgan (cM). When defining the     boundaries for each SNP, <code>magenpy</code> takes the intersection of the boundaries defined by each window.</p> <pre><code>import magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\ngdl.compute_ld('windowed', \n               output_dir='output/ld/',\n               window_size=100, kb_window_size=1000, cm_window_size=2.)\ngdl.cleanup()\n</code></pre> <p>2) <code>block</code>: The block estimator estimates the pairwise correlation coefficient between  variants that are in the same LD block, as defined by, e.g. LDetect. Given an LD block file,     we can compute a block-based LD matrix as follows:</p> <pre><code>import magenpy as mgp\nld_block_url = \"https://bitbucket.org/nygcresearch/ldetect-data/raw/ac125e47bf7ff3e90be31f278a7b6a61daaba0dc/EUR/fourier_ls-all.bed\"\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\ngdl.compute_ld('block', \n               output_dir='output/ld/',\n               ld_blocks_file=ld_block_url)\ngdl.cleanup()\n</code></pre> <p>If you have the LD blocks file on your system, you can also pass the path to the file instead.</p> <p>3) <code>shrinkage</code>: For the shrinkage estimator, we shrink the entries of the LD matrix by a     quantity related to the distance between SNPs along the chromosome + some additional information     related to the sample from which the genetic map was estimated. In particular,     we need to specify the effective population size and the sample size used to     estimate the genetic map. Also, to make the matrix sparse, we often specify a threshold value     below which we consider the correlation to be zero. Here's an example for the 1000G sample:</p> <pre><code>import magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\ngdl.compute_ld('shrinkage', \n               output_dir='output/ld/',\n               genetic_map_ne=11400, # effective population size (Ne)\n               genetic_map_sample_size=183, # Sample size\n               threshold=1e-3) # The cutoff value\ngdl.cleanup()\n</code></pre> <p>4) <code>sample</code>: This estimator computes the pairwise correlation coefficient between all SNPs on     the same chromosome and thus results in a dense matrix. Thus, it is rarely used in practice and     we include it here for testing/debugging purposes mostly. To compute the sample LD matrix, you only need     to specify the correct estimator:</p> <pre><code>import magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\ngdl.compute_ld('sample', output_dir='output/ld/')\ngdl.cleanup()\n</code></pre>"},{"location":"features/#4-data-harmonization","title":"(4) Data harmonization","text":"<p>There are many different statistical genetics data sources and formats out there. One of the goals of  <code>magenpy</code> is to create a friendly interface for matching and merging these data sources for  downstream analyses. For example, for summary statistics-based methods, we often need  to merge the LD matrix derived from a reference panel with the GWAS summary statistics estimated  in a different cohort. While this is a simple task, it can be tricky sometimes, e.g. in  cases where the effect allele is flipped between the two cohort.</p> <p>The functionalities that we provide for this are minimal at this stage and mainly geared towards  harmonizing <code>Zarr</code>-formatted LD matrices with GWAS summary statistics. The following example  shows how to do this in a simple case:</p> <pre><code>import magenpy as mgp\n# First, generate some summary statistics from a simulation:\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path())\ng_sim.simulate()\ng_sim.to_summary_statistics_table().to_csv(\n    \"chr_22.sumstats\", sep=\"\\t\", index=False\n)\n# Then load those summary statistics and match them with previously\n# computed windowed LD matrix for chromosome 22:\ngdl = mgp.GWADataLoader(ld_store_files='output/windowed_ld/chr_22/',\n                        sumstats_files='chr_22.sumstats',\n                        sumstats_format='magenpy')\n</code></pre> <p>Here, the <code>GWADataLoader</code> object takes care of the harmonization step by  automatically invoking the <code>.harmonize_data()</code> method. When you read or update  any of the data sources, we recommend that you invoke the <code>.harmonize_data()</code> method again  to make sure that all the data sources are aligned properly. In the near future,  we are planning to add many other functionalities in this space. Stay tuned.</p>"},{"location":"features/#5-using-plink-as-backend","title":"(5) Using <code>plink</code> as backend","text":"<p>Many of the functionalities that <code>magenpy</code> supports require access to and performing linear algebra  operations on the genotype matrix. By default, <code>magenpy</code> uses <code>xarray</code> and <code>dask</code>  to carry out these operations, as these are the tools supported by our main dependency: <code>pandas-plink</code>.</p> <p>However, <code>dask</code> can be quite slow and inefficient when deployed on large-scale genotype matrices. To get  around this difficulty, for many operations, such as linear scoring or computing minor allele frequency,  we support (and recommend) using <code>plink</code> as a backend.</p> <p>To use <code>plink</code> as a backend for <code>magenpy</code>, first you may need to configure the paths  on your system. By default, <code>magenpy</code> assumes that, in the shell, the name <code>plink2</code> invokes the <code>plink2</code>  executable and <code>plink</code> invokes <code>plink1.9</code> software. To change this behavior, you can update the  configuration file as follows. First, let's see the default configurations that ship with <code>magenpy</code>:</p> <pre><code>import magenpy as mgp\nmgp.print_options()\n</code></pre> <pre><code>-&gt; Section: DEFAULT\n---&gt; plink1.9_path: plink\n---&gt; plink2_path: plink2\n</code></pre> <p>The above shows the default configurations for the <code>plink1.9</code> and <code>plink2</code> paths. To change  the path for <code>plink2</code>, for example, you can use the <code>set_option()</code> function:</p> <pre><code>mgp.set_option(\"plink2_path\", \"~/software/plink2/plink2\")\nmgp.print_options()\n</code></pre> <pre><code>-&gt; Section: USER\n---&gt; plink2_path: ~/software/plink2/plink2\n---&gt; plink1.9_path: plink\n-&gt; Section: DEFAULT\n---&gt; plink1.9_path: plink\n---&gt; plink2_path: plink2\n</code></pre> <p>As you can see, this added a new section to the configuration file, named <code>USER</code>, that has the  new path for the <code>plink2</code> software. Now, every time <code>magenpy</code> needs to invoke <code>plink2</code>, it calls  the executable stored at <code>~/software/plink2/</code>. Note that you only need to do this once on any particular  machine or system, as this preference is now recorded in the configuration file and will be taken into  account for all future operations.</p> <p>Note that for most of the operations, we assume that the user has <code>plink2</code> installed. We only  use <code>plink1.9</code> for some operations that are currently not supported by <code>plink2</code>, especially for  e.g. LD computation. This behavior may change in the near future.</p> <p>Once the paths are configured, to use <code>plink</code> as a backend for the various computations and  tools, make sure that you specify the <code>backend='plink'</code> flag in <code>GWADataLoader</code> and all of its  derived data structures (including all the <code>PhenotypeSimulator</code> classes):</p> <pre><code>import magenpy as mgp\ngdl = mgp.GWADataLoader(mgp.tgp_eur_data_path(),\n                        backend='plink')\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p><code>magenpy</code> is a <code>python</code> package that aims to streamline working with statistical genetics data  in order to facilitate downstream analyses. The package comes with a sample dataset from the 1000G project that  you can use to experiment and familiarize yourself with its features.  Once the package is installed, you can run a couple of quick tests  to verify that the main features are working properly.</p> <p>For example, to simulate a quantitative trait, you can invoke  the following commands in a <code>python</code> interpreter:</p> <pre><code>import magenpy as mgp\ng_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),  # Provide path to 1000G data\n                               h2=0.1) # Heritability set to 0.1\ng_sim.simulate() # Simulate the phenotype\ng_sim.to_phenotype_table()\n</code></pre> <pre><code>          FID      IID  phenotype\n 0    HG00096  HG00096   0.795651\n 1    HG00097  HG00097   0.550914\n 2    HG00099  HG00099  -0.928486\n 3    HG00100  HG00100   0.893626\n 4    HG00101  HG00101  -0.670106\n ..       ...      ...        ...\n 373  NA20815  NA20815   0.246071\n 374  NA20818  NA20818   1.821426\n 375  NA20819  NA20819  -0.457994\n 376  NA20826  NA20826   0.954208\n 377  NA20828  NA20828   0.088412\n\n [378 rows x 3 columns]\n</code></pre> <p>This simulates a quantitative trait with heritability set to 0.1,  using genotype data for a subset of 378 individuals of European ancestry  from the 1000G project and approximately 15,000 SNPs on chromosome 22.  By default, the simulator assumes that only 10% of the SNPs are  causal (this is drawn at random from a Bernoulli distribution with <code>p=0.1</code>). To obtain a list of the causal SNPs in this simulation, you can invoke the  <code>.get_causal_status()</code> method, which returns a boolean vector indicating  whether each SNP is causal or not:</p> <pre><code>g_sim.get_causal_status()\n</code></pre> <pre><code>{22: array([ True, False, False, ..., False, False, False])}\n</code></pre> <p>In this case, for example, the first SNP is causal for the simulated phenotype. A note  about the design of data structures in <code>magenpy</code>. Our main data structure is a class known  as <code>GWADataLoader</code>, which is an all-purpose object that brings together different data sources and  harmonizes them together. In <code>GWADataLoader</code>, SNP-related data sources are stored in dictionaries, where  the key is the chromosome number and the value is the data structure associated with that chromosome.  Thus, in the output above, the data is for chromosome 22 and the feature is a boolean  vector indicating whether a given SNP is causal or not. </p> <p>You can also get the full information  about the genetic architecture by invoking the method <code>.to_true_beta_table()</code>, which returns a <code>pandas</code> dataframe with the effect size, expected heritability contribution,  and causal status of each variant in the simulation:</p> <pre><code>g_sim.to_true_beta_table()\n</code></pre> <pre><code>        CHR         SNP A1  MixtureComponent  Heritability      BETA  Causal\n 0       22    rs131538  A                 1      0.000063 -0.008013    True\n 1       22   rs9605903  C                 0      0.000000  0.000000   False\n 2       22   rs5746647  G                 0      0.000000  0.000000   False\n 3       22  rs16980739  T                 0      0.000000  0.000000   False\n 4       22   rs9605923  A                 0      0.000000  0.000000   False\n ...    ...         ... ..               ...           ...       ...     ...\n 15933   22   rs8137951  A                 0      0.000000  0.000000   False\n 15934   22   rs2301584  A                 0      0.000000  0.000000   False\n 15935   22   rs3810648  G                 0      0.000000  0.000000   False\n 15936   22   rs2285395  A                 0      0.000000  0.000000   False\n 15937   22  rs28729663  A                 0      0.000000  0.000000   False\n\n [15938 rows x 7 columns]\n</code></pre> <p>We can also simulate a more complex genetic architecture by, e.g. simulating effect sizes from  4-component sparse Gaussian mixture density, instead of the standard spike-and-slab density used by default:</p> <pre><code>g_sim = mgp.PhenotypeSimulator(mgp.tgp_eur_data_path(),\n                               pi=[.9, .03, .03, .04],  # Mixing proportions\n                               d=[0., .01, .1, 1.],  # Variance multipliers\n                               h2=0.1)\ng_sim.simulate()\ng_sim.to_phenotype_table()\n</code></pre> <pre><code>         FID      IID  phenotype\n 0    HG00096  HG00096   0.435024\n 1    HG00097  HG00097   1.030874\n 2    HG00099  HG00099   0.042322\n 3    HG00100  HG00100   1.392733\n 4    HG00101  HG00101   0.722763\n ..       ...      ...        ...\n 373  NA20815  NA20815  -0.402506\n 374  NA20818  NA20818  -0.321429\n 375  NA20819  NA20819  -0.845630\n 376  NA20826  NA20826  -0.690078\n 377  NA20828  NA20828   0.256937\n\n [378 rows x 3 columns]\n</code></pre> <p>The parameter <code>pi</code> specifies the mixing proportions for the Gaussian mixture  distribution and the <code>d</code> is a multiplier on the variance (see references below). In this case, 90% of the variants  are not causal, and the remaining 10% are divided between 3 mixture components that contribute  differentially to the heritability. The last component, which constitutes 4% of all SNPs, contributes 100  times and 10 times to the heritability than components 2 an 3, respectively.</p>"},{"location":"installation/","title":"Installation","text":"<p>The <code>magenpy</code> software is written in <code>Cython/Python3</code>. The software is designed to be used in a variety of computing environments, including local workstations,  shared computing environments, and cloud-based computing environments. Because of the dependencies on <code>Cython</code>, you need  to ensure that a <code>C/C++</code> Compiler (with appropriate flags) is present on your system.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>Building the <code>magenpy</code> package requires the following dependencies:</p> <ul> <li><code>Python</code> (&gt;=3.8)</li> <li><code>C/C++</code> Compiler</li> <li><code>Cython</code> (&gt;=0.29.21)</li> <li><code>NumPy</code> (&gt;=1.19.5)</li> </ul>"},{"location":"installation/#setting-up-the-environment-with-conda","title":"Setting up the environment with <code>conda</code>","text":"<p>If you can use <code>Anaconda</code> or <code>miniconda</code> to manage your Python environment, we recommend using them to create  a new environment with the required dependencies as follows:</p> <pre><code>python_version=3.11  # Change python version here if needed\nconda create --name \"magenpy_env\" -c anaconda -c conda-forge python=$python_version compilers openblas -y\nconda activate magenpy_env\n</code></pre> <p>Using <code>conda</code> to set up and manage your environment is especially recommended if you have trouble compiling  the <code>C/C++</code> extensions on your system.</p>"},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>The package is available for easy installation via the Python Package Index (<code>pypi</code>) can be installed using <code>pip</code>:</p> <pre><code>python -m pip install magenpy&gt;=0.1\n</code></pre>"},{"location":"installation/#building-from-source","title":"Building from source","text":"<p>You may also build the package from source, by cloning the repository and running the <code>make install</code> command:</p> <pre><code>git clone https://github.com/shz9/magenpy.git\ncd magenpy\nmake install\n</code></pre>"},{"location":"installation/#using-a-virtual-environment","title":"Using a virtual environment","text":"<p>If you wish to use <code>magenpy</code> on a shared computing environment or cluster, it is recommended that you install  the package in a virtual environment. Here's a quick example of how to install <code>magenpy</code> on a SLURM-based cluster:</p> <pre><code>module load python/3.8\npython -m venv magenpy_env\nsource magenpy_env/bin/activate\npython -m pip install --upgrade pip\npython -m pip install magenpy&gt;=0.1\n</code></pre>"},{"location":"installation/#using-docker-containers","title":"Using <code>Docker</code> containers","text":"<p>If you are using <code>Docker</code> containers, you can build a container with the <code>viprs</code> package  and all its dependencies by downloading the relevant <code>Dockerfile</code> from the  repository and building it  as follows:</p> <pre><code># Build the docker image:\ndocker build -f cli.Dockerfile -t magenpy-cli .\n# Run the container in interactive mode:\ndocker run -it magenpy-cli /bin/bash\n# Test that the package installed successfully:\nmagenpy_ld -h\n</code></pre> <p>We plan to publish pre-built <code>Docker</code> images on <code>DockerHub</code> in the future.</p>"},{"location":"api/AnnotationMatrix/","title":"AnnotationMatrix","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for handling annotation matrices, which are essentially tables of features for each variant in the genome. These features include information such as whether the variant is in coding regions, enhancers, etc. It can also include continuous features derived from experimental assays or other sources.</p> <p>The purpose of this class is to present a unified and consistent interface for handling annotations across different tools and applications. It should be able to read and write annotation matrices in different formats, filter annotations, and perform basic operations on the annotation matrix. It should also allow users to define new custom annotations that can be used for downstream statistical genetics applications.</p> <p>Attributes:</p> Name Type Description <code>table</code> <p>A pandas dataframe containing the annotation information.</p> <code>_annotations</code> <p>A list or array of column namees to consider as annotations. If not provided, will be inferred heuristically, though we recommend that the user specify this information.</p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>class AnnotationMatrix(object):\n    \"\"\"\n    A wrapper class for handling annotation matrices, which are essentially tables of\n    features for each variant in the genome. These features include information such as\n    whether the variant is in coding regions, enhancers, etc. It can also include continuous\n    features derived from experimental assays or other sources.\n\n    The purpose of this class is to present a unified and consistent interface for handling\n    annotations across different tools and applications. It should be able to read and write\n    annotation matrices in different formats, filter annotations, and perform basic operations\n    on the annotation matrix. It should also allow users to define new custom annotations\n    that can be used for downstream statistical genetics applications.\n\n    :ivar table: A pandas dataframe containing the annotation information.\n    :ivar _annotations: A list or array of column namees to consider as annotations. If not provided,\n    will be inferred heuristically, though we recommend that the user specify this information.\n    \"\"\"\n\n    def __init__(self, annotation_table=None, annotations=None):\n        \"\"\"\n        Initialize an AnnotationMatrix object\n\n        :param annotation_table: A pandas dataframe containing the annotation information.\n        :param annotations: A list of array of columns to consider as annotations. If not provided, will be\n        inferred heuristically, though we recommend that the user specify this information.\n        \"\"\"\n\n        self.table = annotation_table\n        self._annotations = annotations\n\n        if self.table is not None:\n            if self._annotations is None:\n                self._annotations = [ann for ann in self.table.columns if ann not in ('CHR', 'SNP', 'POS')]\n                if len(self._annotations) &lt; 1:\n                    self._annotations = None\n\n    @classmethod\n    def from_file(cls, annot_file, annot_format='magenpy', annot_parser=None,\n                  **parse_kwargs):\n        \"\"\"\n        Initialize an AnnotationMatrix object from a file.\n\n        :param annot_file: The path to the annotation file.\n        :param annot_format: The format of the annotation file. For now, we mainly support\n        annotation files in the `magenpy` and `ldsc` formats.\n        :param annot_parser: An `AnnotationMatrixParser` derived object, which can be tailored to\n        specific annotation formats that the user has.\n        :param parse_kwargs: arguments for the pandas `read_csv` function, such as the delimiter.\n\n        :return: An instance of the `AnnotationMatrix` class.\n        \"\"\"\n\n        from .parsers.annotation_parsers import AnnotationMatrixParser, LDSCAnnotationMatrixParser\n\n        if annot_parser is None:\n            if annot_format == 'magenpy':\n                annot_parser = AnnotationMatrixParser(None, **parse_kwargs)\n            elif annot_format == 'ldsc':\n                annot_parser = LDSCAnnotationMatrixParser(None, **parse_kwargs)\n            else:\n                raise KeyError(f\"Annotation matrix format {annot_format} not recognized!\")\n\n        annot_table, annotations = annot_parser.parse(annot_file)\n\n        annot_mat = cls(annotation_table=annot_table, annotations=annotations)\n\n        return annot_mat\n\n    @property\n    def shape(self):\n        \"\"\"\n        :return: The dimensions of the annotation matrix (number of variants x number of annotations).\n        \"\"\"\n        return self.n_snps, self.n_annotations\n\n    @property\n    def n_snps(self):\n        \"\"\"\n        :return: The number of variants in the annotation matrix.\n        \"\"\"\n        return len(self.table)\n\n    @property\n    def chromosome(self):\n        \"\"\"\n        A convenience method to get the chromosome if there is only one chromosome in the annotation matrix.\n\n        :return: The chromosome number if there is only one chromosome in the annotation matrix. Otherwise, None.\n        \"\"\"\n        chrom = self.chromosomes\n        if chrom is not None:\n            if len(chrom) == 1:\n                return chrom[0]\n\n    @property\n    def chromosomes(self):\n        \"\"\"\n        :return: The list of unique chromosomes in the annotation matrix.\n        \"\"\"\n        if 'CHR' in self.table.columns:\n            return self.table['CHR'].unique()\n\n    @property\n    def snps(self):\n        \"\"\"\n        :return: The list of SNP rsIDs in the annotation matrix.\n        \"\"\"\n        return self.table['SNP'].values\n\n    @property\n    def n_annotations(self):\n        \"\"\"\n        :return: The number of annotations in the annotation matrix.\n        \"\"\"\n        if self.annotations is None:\n            return 0\n        else:\n            return len(self.annotations)\n\n    @property\n    def binary_annotations(self):\n        \"\"\"\n        :return: A list of binary (0/1) annotations in the annotation matrix.\n        \"\"\"\n        assert self.annotations is not None\n        return np.array([c for c in self.annotations\n                         if len(self.table[c].unique()) == 2])\n\n    @property\n    def annotations(self):\n        \"\"\"\n        :return: The list of annotation names or IDs in the annotation matrix.\n        \"\"\"\n        return self._annotations\n\n    def values(self, add_intercept=False):\n        \"\"\"\n        :param add_intercept: Adds a base annotation corresponding to the intercept.\n\n        :return: The annotation matrix as a numpy matrix.\n        :raises KeyError: If no annotations are defined in the table.\n        \"\"\"\n\n        if self.annotations is None:\n            raise KeyError(\"No annotations are defined in this table!\")\n        annot_mat = self.table[self.annotations].values\n        if add_intercept:\n            return np.hstack([np.ones((annot_mat.shape[0], 1)), annot_mat])\n        else:\n            return annot_mat\n\n    def filter_snps(self, extract_snps=None, extract_file=None):\n        \"\"\"\n        Filter variants from the annotation matrix. User must specify\n        either a list of variants to extract or the path to a file\n        with the list of variants to extract.\n\n        :param extract_snps: A list or array of SNP IDs to keep in the annotation matrix.\n        :param extract_file: The path to a file with the list of variants to extract.\n        \"\"\"\n\n        assert extract_snps is not None or extract_file is not None\n\n        if extract_file is not None:\n            from .parsers.misc_parsers import read_snp_filter_file\n            extract_snps = read_snp_filter_file(extract_file)\n\n        from .utils.compute_utils import intersect_arrays\n\n        arr_idx = intersect_arrays(self.snps, extract_snps, return_index=True)\n\n        self.table = self.table.iloc[arr_idx, :].reset_index()\n\n    def filter_annotations(self, keep_annotations):\n        \"\"\"\n        Filter the list of annotations in the matrix.\n        :param keep_annotations: A list or array of annotations to keep.\n        \"\"\"\n\n        if self.annotations is None:\n            return\n\n        self._annotations = [annot for annot in self._annotations if annot in keep_annotations]\n        self.table = self.table[['CHR', 'SNP', 'POS'] + self._annotations]\n\n    def add_annotation(self, annot_vec, annotation_name):\n        \"\"\"\n        Add an annotation vector or list to the AnnotationMatrix object.\n\n        :param annot_vec: A vector/list/Series containing the annotation information for each SNP in the\n        AnnotationMatrix. For now, it's the responsibility of the user to make sure that the annotation\n        list or vector are sorted properly.\n        :param annotation_name: The name of the annotation to create. Make sure the name is not already\n        in the matrix!\n        \"\"\"\n\n        if self.annotations is not None:\n            assert annotation_name not in self.annotations\n        assert len(annot_vec) == self.n_snps\n\n        self.table[annotation_name] = annot_vec\n\n        if self.annotations is None:\n            self._annotations = [annotation_name]\n        else:\n            self._annotations = list(self._annotations) + [annotation_name]\n\n    def add_annotation_from_bed(self, bed_file, annotation_name):\n        \"\"\"\n        Add an annotation to the AnnotationMatrix from a BED file that lists\n        the range of coordinates associated with that annotation (e.g. coding regions, enhancers, etc.).\n        The BED file has to adhere to the format specified by,\n        https://uswest.ensembl.org/info/website/upload/bed.html\n        with the first three columns being:\n\n        CHR StartCoordinate EndCoordinate ...\n\n        !!! note\n            This implementation is quite slow at the moment. May need to find more efficient\n            ways to do the merge over list of ranges.\n\n        :param bed_file: The path to the BED file containing the annotation coordinates.\n        :param annotation_name: The name of the annotation to create. Make sure the name is not already\n        in the matrix!\n\n        :raises AssertionError: If the annotation name is already in the matrix.\n        \"\"\"\n\n        from .parsers.annotation_parsers import parse_annotation_bed_file\n\n        if self.annotations is not None:\n            assert annotation_name not in self.annotations\n\n        bed_df = parse_annotation_bed_file(bed_file)\n        # Group the BED annotation file by chromosome:\n        range_groups = bed_df.groupby('CHR').groups\n\n        def annotation_overlap(row):\n            \"\"\"\n            This function takes a row from the annotation matrix table\n            and returns True if and only if the BP position for the\n            SNP is within the range specified by the annotation BED file.\n            \"\"\"\n            try:\n                chr_range = bed_df.iloc[range_groups[row['CHR']], :]\n            except KeyError:\n                return False\n\n            check = (chr_range.Start &lt;= row['POS']) &amp; (chr_range.End &gt;= row['POS'])\n            return int(np.any(check))\n\n        self.table[annotation_name] = self.table.apply(annotation_overlap, axis=1)\n\n        if self.annotations is None:\n            self._annotations = [annotation_name]\n        else:\n            self._annotations = list(self._annotations) + [annotation_name]\n\n    def get_binary_annotation_index(self, bin_annot):\n        \"\"\"\n        :param bin_annot: The name of the binary annotation for which to fetch the relevant variants.\n        :return: The indices of all variants that belong to binary annotation `bin_annot`\n        \"\"\"\n        assert bin_annot in self.binary_annotations\n        return np.where(self.table[bin_annot] == 1)[0]\n\n    def split_by_chromosome(self):\n        \"\"\"\n        Split the annotation matrix by chromosome.\n\n        :return: A dictionary of `AnnotationMatrix` objects, where the keys are the chromosome numbers.\n        \"\"\"\n\n        if 'CHR' in self.table.columns:\n            chrom_tables = self.table.groupby('CHR')\n            return {\n                c: AnnotationMatrix(annotation_table=chrom_tables.get_group(c),\n                                    annotations=self.annotations)\n                for c in chrom_tables.groups\n            }\n        else:\n            raise KeyError(\"Chromosome information is not available in the annotation table!\")\n\n    def to_file(self, output_path, col_subset=None, compress=True, **to_csv_kwargs):\n        \"\"\"\n        A convenience method to write the annotation matrix to a file.\n\n        :param output_path: The path and prefix to the file where to write the annotation matrix.\n        :param col_subset: A subset of the columns to write to file.\n        :param compress: Whether to compress the output file (default: True).\n        :param to_csv_kwargs: Key-word arguments to the pandas csv writer.\n        \"\"\"\n\n        if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n            to_csv_kwargs['sep'] = '\\t'\n\n        if 'index' not in to_csv_kwargs:\n            to_csv_kwargs['index'] = False\n\n        if col_subset is not None:\n            table = self.table[col_subset]\n        else:\n            table = self.table\n\n        file_name = output_path + '.annot'\n        if compress:\n            file_name += '.gz'\n\n        table.to_csv(file_name, **to_csv_kwargs)\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.annotations","title":"<code>annotations</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of annotation names or IDs in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.binary_annotations","title":"<code>binary_annotations</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A list of binary (0/1) annotations in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.chromosome","title":"<code>chromosome</code>  <code>property</code>","text":"<p>A convenience method to get the chromosome if there is only one chromosome in the annotation matrix.</p> <p>Returns:</p> Type Description <p>The chromosome number if there is only one chromosome in the annotation matrix. Otherwise, None.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.chromosomes","title":"<code>chromosomes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of unique chromosomes in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.n_annotations","title":"<code>n_annotations</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of annotations in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of variants in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The dimensions of the annotation matrix (number of variants x number of annotations).</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of SNP rsIDs in the annotation matrix.</p>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.__init__","title":"<code>__init__(annotation_table=None, annotations=None)</code>","text":"<p>Initialize an AnnotationMatrix object</p> <p>Parameters:</p> Name Type Description Default <code>annotation_table</code> <p>A pandas dataframe containing the annotation information.</p> <code>None</code> <code>annotations</code> <p>A list of array of columns to consider as annotations. If not provided, will be inferred heuristically, though we recommend that the user specify this information.</p> <code>None</code> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def __init__(self, annotation_table=None, annotations=None):\n    \"\"\"\n    Initialize an AnnotationMatrix object\n\n    :param annotation_table: A pandas dataframe containing the annotation information.\n    :param annotations: A list of array of columns to consider as annotations. If not provided, will be\n    inferred heuristically, though we recommend that the user specify this information.\n    \"\"\"\n\n    self.table = annotation_table\n    self._annotations = annotations\n\n    if self.table is not None:\n        if self._annotations is None:\n            self._annotations = [ann for ann in self.table.columns if ann not in ('CHR', 'SNP', 'POS')]\n            if len(self._annotations) &lt; 1:\n                self._annotations = None\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.add_annotation","title":"<code>add_annotation(annot_vec, annotation_name)</code>","text":"<p>Add an annotation vector or list to the AnnotationMatrix object.</p> <p>Parameters:</p> Name Type Description Default <code>annot_vec</code> <p>A vector/list/Series containing the annotation information for each SNP in the AnnotationMatrix. For now, it's the responsibility of the user to make sure that the annotation list or vector are sorted properly.</p> required <code>annotation_name</code> <p>The name of the annotation to create. Make sure the name is not already in the matrix!</p> required Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def add_annotation(self, annot_vec, annotation_name):\n    \"\"\"\n    Add an annotation vector or list to the AnnotationMatrix object.\n\n    :param annot_vec: A vector/list/Series containing the annotation information for each SNP in the\n    AnnotationMatrix. For now, it's the responsibility of the user to make sure that the annotation\n    list or vector are sorted properly.\n    :param annotation_name: The name of the annotation to create. Make sure the name is not already\n    in the matrix!\n    \"\"\"\n\n    if self.annotations is not None:\n        assert annotation_name not in self.annotations\n    assert len(annot_vec) == self.n_snps\n\n    self.table[annotation_name] = annot_vec\n\n    if self.annotations is None:\n        self._annotations = [annotation_name]\n    else:\n        self._annotations = list(self._annotations) + [annotation_name]\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.add_annotation_from_bed","title":"<code>add_annotation_from_bed(bed_file, annotation_name)</code>","text":"<p>Add an annotation to the AnnotationMatrix from a BED file that lists the range of coordinates associated with that annotation (e.g. coding regions, enhancers, etc.). The BED file has to adhere to the format specified by, https://uswest.ensembl.org/info/website/upload/bed.html with the first three columns being:</p> <p>CHR StartCoordinate EndCoordinate ...</p> <p>Note</p> <p>This implementation is quite slow at the moment. May need to find more efficient ways to do the merge over list of ranges.</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <p>The path to the BED file containing the annotation coordinates.</p> required <code>annotation_name</code> <p>The name of the annotation to create. Make sure the name is not already in the matrix!</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the annotation name is already in the matrix.</p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def add_annotation_from_bed(self, bed_file, annotation_name):\n    \"\"\"\n    Add an annotation to the AnnotationMatrix from a BED file that lists\n    the range of coordinates associated with that annotation (e.g. coding regions, enhancers, etc.).\n    The BED file has to adhere to the format specified by,\n    https://uswest.ensembl.org/info/website/upload/bed.html\n    with the first three columns being:\n\n    CHR StartCoordinate EndCoordinate ...\n\n    !!! note\n        This implementation is quite slow at the moment. May need to find more efficient\n        ways to do the merge over list of ranges.\n\n    :param bed_file: The path to the BED file containing the annotation coordinates.\n    :param annotation_name: The name of the annotation to create. Make sure the name is not already\n    in the matrix!\n\n    :raises AssertionError: If the annotation name is already in the matrix.\n    \"\"\"\n\n    from .parsers.annotation_parsers import parse_annotation_bed_file\n\n    if self.annotations is not None:\n        assert annotation_name not in self.annotations\n\n    bed_df = parse_annotation_bed_file(bed_file)\n    # Group the BED annotation file by chromosome:\n    range_groups = bed_df.groupby('CHR').groups\n\n    def annotation_overlap(row):\n        \"\"\"\n        This function takes a row from the annotation matrix table\n        and returns True if and only if the BP position for the\n        SNP is within the range specified by the annotation BED file.\n        \"\"\"\n        try:\n            chr_range = bed_df.iloc[range_groups[row['CHR']], :]\n        except KeyError:\n            return False\n\n        check = (chr_range.Start &lt;= row['POS']) &amp; (chr_range.End &gt;= row['POS'])\n        return int(np.any(check))\n\n    self.table[annotation_name] = self.table.apply(annotation_overlap, axis=1)\n\n    if self.annotations is None:\n        self._annotations = [annotation_name]\n    else:\n        self._annotations = list(self._annotations) + [annotation_name]\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.filter_annotations","title":"<code>filter_annotations(keep_annotations)</code>","text":"<p>Filter the list of annotations in the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>keep_annotations</code> <p>A list or array of annotations to keep.</p> required Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def filter_annotations(self, keep_annotations):\n    \"\"\"\n    Filter the list of annotations in the matrix.\n    :param keep_annotations: A list or array of annotations to keep.\n    \"\"\"\n\n    if self.annotations is None:\n        return\n\n    self._annotations = [annot for annot in self._annotations if annot in keep_annotations]\n    self.table = self.table[['CHR', 'SNP', 'POS'] + self._annotations]\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None)</code>","text":"<p>Filter variants from the annotation matrix. User must specify either a list of variants to extract or the path to a file with the list of variants to extract.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list or array of SNP IDs to keep in the annotation matrix.</p> <code>None</code> <code>extract_file</code> <p>The path to a file with the list of variants to extract.</p> <code>None</code> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None):\n    \"\"\"\n    Filter variants from the annotation matrix. User must specify\n    either a list of variants to extract or the path to a file\n    with the list of variants to extract.\n\n    :param extract_snps: A list or array of SNP IDs to keep in the annotation matrix.\n    :param extract_file: The path to a file with the list of variants to extract.\n    \"\"\"\n\n    assert extract_snps is not None or extract_file is not None\n\n    if extract_file is not None:\n        from .parsers.misc_parsers import read_snp_filter_file\n        extract_snps = read_snp_filter_file(extract_file)\n\n    from .utils.compute_utils import intersect_arrays\n\n    arr_idx = intersect_arrays(self.snps, extract_snps, return_index=True)\n\n    self.table = self.table.iloc[arr_idx, :].reset_index()\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.from_file","title":"<code>from_file(annot_file, annot_format='magenpy', annot_parser=None, **parse_kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize an AnnotationMatrix object from a file.</p> <p>Parameters:</p> Name Type Description Default <code>annot_file</code> <p>The path to the annotation file.</p> required <code>annot_format</code> <p>The format of the annotation file. For now, we mainly support annotation files in the <code>magenpy</code> and <code>ldsc</code> formats.</p> <code>'magenpy'</code> <code>annot_parser</code> <p>An <code>AnnotationMatrixParser</code> derived object, which can be tailored to specific annotation formats that the user has.</p> <code>None</code> <code>parse_kwargs</code> <p>arguments for the pandas <code>read_csv</code> function, such as the delimiter.</p> <code>{}</code> <p>Returns:</p> Type Description <p>An instance of the <code>AnnotationMatrix</code> class.</p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>@classmethod\ndef from_file(cls, annot_file, annot_format='magenpy', annot_parser=None,\n              **parse_kwargs):\n    \"\"\"\n    Initialize an AnnotationMatrix object from a file.\n\n    :param annot_file: The path to the annotation file.\n    :param annot_format: The format of the annotation file. For now, we mainly support\n    annotation files in the `magenpy` and `ldsc` formats.\n    :param annot_parser: An `AnnotationMatrixParser` derived object, which can be tailored to\n    specific annotation formats that the user has.\n    :param parse_kwargs: arguments for the pandas `read_csv` function, such as the delimiter.\n\n    :return: An instance of the `AnnotationMatrix` class.\n    \"\"\"\n\n    from .parsers.annotation_parsers import AnnotationMatrixParser, LDSCAnnotationMatrixParser\n\n    if annot_parser is None:\n        if annot_format == 'magenpy':\n            annot_parser = AnnotationMatrixParser(None, **parse_kwargs)\n        elif annot_format == 'ldsc':\n            annot_parser = LDSCAnnotationMatrixParser(None, **parse_kwargs)\n        else:\n            raise KeyError(f\"Annotation matrix format {annot_format} not recognized!\")\n\n    annot_table, annotations = annot_parser.parse(annot_file)\n\n    annot_mat = cls(annotation_table=annot_table, annotations=annotations)\n\n    return annot_mat\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.get_binary_annotation_index","title":"<code>get_binary_annotation_index(bin_annot)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bin_annot</code> <p>The name of the binary annotation for which to fetch the relevant variants.</p> required <p>Returns:</p> Type Description <p>The indices of all variants that belong to binary annotation <code>bin_annot</code></p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def get_binary_annotation_index(self, bin_annot):\n    \"\"\"\n    :param bin_annot: The name of the binary annotation for which to fetch the relevant variants.\n    :return: The indices of all variants that belong to binary annotation `bin_annot`\n    \"\"\"\n    assert bin_annot in self.binary_annotations\n    return np.where(self.table[bin_annot] == 1)[0]\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.split_by_chromosome","title":"<code>split_by_chromosome()</code>","text":"<p>Split the annotation matrix by chromosome.</p> <p>Returns:</p> Type Description <p>A dictionary of <code>AnnotationMatrix</code> objects, where the keys are the chromosome numbers.</p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def split_by_chromosome(self):\n    \"\"\"\n    Split the annotation matrix by chromosome.\n\n    :return: A dictionary of `AnnotationMatrix` objects, where the keys are the chromosome numbers.\n    \"\"\"\n\n    if 'CHR' in self.table.columns:\n        chrom_tables = self.table.groupby('CHR')\n        return {\n            c: AnnotationMatrix(annotation_table=chrom_tables.get_group(c),\n                                annotations=self.annotations)\n            for c in chrom_tables.groups\n        }\n    else:\n        raise KeyError(\"Chromosome information is not available in the annotation table!\")\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.to_file","title":"<code>to_file(output_path, col_subset=None, compress=True, **to_csv_kwargs)</code>","text":"<p>A convenience method to write the annotation matrix to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <p>The path and prefix to the file where to write the annotation matrix.</p> required <code>col_subset</code> <p>A subset of the columns to write to file.</p> <code>None</code> <code>compress</code> <p>Whether to compress the output file (default: True).</p> <code>True</code> <code>to_csv_kwargs</code> <p>Key-word arguments to the pandas csv writer.</p> <code>{}</code> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def to_file(self, output_path, col_subset=None, compress=True, **to_csv_kwargs):\n    \"\"\"\n    A convenience method to write the annotation matrix to a file.\n\n    :param output_path: The path and prefix to the file where to write the annotation matrix.\n    :param col_subset: A subset of the columns to write to file.\n    :param compress: Whether to compress the output file (default: True).\n    :param to_csv_kwargs: Key-word arguments to the pandas csv writer.\n    \"\"\"\n\n    if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n        to_csv_kwargs['sep'] = '\\t'\n\n    if 'index' not in to_csv_kwargs:\n        to_csv_kwargs['index'] = False\n\n    if col_subset is not None:\n        table = self.table[col_subset]\n    else:\n        table = self.table\n\n    file_name = output_path + '.annot'\n    if compress:\n        file_name += '.gz'\n\n    table.to_csv(file_name, **to_csv_kwargs)\n</code></pre>"},{"location":"api/AnnotationMatrix/#magenpy.AnnotationMatrix.AnnotationMatrix.values","title":"<code>values(add_intercept=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>add_intercept</code> <p>Adds a base annotation corresponding to the intercept.</p> <code>False</code> <p>Returns:</p> Type Description <p>The annotation matrix as a numpy matrix.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no annotations are defined in the table.</p> Source code in <code>magenpy/AnnotationMatrix.py</code> <pre><code>def values(self, add_intercept=False):\n    \"\"\"\n    :param add_intercept: Adds a base annotation corresponding to the intercept.\n\n    :return: The annotation matrix as a numpy matrix.\n    :raises KeyError: If no annotations are defined in the table.\n    \"\"\"\n\n    if self.annotations is None:\n        raise KeyError(\"No annotations are defined in this table!\")\n    annot_mat = self.table[self.annotations].values\n    if add_intercept:\n        return np.hstack([np.ones((annot_mat.shape[0], 1)), annot_mat])\n    else:\n        return annot_mat\n</code></pre>"},{"location":"api/GWADataLoader/","title":"GWADataLoader","text":"<p>               Bases: <code>object</code></p> <p>A class to load and manage multiple data sources for genetic association studies. This class is designed to handle genotype matrices, summary statistics, LD matrices, and annotation matrices. It also provides functionalities to filter samples and/or SNPs, harmonize data sources, and compute LD matrices. This is all done in order to facilitate downstream statistical genetics analyses that require multiple data sources to be aligned and harmonized. The use cases include:</p> <ul> <li>Summary statistics-based PRS computation</li> <li>Summary statistics-based heritability estimation.</li> <li>Complex trait simulation.</li> <li>Performing Genome-wide association tests.</li> </ul> <p>Attributes:</p> Name Type Description <code>genotype</code> <code>Union[Dict[int, GenotypeMatrix], None]</code> <p>A dictionary of <code>GenotypeMatrix</code> objects, where the key is the chromosome number.</p> <code>sample_table</code> <code>Union[SampleTable, None]</code> <p>A <code>SampleTable</code> object containing the sample information.</p> <code>phenotype_likelihood</code> <code>str</code> <p>The likelihood of the phenotype (e.g. <code>gaussian</code>, <code>binomial</code>).</p> <code>ld</code> <code>Union[Dict[int, LDMatrix], None]</code> <p>A dictionary of <code>LDMatrix</code> objects, where the key is the chromosome number.</p> <code>sumstats_table</code> <code>Union[Dict[int, SumstatsTable], None]</code> <p>A dictionary of <code>SumstatsTable</code> objects, where the key is the chromosome number.</p> <code>annotation</code> <code>Union[Dict[int, AnnotationMatrix], None]</code> <p>A dictionary of <code>AnnotationMatrix</code> objects, where the key is the chromosome number.</p> <code>backend</code> <p>The backend software used for the computation. Currently, supports <code>xarray</code> and <code>plink</code>.</p> <code>temp_dir</code> <p>The temporary directory where we store intermediate files (if necessary).</p> <code>output_dir</code> <p>The output directory where we store the results of the computation.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>class GWADataLoader(object):\n    \"\"\"\n    A class to load and manage multiple data sources for genetic association studies.\n    This class is designed to handle genotype matrices, summary statistics, LD matrices,\n    and annotation matrices. It also provides functionalities to filter samples and/or SNPs,\n    harmonize data sources, and compute LD matrices. This is all done in order to facilitate\n    downstream statistical genetics analyses that require multiple data sources to be aligned\n    and harmonized. The use cases include:\n\n    * Summary statistics-based PRS computation\n    * Summary statistics-based heritability estimation.\n    * Complex trait simulation.\n    * Performing Genome-wide association tests.\n\n    :ivar genotype: A dictionary of `GenotypeMatrix` objects, where the key is the chromosome number.\n    :ivar sample_table: A `SampleTable` object containing the sample information.\n    :ivar phenotype_likelihood: The likelihood of the phenotype (e.g. `gaussian`, `binomial`).\n    :ivar ld: A dictionary of `LDMatrix` objects, where the key is the chromosome number.\n    :ivar sumstats_table: A dictionary of `SumstatsTable` objects, where the key is the chromosome number.\n    :ivar annotation: A dictionary of `AnnotationMatrix` objects, where the key is the chromosome number.\n    :ivar backend: The backend software used for the computation. Currently, supports `xarray` and `plink`.\n    :ivar temp_dir: The temporary directory where we store intermediate files (if necessary).\n    :ivar output_dir: The output directory where we store the results of the computation.\n    \"\"\"\n\n    def __init__(self,\n                 bed_files=None,\n                 phenotype_file=None,\n                 covariates_file=None,\n                 keep_samples=None,\n                 keep_file=None,\n                 extract_snps=None,\n                 extract_file=None,\n                 min_maf=None,\n                 min_mac=None,\n                 drop_duplicated=True,\n                 phenotype_likelihood='gaussian',\n                 sumstats_files=None,\n                 sumstats_format='magenpy',\n                 ld_store_files=None,\n                 annotation_files=None,\n                 annotation_format='magenpy',\n                 backend='xarray',\n                 temp_dir='temp',\n                 output_dir='output',\n                 threads=1):\n        \"\"\"\n        Initialize the `GWADataLoader` object with the data sources required for\n        downstream statistical genetics analyses.\n\n        :param bed_files: The path to the BED file(s). You may use a wildcard here to read files for multiple\n        chromosomes.\n        :param phenotype_file: The path to the phenotype file.\n        (Default: tab-separated file with `FID IID phenotype` columns).\n        :param covariates_file: The path to the covariates file.\n        (Default: tab-separated file starting with the `FID IID ...` columns and followed by the covariate columns).\n        :param keep_samples: A vector or list of sample IDs to keep when filtering the genotype matrix.\n        :param keep_file: A path to a plink-style keep file to select a subset of individuals.\n        :param extract_snps: A vector or list of SNP IDs to keep when filtering the genotype matrix.\n        :param extract_file: A path to a plink-style extract file to select a subset of SNPs.\n        :param min_maf: The minimum minor allele frequency cutoff.\n        :param min_mac: The minimum minor allele count cutoff.\n        :param drop_duplicated: If True, drop SNPs with duplicated rsID.\n        :param phenotype_likelihood: The likelihood of the phenotype (e.g. `gaussian`, `binomial`).\n        :param sumstats_files: The path to the summary statistics file(s). The path may be a wildcard.\n        :param sumstats_format: The format for the summary statistics. Currently, supports the following\n        formats: `plink1.9`, `plink2`, `magenpy`, `fastGWA`, `COJO`, `SAIGE`, or `GWASCatalog` for the standard\n        summary statistics format (also known as `ssf` or `gwas-ssf`).\n        :param ld_store_files: The path to the LD matrices. This may be a wildcard to accommodate reading data\n        for multiple chromosomes.\n        :param annotation_files: The path to the annotation file(s). The path may contain a wildcard.\n        :param annotation_format: The format for the summary statistics. Currently, supports the following\n        formats: `magenpy`, `ldsc`.\n        :param backend: The backend software used for computations with the genotype matrix. Currently, supports\n        `xarray` and `plink`.\n        :param temp_dir: The temporary directory where to store intermediate files.\n        :param output_dir: The output directory where to store the results of the computation.\n        :param threads: The number of threads to use for computations.\n        \"\"\"\n\n        # ------- Sanity checks -------\n\n        assert backend in ('xarray', 'plink', 'bed-reader')\n        assert phenotype_likelihood in ('gaussian', 'binomial')\n\n        # ------- General options -------\n\n        self.backend = backend\n\n        self.temp_dir = temp_dir\n        self.output_dir = output_dir\n        self.cleanup_dir_list = []  # Directories to clean up after execution.\n\n        makedir([temp_dir, output_dir])\n\n        self.threads = threads\n\n        # ------- General parameters -------\n\n        self.phenotype_likelihood: str = phenotype_likelihood\n\n        self.genotype: Union[Dict[int, GenotypeMatrix], None] = None\n        self.sample_table: Union[SampleTable, None] = None\n        self.ld: Union[Dict[int, LDMatrix], None] = None\n        self.sumstats_table: Union[Dict[int, SumstatsTable], None] = None\n        self.annotation: Union[Dict[int, AnnotationMatrix], None] = None\n\n        # ------- Read data files -------\n\n        self.read_genotypes(bed_files,\n                            min_maf=min_maf,\n                            min_mac=min_mac,\n                            drop_duplicated=drop_duplicated)\n        self.read_phenotype(phenotype_file)\n        self.read_covariates(covariates_file)\n        self.read_ld(ld_store_files)\n        self.read_annotations(annotation_files,\n                              annot_format=annotation_format)\n        self.read_summary_statistics(sumstats_files,\n                                     sumstats_format,\n                                     drop_duplicated=drop_duplicated)\n\n        # ------- Filter samples or SNPs -------\n\n        if extract_snps is not None or extract_file is not None:\n            self.filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n        if keep_samples is not None or keep_file is not None:\n            self.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n        # ------- Harmonize data sources -------\n\n        self.harmonize_data()\n\n    @property\n    def samples(self):\n        \"\"\"\n        :return: The list of samples retained in the sample table.\n        \"\"\"\n        if self.sample_table is not None:\n            return self.sample_table.iid\n\n    @property\n    def sample_size(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [n][magenpy.GWADataLoader.GWADataLoader.n]\n\n        :return: The number of samples in the genotype matrix.\n\n        \"\"\"\n        if self.sample_table is not None:\n            return self.sample_table.n\n        elif self.sumstats_table is not None:\n            return np.max([np.max(ss.n_per_snp) for ss in self.sumstats_table.values()])\n        else:\n            raise ValueError(\"Information about the sample size is not available!\")\n\n    @property\n    def n(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [sample_size][magenpy.GWADataLoader.GWADataLoader.sample_size]\n\n        :return: The number of samples in the genotype matrix.\n        \"\"\"\n\n        return self.sample_size\n\n    @property\n    def snps(self):\n        \"\"\"\n        :return: The list of SNP rsIDs retained in each chromosome.\n        :rtype: dict\n        \"\"\"\n        if self.genotype is not None:\n            return {c: g.snps for c, g in self.genotype.items()}\n        elif self.sumstats_table is not None:\n            return {c: s.snps for c, s in self.sumstats_table.items()}\n        elif self.ld is not None:\n            return {c: l.snps for c, l in self.ld.items()}\n        elif self.annotation is not None:\n            return {c: a.snps for c, a in self.annotation.items()}\n        else:\n            raise ValueError(\"GWADataLoader instance is not properly initialized!\")\n\n    @property\n    def m(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n_snps][magenpy.GWADataLoader.GWADataLoader.n_snps]\n\n        :return: The number of variants in the harmonized data sources.\n        \"\"\"\n        return sum(self.shapes.values())\n\n    @property\n    def n_snps(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [m][magenpy.GWADataLoader.GWADataLoader.m]\n\n        :return: The number of variants in the harmonized data sources.\n        \"\"\"\n        return self.m\n\n    @property\n    def shapes(self):\n        \"\"\"\n        :return: A dictionary where the key is the chromosome number and the value is\n        the number of variants on that chromosome.\n        \"\"\"\n        if self.genotype is not None:\n            return {c: g.shape[1] for c, g in self.genotype.items()}\n        elif self.sumstats_table is not None:\n            return {c: s.shape[0] for c, s in self.sumstats_table.items()}\n        elif self.ld is not None:\n            return {c: l.n_snps for c, l in self.ld.items()}\n        elif self.annotation is not None:\n            return {c: a.shape[0] for c, a in self.annotation.items()}\n        else:\n            raise ValueError(\"GWADataLoader instance is not properly initialized!\")\n\n    @property\n    def chromosomes(self):\n        \"\"\"\n        :return: The list of chromosomes that were loaded to `GWADataLoader`.\n        \"\"\"\n        return sorted(list(self.shapes.keys()))\n\n    @property\n    def n_annotations(self):\n        \"\"\"\n        :return: The number of annotations included in the annotation matrices.\n        \"\"\"\n        if self.annotation is not None:\n            return self.annotation[self.chromosomes[0]].n_annotations\n\n    def filter_snps(self, extract_snps=None, extract_file=None, chromosome=None):\n        \"\"\"\n        Filter the SNP set from all the GWADataLoader objects.\n        :param extract_snps: A list or array of SNP rsIDs to keep.\n        :param extract_file: A path to a plink-style file with SNP rsIDs to keep.\n        :param chromosome: Chromosome number. If specified, applies the filter to that chromosome only.\n        \"\"\"\n\n        if extract_snps is None and extract_file is None:\n            return\n\n        if chromosome is not None:\n            chroms = [chromosome]\n        else:\n            chroms = self.chromosomes\n\n        if extract_snps is None:\n            from .parsers.misc_parsers import read_snp_filter_file\n            extract_snps = read_snp_filter_file(extract_file)\n\n        for c in chroms:\n\n            # Filter the genotype matrix:\n            if self.genotype is not None and c in self.genotype:\n                self.genotype[c].filter_snps(extract_snps=extract_snps)\n\n                # If no SNPs remain in the genotype matrix for that chromosome, then remove it:\n                if self.genotype[c].shape[1] &lt; 1:\n                    del self.genotype[c]\n\n            # Filter the summary statistics table:\n            if self.sumstats_table is not None and c in self.sumstats_table:\n                self.sumstats_table[c].filter_snps(extract_snps=extract_snps)\n\n                # If no SNPs remain in the summary statistics table for that chromosome, then remove it:\n                if self.sumstats_table[c].shape[0] &lt; 1:\n                    del self.sumstats_table[c]\n\n            if self.ld is not None and c in self.ld:\n                self.ld[c].filter_snps(extract_snps=extract_snps)\n\n                # If no SNPs remain in the summary statistics table for that chromosome, then remove it:\n                if self.ld[c].n_snps &lt; 1:\n                    del self.ld[c]\n\n            # Filter the annotation matrix:\n            if self.annotation is not None and c in self.annotation:\n                self.annotation[c].filter_snps(extract_snps=extract_snps)\n\n                if self.annotation[c].shape[0] &lt; 1:\n                    del self.annotation[c]\n\n    def filter_samples(self, keep_samples=None, keep_file=None):\n        \"\"\"\n        Filter samples from the samples table. User must specify\n        either a list of samples to keep or the path to a file\n        with the list of samples to keep.\n\n        :param keep_samples: A list or array of sample IDs to keep.\n        :param keep_file: The path to a file with the list of samples to keep.\n        \"\"\"\n\n        self.sample_table.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n        self.sync_sample_tables()\n\n    def read_annotations(self, annot_path,\n                         annot_format='magenpy',\n                         parser=None,\n                         **parse_kwargs):\n        \"\"\"\n        Read the annotation matrix from file. Annotations are a set of features associated\n        with each SNP and are generally represented in table format.\n        Consult the documentation for `AnnotationMatrix` for more details.\n\n        :param annot_path: The path to the annotation file(s). The path may contain a wildcard.\n        :param annot_format: The format for the summary statistics. Currently, supports the following\n         formats: `magenpy`, `ldsc`.\n        :param parser: If the annotation file does not follow any of the formats above, you can create\n        your own parser by inheriting from the base `AnnotationMatrixParser` class and passing it here as an argument.\n        :param parse_kwargs: keyword arguments for the parser. These are mainly parameters that will be passed to\n        `pandas.read_csv` function, such as the delimiter, header information, etc.\n        \"\"\"\n\n        if annot_path is None:\n            return\n\n        # Find all the relevant files in the path passed by the user:\n        if not iterable(annot_path):\n            annot_files = get_filenames(annot_path, extension='.annot')\n        else:\n            annot_files = annot_path\n\n        if len(annot_files) &lt; 1:\n            logger.warning(f\"No annotation files were found at: {annot_path}\")\n            return\n\n        logger.info(\"&gt; Reading annotation file...\")\n\n        self.annotation = {}\n\n        for annot_file in tqdm(annot_files,\n                               total=len(annot_files),\n                               desc=\"Reading annotation files\"):\n            annot_mat = AnnotationMatrix.from_file(annot_file,\n                                                   annot_format=annot_format,\n                                                   annot_parser=parser,\n                                                   **parse_kwargs)\n            self.annotation[annot_mat.chromosome] = annot_mat\n\n    def read_genotypes(self,\n                       bed_paths,\n                       keep_samples=None,\n                       keep_file=None,\n                       extract_snps=None,\n                       extract_file=None,\n                       min_maf=None,\n                       min_mac=1,\n                       drop_duplicated=True):\n        \"\"\"\n        Read the genotype matrix and/or associated metadata from plink's BED file format.\n        Consult the documentation for `GenotypeMatrix` for more details.\n\n        :param bed_paths: The path to the BED file(s). You may use a wildcard here to read files for multiple\n        chromosomes.\n        :param keep_samples: A vector or list of sample IDs to keep when filtering the genotype matrix.\n        :param keep_file: A path to a plink-style file containing sample IDs to keep.\n        :param extract_snps: A vector or list of SNP IDs to keep when filtering the genotype matrix.\n        :param extract_file: A path to a plink-style file containing SNP IDs to keep.\n        :param min_maf: The minimum minor allele frequency cutoff.\n        :param min_mac: The minimum minor allele count cutoff.\n        :param drop_duplicated: If True, drop SNPs with duplicated rsID.\n        \"\"\"\n\n        if bed_paths is None:\n            return\n\n        # Find all the relevant files in the path passed by the user:\n        if not iterable(bed_paths):\n            bed_files = get_filenames(bed_paths, extension='.bed')\n        else:\n            bed_files = bed_paths\n\n        if len(bed_files) &lt; 1:\n            logger.warning(f\"No BED files were found at: {bed_paths}\")\n            return\n\n        # Depending on the backend, select the `GenotypeMatrix` class:\n        if self.backend == 'xarray':\n            gmat_class = xarrayGenotypeMatrix\n        elif self.backend == 'bed-reader':\n            gmat_class = bedReaderGenotypeMatrix\n        else:\n            gmat_class = plinkBEDGenotypeMatrix\n\n        logger.info(\"&gt; Reading genotype metadata...\")\n\n        self.genotype = {}\n\n        for bfile in tqdm(bed_files,\n                          total=len(bed_files),\n                          desc=\"Reading genotype metadata\"):\n            # Read BED file and update the genotypes dictionary:\n            self.genotype.update(gmat_class.from_file(bfile,\n                                                      temp_dir=self.temp_dir,\n                                                      threads=self.threads).split_by_chromosome())\n\n        # After reading the genotype matrices, apply some standard filters:\n        for i, (c, g) in enumerate(self.genotype.items()):\n\n            # Filter the genotype matrix to keep certain subsample:\n            if keep_samples or keep_file:\n                g.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n            # Filter the genotype matrix to keep certain SNPs\n            if extract_snps or extract_file:\n                g.filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n            # Drop duplicated SNP IDs\n            if drop_duplicated:\n                g.drop_duplicated_snps()\n\n            # Filter SNPs by minor allele frequency and/or count:\n            g.filter_by_allele_frequency(min_maf=min_maf, min_mac=min_mac)\n\n            if i == 0:\n                self.sample_table = g.sample_table\n\n    def read_phenotype(self, phenotype_file, drop_na=True, **read_csv_kwargs):\n        \"\"\"\n        Read the phenotype file and integrate it with the sample tables and genotype matrices.\n\n        :param phenotype_file: The path to the phenotype file\n        (Default: tab-separated file with `FID IID phenotype` columns). If different, supply\n        details as additional arguments to this function.\n        :param drop_na: Drop samples with missing phenotype information.\n        :param read_csv_kwargs: keyword arguments for the `read_csv` function of `pandas`.\n        \"\"\"\n\n        if phenotype_file is None:\n            return\n\n        logger.info(\"&gt; Reading phenotype file...\")\n\n        assert self.sample_table is not None\n\n        self.sample_table.read_phenotype_file(phenotype_file, drop_na=drop_na, **read_csv_kwargs)\n        self.sync_sample_tables()\n\n    def set_phenotype(self, new_phenotype, phenotype_likelihood=None):\n        \"\"\"\n        A convenience method to update the phenotype column for the samples.\n        :param new_phenotype: A vector or list of phenotype values.\n        :param phenotype_likelihood: The phenotype likelihood (e.g. `binomial`, `gaussian`). Optional.\n        \"\"\"\n\n        self.sample_table.set_phenotype(new_phenotype,\n                                        phenotype_likelihood=phenotype_likelihood or self.phenotype_likelihood)\n        self.sync_sample_tables()\n\n    def read_covariates(self, covariates_file, **read_csv_kwargs):\n        \"\"\"\n        Read the covariates file and integrate it with the sample tables and genotype matrices.\n\n        :param covariates_file: The path to the covariates file\n        (Default: tab-separated file starting with the `FID IID ...` columns and followed by the covariate columns).\n        :param read_csv_kwargs: keyword arguments for the `read_csv` function of `pandas`.\n        \"\"\"\n\n        if covariates_file is None:\n            return\n\n        logger.info(\"&gt; Reading covariates file...\")\n\n        assert self.sample_table is not None\n\n        self.sample_table.read_covariates_file(covariates_file, **read_csv_kwargs)\n        self.sync_sample_tables()\n\n    def read_summary_statistics(self,\n                                sumstats_path,\n                                sumstats_format='magenpy',\n                                parser=None,\n                                drop_duplicated=True,\n                                **parse_kwargs):\n        \"\"\"\n        Read GWAS summary statistics file(s) and parse them to `SumstatsTable` objects.\n\n        :param sumstats_path: The path to the summary statistics file(s). The path may be a wildcard.\n        :param sumstats_format: The format for the summary statistics. Currently supports the following\n         formats: `plink1.9`, `plink2`, `magenpy`, `fastGWA`, `COJO`, `SAIGE`, or `GWASCatalog` for the standard\n         summary statistics format (also known as `ssf` or `gwas-ssf`).\n        :param parser: If the summary statistics file does not follow any of the formats above, you can create\n        your own parser by inheriting from the base `SumstatsParser` class and passing it here as an argument.\n        :param drop_duplicated: Drop SNPs with duplicated rsIDs.\n        :param parse_kwargs: keyword arguments for the parser. These are mainly parameters that will be passed to\n        `pandas.read_csv` function, such as the delimiter, header information, etc.\n        \"\"\"\n\n        if sumstats_path is None:\n            return\n\n        if not iterable(sumstats_path):\n            sumstats_files = get_filenames(sumstats_path)\n\n            from .utils.system_utils import valid_url\n            if len(sumstats_files) &lt; 1 and valid_url(sumstats_path):\n                sumstats_files = [sumstats_path]\n        else:\n            sumstats_files = sumstats_path\n\n        if len(sumstats_files) &lt; 1:\n            logger.warning(f\"No summary statistics files were found at: {sumstats_path}\")\n            return\n\n        logger.info(\"&gt; Reading summary statistics...\")\n\n        self.sumstats_table = {}\n\n        for f in tqdm(sumstats_files,\n                      total=len(sumstats_files),\n                      desc=\"Reading summary statistics\"):\n\n            ss_tab = SumstatsTable.from_file(f,\n                                             sumstats_format=sumstats_format,\n                                             parser=parser,\n                                             **parse_kwargs)\n\n            if drop_duplicated:\n                ss_tab.drop_duplicates()\n\n            if 'CHR' in ss_tab.table.columns:\n                self.sumstats_table.update(ss_tab.split_by_chromosome())\n            else:\n                if self.genotype is not None:\n                    ref_table = {c: g.snps for c, g in self.genotype.items()}\n                elif self.ld is not None:\n                    ref_table = {c: ld.snps for c, ld in self.ld.items()}\n                else:\n                    raise ValueError(\"Cannot index summary statistics tables without chromosome information!\")\n\n                self.sumstats_table.update(ss_tab.split_by_chromosome(snps_per_chrom=ref_table))\n\n        # If SNP information is not present in the sumstats tables, try to impute it\n        # using other reference tables:\n\n        missing_snp = any('SNP' not in ss.table.columns for ss in self.sumstats_table.values())\n\n        if missing_snp and (self.genotype is not None or self.ld is not None):\n\n            ref_table = self.to_snp_table(col_subset=['CHR', 'POS', 'SNP'], per_chromosome=True)\n\n            for c, ss in self.sumstats_table.items():\n                if 'SNP' not in ss.table.columns and c in ref_table:\n                    ss.infer_snp_id(ref_table[c], allow_na=True)\n\n    def read_ld(self, ld_store_paths):\n        \"\"\"\n        Read the LD matrix files stored on-disk in Zarr array format.\n        :param ld_store_paths: The path to the LD matrices. This may be a wildcard to accommodate reading data\n        for multiple chromosomes.\n        \"\"\"\n\n        if ld_store_paths is None:\n            return\n\n        if not iterable(ld_store_paths):\n            if 's3://' in ld_store_paths:\n                from .utils.system_utils import glob_s3_path\n                ld_store_files = glob_s3_path(ld_store_paths)\n            else:\n                ld_store_files = get_filenames(ld_store_paths, extension='.zgroup')\n        else:\n            ld_store_files = ld_store_paths\n\n        if len(ld_store_files) &lt; 1:\n            logger.warning(f\"No LD matrix files were found at: {ld_store_paths}\")\n            return\n\n        logger.info(\"&gt; Reading LD metadata...\")\n\n        self.ld = {}\n\n        for f in tqdm(ld_store_files,\n                      total=len(ld_store_files),\n                      desc=\"Reading LD metadata\"):\n            z = LDMatrix.from_path(f)\n            self.ld[z.chromosome] = z\n\n    def load_ld(self):\n        \"\"\"\n        A utility method to load the LD matrices to memory from on-disk storage.\n        \"\"\"\n        if self.ld is not None:\n            for ld in self.ld.values():\n                ld.load()\n\n    def release_ld(self):\n        \"\"\"\n        A utility function to release the LD matrices from memory.\n        \"\"\"\n        if self.ld is not None:\n            for ld in self.ld.values():\n                ld.release()\n\n    def compute_ld(self,\n                   estimator,\n                   output_dir,\n                   dtype='int16',\n                   compressor_name='zstd',\n                   compression_level=7,\n                   compute_spectral_properties=False,\n                   **ld_kwargs):\n        \"\"\"\n        Compute the Linkage-Disequilibrium (LD) matrix or SNP-by-SNP Pearson\n        correlation matrix between genetic variants. This function only considers correlations\n        between SNPs on the same chromosome. This is a utility function that calls the\n        `.compute_ld()` method of the `GenotypeMatrix` objects associated with\n        GWADataLoader.\n\n        :param estimator: The estimator for the LD matrix. We currently support\n        4 different estimators: `sample`, `windowed`, `shrinkage`, and `block`.\n        :param output_dir: The output directory where the Zarr array containing the\n        entries of the LD matrix will be stored.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compression algorithm to use for the LD matrix.\n        :param compression_level: The compression level to use for the entries of the LD matrix (1-9).\n        :param compute_spectral_properties: If True, compute the spectral properties of the LD matrix.\n        :param ld_kwargs: keyword arguments for the various LD estimators. Consult\n        the implementations of `WindowedLD`, `ShrinkageLD`, and `BlockLD` for details.\n        \"\"\"\n\n        if self.genotype is None:\n            raise ValueError(\"Cannot compute LD without genotype data.\")\n\n        logger.info(\"&gt; Computing LD matrix...\")\n\n        self.ld = {\n            c: g.compute_ld(estimator,\n                            output_dir,\n                            dtype=dtype,\n                            compressor_name=compressor_name,\n                            compression_level=compression_level,\n                            compute_spectral_properties=compute_spectral_properties,\n                            **ld_kwargs)\n            for c, g in tqdm(sorted(self.genotype.items(), key=lambda x: x[0]),\n                             total=len(self.genotype),\n                             desc='Computing LD matrices')\n        }\n\n    def get_ld_matrices(self):\n        \"\"\"\n        :return: A dictionary containing the chromosome ID as key and corresponding LD matrices\n        as value.\n        \"\"\"\n        return self.ld\n\n    def harmonize_data(self):\n        \"\"\"\n        This method ensures that the data sources (reference genotype,\n        LD matrices, summary statistics, annotations) are all aligned in terms of the\n        set of variants that they operate on as well as the designation of the effect allele for\n        each variant.\n\n        !!! note\n            This method is called automatically during the initialization of the `GWADataLoader` object.\n            However, if you read or manipulate the data sources after initialization,\n            you may need to call this method again to ensure that the data sources remain aligned.\n\n        !!! warning\n            Harmonization for now depends on having SNP rsID be present in all the resources. Hopefully\n            this requirement will be relaxed in the future.\n\n        \"\"\"\n\n        data_sources = (self.genotype, self.sumstats_table, self.ld, self.annotation)\n        initialized_data_sources = [ds for ds in data_sources if ds is not None]\n\n        # If less than two data sources are present, skip harmonization...\n        if len(initialized_data_sources) &lt; 2:\n            return\n\n        # Get the chromosomes information from all the data sources:\n        chromosomes = list(set.union(*[set(ds.keys()) for ds in initialized_data_sources]))\n\n        logger.info(\"&gt; Harmonizing data...\")\n\n        for c in tqdm(chromosomes,\n                      total=len(chromosomes),\n                      desc='Harmonizing data'):\n\n            # Which initialized data sources have information for chromosome `c`\n            miss_chroms = [c not in ds for ds in initialized_data_sources]\n\n            if sum(miss_chroms) &gt; 0:\n                # If the chromosome data only exists for some data sources but not others, remove the chromosome\n                # from all data source.\n                # Is this the best way to handle the missingness? Should we just post a warning?\n                logger.debug(f\"Chromosome {c} is missing in some data sources. \"\n                             f\"Removing it from all data sources.\")\n                for ds in initialized_data_sources:\n                    if c in ds:\n                        del ds[c]\n\n            else:\n\n                # Find the set of SNPs that are shared across all data sources (exclude missing values):\n                common_snps = intersect_multiple_arrays([ds[c].snps for ds in initialized_data_sources])\n\n                # If necessary, filter the data sources to only have the common SNPs:\n                for ds in initialized_data_sources:\n                    if ds[c].n_snps != len(common_snps):\n                        ds[c].filter_snps(extract_snps=common_snps)\n\n                # Harmonize the summary statistics data with either genotype or LD reference.\n                # This procedure checks for flips in the effect allele between data sources.\n                if self.sumstats_table is not None:\n\n                    id_cols = self.sumstats_table[c].identifier_cols\n\n                    if self.genotype is not None:\n                        self.sumstats_table[c].match(self.genotype[c].get_snp_table(\n                            col_subset=id_cols + ['A1', 'A2']\n                        ))\n                    elif self.ld is not None:\n                        self.sumstats_table[c].match(self.ld[c].to_snp_table(\n                            col_subset=id_cols + ['A1', 'A2']\n                        ))\n\n                    # If during the allele matching process we discover incompatibilities,\n                    # we filter those SNPs:\n                    for ds in initialized_data_sources:\n                        if ds[c].n_snps != self.sumstats_table[c].n_snps:\n                            ds[c].filter_snps(extract_snps=self.sumstats_table[c].snps)\n\n    def perform_gwas(self, **gwa_kwargs):\n        \"\"\"\n        Perform genome-wide association testing of all variants against the phenotype.\n        This is a utility function that calls the `.perform_gwas()` method of the\n        `GenotypeMatrix` objects associated with GWADataLoader.\n\n        :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult stats.gwa.utils\n        for relevant keyword arguments for each backend.\n        \"\"\"\n\n        logger.info(\"&gt; Performing GWAS...\")\n\n        self.sumstats_table = {\n            c: g.perform_gwas(**gwa_kwargs)\n            for c, g in tqdm(sorted(self.genotype.items(), key=lambda x: x[0]),\n                             total=len(self.genotype),\n                             desc='Performing GWAS')\n        }\n\n    def score(self, beta=None, standardize_genotype=False):\n        \"\"\"\n        Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, `beta`.\n\n        :param beta: A dictionary where the keys are the chromosome numbers and the\n        values are a vector of effect sizes for each variant on that chromosome. If the\n        betas are not provided, we use the marginal betas by default (if those are available).\n        :param standardize_genotype: If True, standardize the genotype matrix before scoring.\n        \"\"\"\n\n        if beta is None:\n            try:\n                beta = {c: s.marginal_beta or s.get_snp_pseudo_corr() for c, s in self.sumstats_table.items()}\n            except Exception:\n                raise ValueError(\"To perform linear scoring, you must \"\n                                 \"provide effect size estimates (BETA)!\")\n\n        # Here, we have a very ugly way of accounting for\n        # the fact that the chromosomes may be coded differently between the genotype\n        # and the beta dictionary. Maybe we can find a better solution in the future.\n        common_chr_g, common_chr_b = match_chromosomes(self.genotype.keys(), beta.keys(), return_both=True)\n\n        if len(common_chr_g) &lt; 1:\n            raise ValueError(\"No common chromosomes found between \"\n                             \"the genotype and the effect size estimates!\")\n\n        logger.info(\"&gt; Generating polygenic scores...\")\n\n        pgs = None\n\n        for c_g, c_b in tqdm(zip(common_chr_g, common_chr_b),\n                             total=len(common_chr_g),\n                             desc='Generating polygenic scores'):\n\n            if pgs is None:\n                pgs = self.genotype[c_g].score(beta[c_b], standardize_genotype=standardize_genotype)\n            else:\n                pgs += self.genotype[c_g].score(beta[c_b], standardize_genotype=standardize_genotype)\n\n        # If we only have a single set of betas, flatten the PGS vector:\n        if len(pgs.shape) &gt; 1 and pgs.shape[1] == 1:\n            pgs = pgs.flatten()\n\n        return pgs\n\n    def predict(self, beta=None):\n        \"\"\"\n        Predict the phenotype for the genotyped samples using the provided effect size\n        estimates `beta`. For quantitative traits, this is equivalent to performing\n        linear scoring. For binary phenotypes, we transform the output using probit link function.\n\n        :param beta: A dictionary where the keys are the chromosome numbers and the\n        values are a vector of effect sizes for each variant on that chromosome. If the\n        betas are not provided, we use the marginal betas by default (if those are available).\n        \"\"\"\n\n        # Perform linear scoring:\n        pgs = self.score(beta)\n\n        if self.phenotype_likelihood == 'binomial':\n            # Apply probit link function:\n            from scipy.stats import norm\n            pgs = norm.cdf(pgs)\n\n        return pgs\n\n    def to_individual_table(self):\n        \"\"\"\n        :return: A plink-style dataframe of individual IDs, in the form of\n        Family ID (FID) and Individual ID (IID).\n        \"\"\"\n        assert self.sample_table is not None\n\n        return self.sample_table.get_individual_table()\n\n    def to_phenotype_table(self):\n        \"\"\"\n        :return: A plink-style dataframe with each individual's Family ID (FID),\n        Individual ID (IID), and phenotype value.\n        \"\"\"\n\n        assert self.sample_table is not None\n\n        return self.sample_table.get_phenotype_table()\n\n    def to_snp_table(self, col_subset=None, per_chromosome=False, resource='auto'):\n        \"\"\"\n        Get a dataframe of SNP data for all variants\n        across different chromosomes.\n\n        :param col_subset: The subset of columns to obtain.\n        :param per_chromosome: If True, returns a dictionary where the key\n        is the chromosome number and the value is the SNP table per\n        chromosome.\n        :param resource: The data source to extract the SNP table from. By default, the method\n        will try to extract the SNP table from the genotype matrix. If the genotype matrix is not\n        available, then it will try to extract the SNP information from the LD matrix or the summary\n        statistics table. Possible values: `auto`, `genotype`, `ld`, `sumstats`.\n\n        :return: A dataframe (or dictionary of dataframes) of SNP data.\n        \"\"\"\n\n        # Sanity checks:\n        assert resource in ('auto', 'genotype', 'ld', 'sumstats')\n\n        if resource != 'auto':\n            if resource == 'genotype' and self.genotype is None:\n                raise ValueError(\"Genotype matrix is not available!\")\n            if resource == 'ld' and self.ld is None:\n                raise ValueError(\"LD matrix is not available!\")\n            if resource == 'sumstats' and self.sumstats_table is None:\n                raise ValueError(\"Summary statistics table is not available!\")\n        else:\n            if all(ds is None for ds in (self.genotype, self.ld, self.sumstats_table)):\n                raise ValueError(\"No data sources available to extract SNP data from!\")\n\n        # Extract the SNP data:\n\n        snp_tables = {}\n\n        if resource in ('auto', 'genotype') and self.genotype is not None:\n            for c in self.chromosomes:\n                snp_tables[c] = self.genotype[c].get_snp_table(col_subset=col_subset)\n        elif resource in ('auto', 'ld') and self.ld is not None:\n            for c in self.chromosomes:\n                snp_tables[c] = self.ld[c].to_snp_table(col_subset=col_subset)\n        else:\n            return self.to_summary_statistics_table(col_subset=col_subset,\n                                                    per_chromosome=per_chromosome)\n\n        if per_chromosome:\n            return snp_tables\n        else:\n            return pd.concat(list(snp_tables.values()))\n\n    def to_summary_statistics_table(self, col_subset=None, per_chromosome=False):\n        \"\"\"\n        Get a dataframe of the GWAS summary statistics for all variants\n        across different chromosomes.\n\n        :param col_subset: The subset of columns (or summary statistics) to obtain.\n        :param per_chromosome: If True, returns a dictionary where the key\n        is the chromosome number and the value is the summary statistics table per\n        chromosome.\n\n        :return: A dataframe (or dictionary of dataframes) of summary statistics.\n        \"\"\"\n\n        assert self.sumstats_table is not None\n\n        snp_tables = {}\n\n        for c in self.chromosomes:\n            snp_tables[c] = self.sumstats_table[c].to_table(col_subset=col_subset)\n\n        if per_chromosome:\n            return snp_tables\n        else:\n            return pd.concat(list(snp_tables.values()))\n\n    def sync_sample_tables(self):\n        \"\"\"\n        A utility method to sync the sample tables of the\n        `GenotypeMatrix` objects with the sample table under\n        the `GWADataLoader` object. This is especially important\n        when setting new phenotypes (from the simulators) or reading\n        covariates files, etc.\n        \"\"\"\n\n        for c, g in self.genotype.items():\n            g.set_sample_table(self.sample_table)\n\n    def split_by_chromosome(self):\n        \"\"\"\n        A utility method to split a GWADataLoader object by chromosome ID, such that\n        we would have one `GWADataLoader` object per chromosome. The method returns a dictionary\n        where the key is the chromosome number and the value is the `GWADataLoader` object corresponding\n        to that chromosome only.\n        \"\"\"\n\n        if len(self.chromosomes) == 1:\n            return {self.chromosomes[0]: self}\n\n        else:\n            split_dict = {}\n\n            for c in self.chromosomes:\n                split_dict[c] = copy.copy(self)\n\n                if self.genotype is not None and c in self.genotype:\n                    split_dict[c].genotype = {c: self.genotype[c]}\n                if self.sumstats_table is not None and c in self.sumstats_table:\n                    split_dict[c].sumstats_table = {c: self.sumstats_table[c]}\n                if self.ld is not None and c in self.ld:\n                    split_dict[c].ld = {c: self.ld[c]}\n                if self.annotation is not None and c in self.annotation:\n                    split_dict[c].annotation = {c: self.annotation[c]}\n\n            return split_dict\n\n    def split_by_samples(self, proportions=None, groups=None, keep_original=True):\n        \"\"\"\n        Split the `GWADataLoader` object by samples, if genotype or sample data\n        is available. The user must provide a list or proportion of samples in each split,\n        and the method will return a list of `GWADataLoader` objects with only the samples\n        designated for each split. This may be a useful utility for training/testing split or some\n        other downstream tasks.\n\n        :param proportions: A list with the proportion of samples in each split. Must add to 1.\n        :param groups: A list of lists containing the sample IDs in each split.\n        :param keep_original: If True, keep the original `GWADataLoader` object and do not\n        transform it in the splitting process.\n        \"\"\"\n\n        if self.sample_table is None:\n            raise ValueError(\"The sample table is not set!\")\n\n        if groups is None:\n            if proportions is None:\n                raise ValueError(\"To split a `GWADataloader` object by samples, the user must provide either the list \"\n                                 \"or proportion of individuals in each split.\")\n            else:\n\n                # Assign each sample to a different split randomly by drawing from a multinomial:\n                random_split = np.random.multinomial(1, proportions, size=self.sample_size).astype(bool)\n                # Extract the individuals in each group from the multinomial sample:\n                groups = [self.samples[random_split[:, i]] for i in range(random_split.shape[1])]\n\n        gdls = []\n        for i, g in enumerate(groups):\n\n            if len(g) &lt; 1:\n                raise ValueError(f\"Group {i} is empty! Please ensure that all splits have at least one sample.\")\n\n            if (i + 1) == len(groups) and not keep_original:\n                new_gdl = self\n            else:\n                new_gdl = copy.deepcopy(self)\n\n            new_gdl.filter_samples(keep_samples=g)\n\n            gdls.append(new_gdl)\n\n        return gdls\n\n    def align_with(self, other_gdls, axis='SNP', how='inner'):\n        \"\"\"\n        Align the `GWADataLoader` object with other GDL objects to have the same\n        set of SNPs or samples. This utility method is meant to enable the user to\n        align multiple data sources for downstream analyses.\n\n        :param other_gdls: A `GWADataLoader` or list of `GWADataLoader` objects.\n        :param axis: The axis on which to perform the alignment (can be `sample` for aligning individuals or\n        `SNP` for aligning variants across the datasets).\n        :param how: The type of join to perform across the datasets. For now, we support an inner join sort\n        of operation.\n\n        !!! warning\n            Experimental for now, would like to add more features here in the near future.\n\n        \"\"\"\n\n        if isinstance(other_gdls, GWADataLoader):\n            other_gdls = [other_gdls]\n\n        assert all([isinstance(gdl, GWADataLoader) for gdl in other_gdls])\n\n        if axis == 'SNP':\n            # Ensure that all the GDLs have the same set of SNPs.\n            # This may be useful if the goal is to select a common set of variants\n            # that are shared across different datasets.\n            for c in self.chromosomes:\n                common_snps = set(self.snps[c])\n                for gdl in other_gdls:\n                    common_snps = common_snps.intersection(set(gdl.snps[c]))\n\n                common_snps = np.array(list(common_snps))\n\n                for gdl in other_gdls:\n                    gdl.filter_snps(extract_snps=common_snps, chromosome=c)\n\n                self.filter_snps(extract_snps=common_snps, chromosome=c)\n\n        elif axis == 'sample':\n            # Ensure that all the GDLs have the same set of samples.\n            # This may be useful when different GDLs have different covariates, phenotypes,\n            # or other information pertaining to the individuals.\n\n            common_samples = set(self.samples)\n\n            for gdl in other_gdls:\n                common_samples = common_samples.intersection(set(gdl.samples))\n\n            common_samples = np.array(list(common_samples))\n\n            for gdl in other_gdls:\n                gdl.filter_samples(keep_samples=common_samples)\n\n            self.filter_samples(keep_samples=common_samples)\n\n        else:\n            raise KeyError(\"Alignment axis can only be either 'SNP' or 'sample'!\")\n\n    def cleanup(self):\n        \"\"\"\n        Clean up all temporary files and directories\n        \"\"\"\n\n        logger.info(\"&gt; Cleaning up workspace.\")\n\n        for tmpdir in self.cleanup_dir_list:\n            try:\n                tmpdir.cleanup()\n            except FileNotFoundError:\n                continue\n\n        # Clean up the temporary files associated with the genotype matrices:\n        if self.genotype is not None:\n            for g in self.genotype.values():\n                g.cleanup()\n\n        # Release the LD data from memory:\n        self.release_ld()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.chromosomes","title":"<code>chromosomes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of chromosomes that were loaded to <code>GWADataLoader</code>.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.m","title":"<code>m</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_snps</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the harmonized data sources.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.n","title":"<code>n</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>sample_size</li> </ul> <p>Returns:</p> Type Description <p>The number of samples in the genotype matrix.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.n_annotations","title":"<code>n_annotations</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of annotations included in the annotation matrices.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>m</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the harmonized data sources.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.sample_size","title":"<code>sample_size</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n</li> </ul> <p>Returns:</p> Type Description <p>The number of samples in the genotype matrix.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.samples","title":"<code>samples</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of samples retained in the sample table.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.shapes","title":"<code>shapes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A dictionary where the key is the chromosome number and the value is the number of variants on that chromosome.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict</code> <p>The list of SNP rsIDs retained in each chromosome.</p>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.__init__","title":"<code>__init__(bed_files=None, phenotype_file=None, covariates_file=None, keep_samples=None, keep_file=None, extract_snps=None, extract_file=None, min_maf=None, min_mac=None, drop_duplicated=True, phenotype_likelihood='gaussian', sumstats_files=None, sumstats_format='magenpy', ld_store_files=None, annotation_files=None, annotation_format='magenpy', backend='xarray', temp_dir='temp', output_dir='output', threads=1)</code>","text":"<p>Initialize the <code>GWADataLoader</code> object with the data sources required for downstream statistical genetics analyses.</p> <p>Parameters:</p> Name Type Description Default <code>bed_files</code> <p>The path to the BED file(s). You may use a wildcard here to read files for multiple chromosomes.</p> <code>None</code> <code>phenotype_file</code> <p>The path to the phenotype file. (Default: tab-separated file with <code>FID IID phenotype</code> columns).</p> <code>None</code> <code>covariates_file</code> <p>The path to the covariates file. (Default: tab-separated file starting with the <code>FID IID ...</code> columns and followed by the covariate columns).</p> <code>None</code> <code>keep_samples</code> <p>A vector or list of sample IDs to keep when filtering the genotype matrix.</p> <code>None</code> <code>keep_file</code> <p>A path to a plink-style keep file to select a subset of individuals.</p> <code>None</code> <code>extract_snps</code> <p>A vector or list of SNP IDs to keep when filtering the genotype matrix.</p> <code>None</code> <code>extract_file</code> <p>A path to a plink-style extract file to select a subset of SNPs.</p> <code>None</code> <code>min_maf</code> <p>The minimum minor allele frequency cutoff.</p> <code>None</code> <code>min_mac</code> <p>The minimum minor allele count cutoff.</p> <code>None</code> <code>drop_duplicated</code> <p>If True, drop SNPs with duplicated rsID.</p> <code>True</code> <code>phenotype_likelihood</code> <p>The likelihood of the phenotype (e.g. <code>gaussian</code>, <code>binomial</code>).</p> <code>'gaussian'</code> <code>sumstats_files</code> <p>The path to the summary statistics file(s). The path may be a wildcard.</p> <code>None</code> <code>sumstats_format</code> <p>The format for the summary statistics. Currently, supports the following formats: <code>plink1.9</code>, <code>plink2</code>, <code>magenpy</code>, <code>fastGWA</code>, <code>COJO</code>, <code>SAIGE</code>, or <code>GWASCatalog</code> for the standard summary statistics format (also known as <code>ssf</code> or <code>gwas-ssf</code>).</p> <code>'magenpy'</code> <code>ld_store_files</code> <p>The path to the LD matrices. This may be a wildcard to accommodate reading data for multiple chromosomes.</p> <code>None</code> <code>annotation_files</code> <p>The path to the annotation file(s). The path may contain a wildcard.</p> <code>None</code> <code>annotation_format</code> <p>The format for the summary statistics. Currently, supports the following formats: <code>magenpy</code>, <code>ldsc</code>.</p> <code>'magenpy'</code> <code>backend</code> <p>The backend software used for computations with the genotype matrix. Currently, supports <code>xarray</code> and <code>plink</code>.</p> <code>'xarray'</code> <code>temp_dir</code> <p>The temporary directory where to store intermediate files.</p> <code>'temp'</code> <code>output_dir</code> <p>The output directory where to store the results of the computation.</p> <code>'output'</code> <code>threads</code> <p>The number of threads to use for computations.</p> <code>1</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def __init__(self,\n             bed_files=None,\n             phenotype_file=None,\n             covariates_file=None,\n             keep_samples=None,\n             keep_file=None,\n             extract_snps=None,\n             extract_file=None,\n             min_maf=None,\n             min_mac=None,\n             drop_duplicated=True,\n             phenotype_likelihood='gaussian',\n             sumstats_files=None,\n             sumstats_format='magenpy',\n             ld_store_files=None,\n             annotation_files=None,\n             annotation_format='magenpy',\n             backend='xarray',\n             temp_dir='temp',\n             output_dir='output',\n             threads=1):\n    \"\"\"\n    Initialize the `GWADataLoader` object with the data sources required for\n    downstream statistical genetics analyses.\n\n    :param bed_files: The path to the BED file(s). You may use a wildcard here to read files for multiple\n    chromosomes.\n    :param phenotype_file: The path to the phenotype file.\n    (Default: tab-separated file with `FID IID phenotype` columns).\n    :param covariates_file: The path to the covariates file.\n    (Default: tab-separated file starting with the `FID IID ...` columns and followed by the covariate columns).\n    :param keep_samples: A vector or list of sample IDs to keep when filtering the genotype matrix.\n    :param keep_file: A path to a plink-style keep file to select a subset of individuals.\n    :param extract_snps: A vector or list of SNP IDs to keep when filtering the genotype matrix.\n    :param extract_file: A path to a plink-style extract file to select a subset of SNPs.\n    :param min_maf: The minimum minor allele frequency cutoff.\n    :param min_mac: The minimum minor allele count cutoff.\n    :param drop_duplicated: If True, drop SNPs with duplicated rsID.\n    :param phenotype_likelihood: The likelihood of the phenotype (e.g. `gaussian`, `binomial`).\n    :param sumstats_files: The path to the summary statistics file(s). The path may be a wildcard.\n    :param sumstats_format: The format for the summary statistics. Currently, supports the following\n    formats: `plink1.9`, `plink2`, `magenpy`, `fastGWA`, `COJO`, `SAIGE`, or `GWASCatalog` for the standard\n    summary statistics format (also known as `ssf` or `gwas-ssf`).\n    :param ld_store_files: The path to the LD matrices. This may be a wildcard to accommodate reading data\n    for multiple chromosomes.\n    :param annotation_files: The path to the annotation file(s). The path may contain a wildcard.\n    :param annotation_format: The format for the summary statistics. Currently, supports the following\n    formats: `magenpy`, `ldsc`.\n    :param backend: The backend software used for computations with the genotype matrix. Currently, supports\n    `xarray` and `plink`.\n    :param temp_dir: The temporary directory where to store intermediate files.\n    :param output_dir: The output directory where to store the results of the computation.\n    :param threads: The number of threads to use for computations.\n    \"\"\"\n\n    # ------- Sanity checks -------\n\n    assert backend in ('xarray', 'plink', 'bed-reader')\n    assert phenotype_likelihood in ('gaussian', 'binomial')\n\n    # ------- General options -------\n\n    self.backend = backend\n\n    self.temp_dir = temp_dir\n    self.output_dir = output_dir\n    self.cleanup_dir_list = []  # Directories to clean up after execution.\n\n    makedir([temp_dir, output_dir])\n\n    self.threads = threads\n\n    # ------- General parameters -------\n\n    self.phenotype_likelihood: str = phenotype_likelihood\n\n    self.genotype: Union[Dict[int, GenotypeMatrix], None] = None\n    self.sample_table: Union[SampleTable, None] = None\n    self.ld: Union[Dict[int, LDMatrix], None] = None\n    self.sumstats_table: Union[Dict[int, SumstatsTable], None] = None\n    self.annotation: Union[Dict[int, AnnotationMatrix], None] = None\n\n    # ------- Read data files -------\n\n    self.read_genotypes(bed_files,\n                        min_maf=min_maf,\n                        min_mac=min_mac,\n                        drop_duplicated=drop_duplicated)\n    self.read_phenotype(phenotype_file)\n    self.read_covariates(covariates_file)\n    self.read_ld(ld_store_files)\n    self.read_annotations(annotation_files,\n                          annot_format=annotation_format)\n    self.read_summary_statistics(sumstats_files,\n                                 sumstats_format,\n                                 drop_duplicated=drop_duplicated)\n\n    # ------- Filter samples or SNPs -------\n\n    if extract_snps is not None or extract_file is not None:\n        self.filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n    if keep_samples is not None or keep_file is not None:\n        self.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n    # ------- Harmonize data sources -------\n\n    self.harmonize_data()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.align_with","title":"<code>align_with(other_gdls, axis='SNP', how='inner')</code>","text":"<p>Align the <code>GWADataLoader</code> object with other GDL objects to have the same set of SNPs or samples. This utility method is meant to enable the user to align multiple data sources for downstream analyses.</p> <p>Parameters:</p> Name Type Description Default <code>other_gdls</code> <p>A <code>GWADataLoader</code> or list of <code>GWADataLoader</code> objects.</p> required <code>axis</code> <p>The axis on which to perform the alignment (can be <code>sample</code> for aligning individuals or <code>SNP</code> for aligning variants across the datasets).</p> <code>'SNP'</code> <code>how</code> <p>The type of join to perform across the datasets. For now, we support an inner join sort of operation.  !!! warning Experimental for now, would like to add more features here in the near future.</p> <code>'inner'</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def align_with(self, other_gdls, axis='SNP', how='inner'):\n    \"\"\"\n    Align the `GWADataLoader` object with other GDL objects to have the same\n    set of SNPs or samples. This utility method is meant to enable the user to\n    align multiple data sources for downstream analyses.\n\n    :param other_gdls: A `GWADataLoader` or list of `GWADataLoader` objects.\n    :param axis: The axis on which to perform the alignment (can be `sample` for aligning individuals or\n    `SNP` for aligning variants across the datasets).\n    :param how: The type of join to perform across the datasets. For now, we support an inner join sort\n    of operation.\n\n    !!! warning\n        Experimental for now, would like to add more features here in the near future.\n\n    \"\"\"\n\n    if isinstance(other_gdls, GWADataLoader):\n        other_gdls = [other_gdls]\n\n    assert all([isinstance(gdl, GWADataLoader) for gdl in other_gdls])\n\n    if axis == 'SNP':\n        # Ensure that all the GDLs have the same set of SNPs.\n        # This may be useful if the goal is to select a common set of variants\n        # that are shared across different datasets.\n        for c in self.chromosomes:\n            common_snps = set(self.snps[c])\n            for gdl in other_gdls:\n                common_snps = common_snps.intersection(set(gdl.snps[c]))\n\n            common_snps = np.array(list(common_snps))\n\n            for gdl in other_gdls:\n                gdl.filter_snps(extract_snps=common_snps, chromosome=c)\n\n            self.filter_snps(extract_snps=common_snps, chromosome=c)\n\n    elif axis == 'sample':\n        # Ensure that all the GDLs have the same set of samples.\n        # This may be useful when different GDLs have different covariates, phenotypes,\n        # or other information pertaining to the individuals.\n\n        common_samples = set(self.samples)\n\n        for gdl in other_gdls:\n            common_samples = common_samples.intersection(set(gdl.samples))\n\n        common_samples = np.array(list(common_samples))\n\n        for gdl in other_gdls:\n            gdl.filter_samples(keep_samples=common_samples)\n\n        self.filter_samples(keep_samples=common_samples)\n\n    else:\n        raise KeyError(\"Alignment axis can only be either 'SNP' or 'sample'!\")\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up all temporary files and directories</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Clean up all temporary files and directories\n    \"\"\"\n\n    logger.info(\"&gt; Cleaning up workspace.\")\n\n    for tmpdir in self.cleanup_dir_list:\n        try:\n            tmpdir.cleanup()\n        except FileNotFoundError:\n            continue\n\n    # Clean up the temporary files associated with the genotype matrices:\n    if self.genotype is not None:\n        for g in self.genotype.values():\n            g.cleanup()\n\n    # Release the LD data from memory:\n    self.release_ld()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.compute_ld","title":"<code>compute_ld(estimator, output_dir, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False, **ld_kwargs)</code>","text":"<p>Compute the Linkage-Disequilibrium (LD) matrix or SNP-by-SNP Pearson correlation matrix between genetic variants. This function only considers correlations between SNPs on the same chromosome. This is a utility function that calls the <code>.compute_ld()</code> method of the <code>GenotypeMatrix</code> objects associated with GWADataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator for the LD matrix. We currently support 4 different estimators: <code>sample</code>, <code>windowed</code>, <code>shrinkage</code>, and <code>block</code>.</p> required <code>output_dir</code> <p>The output directory where the Zarr array containing the entries of the LD matrix will be stored.</p> required <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compression algorithm to use for the LD matrix.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the entries of the LD matrix (1-9).</p> <code>7</code> <code>compute_spectral_properties</code> <p>If True, compute the spectral properties of the LD matrix.</p> <code>False</code> <code>ld_kwargs</code> <p>keyword arguments for the various LD estimators. Consult the implementations of <code>WindowedLD</code>, <code>ShrinkageLD</code>, and <code>BlockLD</code> for details.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def compute_ld(self,\n               estimator,\n               output_dir,\n               dtype='int16',\n               compressor_name='zstd',\n               compression_level=7,\n               compute_spectral_properties=False,\n               **ld_kwargs):\n    \"\"\"\n    Compute the Linkage-Disequilibrium (LD) matrix or SNP-by-SNP Pearson\n    correlation matrix between genetic variants. This function only considers correlations\n    between SNPs on the same chromosome. This is a utility function that calls the\n    `.compute_ld()` method of the `GenotypeMatrix` objects associated with\n    GWADataLoader.\n\n    :param estimator: The estimator for the LD matrix. We currently support\n    4 different estimators: `sample`, `windowed`, `shrinkage`, and `block`.\n    :param output_dir: The output directory where the Zarr array containing the\n    entries of the LD matrix will be stored.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compression algorithm to use for the LD matrix.\n    :param compression_level: The compression level to use for the entries of the LD matrix (1-9).\n    :param compute_spectral_properties: If True, compute the spectral properties of the LD matrix.\n    :param ld_kwargs: keyword arguments for the various LD estimators. Consult\n    the implementations of `WindowedLD`, `ShrinkageLD`, and `BlockLD` for details.\n    \"\"\"\n\n    if self.genotype is None:\n        raise ValueError(\"Cannot compute LD without genotype data.\")\n\n    logger.info(\"&gt; Computing LD matrix...\")\n\n    self.ld = {\n        c: g.compute_ld(estimator,\n                        output_dir,\n                        dtype=dtype,\n                        compressor_name=compressor_name,\n                        compression_level=compression_level,\n                        compute_spectral_properties=compute_spectral_properties,\n                        **ld_kwargs)\n        for c, g in tqdm(sorted(self.genotype.items(), key=lambda x: x[0]),\n                         total=len(self.genotype),\n                         desc='Computing LD matrices')\n    }\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.filter_samples","title":"<code>filter_samples(keep_samples=None, keep_file=None)</code>","text":"<p>Filter samples from the samples table. User must specify either a list of samples to keep or the path to a file with the list of samples to keep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_samples</code> <p>A list or array of sample IDs to keep.</p> <code>None</code> <code>keep_file</code> <p>The path to a file with the list of samples to keep.</p> <code>None</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def filter_samples(self, keep_samples=None, keep_file=None):\n    \"\"\"\n    Filter samples from the samples table. User must specify\n    either a list of samples to keep or the path to a file\n    with the list of samples to keep.\n\n    :param keep_samples: A list or array of sample IDs to keep.\n    :param keep_file: The path to a file with the list of samples to keep.\n    \"\"\"\n\n    self.sample_table.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n    self.sync_sample_tables()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None, chromosome=None)</code>","text":"<p>Filter the SNP set from all the GWADataLoader objects.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list or array of SNP rsIDs to keep.</p> <code>None</code> <code>extract_file</code> <p>A path to a plink-style file with SNP rsIDs to keep.</p> <code>None</code> <code>chromosome</code> <p>Chromosome number. If specified, applies the filter to that chromosome only.</p> <code>None</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None, chromosome=None):\n    \"\"\"\n    Filter the SNP set from all the GWADataLoader objects.\n    :param extract_snps: A list or array of SNP rsIDs to keep.\n    :param extract_file: A path to a plink-style file with SNP rsIDs to keep.\n    :param chromosome: Chromosome number. If specified, applies the filter to that chromosome only.\n    \"\"\"\n\n    if extract_snps is None and extract_file is None:\n        return\n\n    if chromosome is not None:\n        chroms = [chromosome]\n    else:\n        chroms = self.chromosomes\n\n    if extract_snps is None:\n        from .parsers.misc_parsers import read_snp_filter_file\n        extract_snps = read_snp_filter_file(extract_file)\n\n    for c in chroms:\n\n        # Filter the genotype matrix:\n        if self.genotype is not None and c in self.genotype:\n            self.genotype[c].filter_snps(extract_snps=extract_snps)\n\n            # If no SNPs remain in the genotype matrix for that chromosome, then remove it:\n            if self.genotype[c].shape[1] &lt; 1:\n                del self.genotype[c]\n\n        # Filter the summary statistics table:\n        if self.sumstats_table is not None and c in self.sumstats_table:\n            self.sumstats_table[c].filter_snps(extract_snps=extract_snps)\n\n            # If no SNPs remain in the summary statistics table for that chromosome, then remove it:\n            if self.sumstats_table[c].shape[0] &lt; 1:\n                del self.sumstats_table[c]\n\n        if self.ld is not None and c in self.ld:\n            self.ld[c].filter_snps(extract_snps=extract_snps)\n\n            # If no SNPs remain in the summary statistics table for that chromosome, then remove it:\n            if self.ld[c].n_snps &lt; 1:\n                del self.ld[c]\n\n        # Filter the annotation matrix:\n        if self.annotation is not None and c in self.annotation:\n            self.annotation[c].filter_snps(extract_snps=extract_snps)\n\n            if self.annotation[c].shape[0] &lt; 1:\n                del self.annotation[c]\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.get_ld_matrices","title":"<code>get_ld_matrices()</code>","text":"<p>Returns:</p> Type Description <p>A dictionary containing the chromosome ID as key and corresponding LD matrices as value.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def get_ld_matrices(self):\n    \"\"\"\n    :return: A dictionary containing the chromosome ID as key and corresponding LD matrices\n    as value.\n    \"\"\"\n    return self.ld\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.harmonize_data","title":"<code>harmonize_data()</code>","text":"<p>This method ensures that the data sources (reference genotype, LD matrices, summary statistics, annotations) are all aligned in terms of the set of variants that they operate on as well as the designation of the effect allele for each variant.</p> <p>Note</p> <p>This method is called automatically during the initialization of the <code>GWADataLoader</code> object. However, if you read or manipulate the data sources after initialization, you may need to call this method again to ensure that the data sources remain aligned.</p> <p>Warning</p> <p>Harmonization for now depends on having SNP rsID be present in all the resources. Hopefully this requirement will be relaxed in the future.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def harmonize_data(self):\n    \"\"\"\n    This method ensures that the data sources (reference genotype,\n    LD matrices, summary statistics, annotations) are all aligned in terms of the\n    set of variants that they operate on as well as the designation of the effect allele for\n    each variant.\n\n    !!! note\n        This method is called automatically during the initialization of the `GWADataLoader` object.\n        However, if you read or manipulate the data sources after initialization,\n        you may need to call this method again to ensure that the data sources remain aligned.\n\n    !!! warning\n        Harmonization for now depends on having SNP rsID be present in all the resources. Hopefully\n        this requirement will be relaxed in the future.\n\n    \"\"\"\n\n    data_sources = (self.genotype, self.sumstats_table, self.ld, self.annotation)\n    initialized_data_sources = [ds for ds in data_sources if ds is not None]\n\n    # If less than two data sources are present, skip harmonization...\n    if len(initialized_data_sources) &lt; 2:\n        return\n\n    # Get the chromosomes information from all the data sources:\n    chromosomes = list(set.union(*[set(ds.keys()) for ds in initialized_data_sources]))\n\n    logger.info(\"&gt; Harmonizing data...\")\n\n    for c in tqdm(chromosomes,\n                  total=len(chromosomes),\n                  desc='Harmonizing data'):\n\n        # Which initialized data sources have information for chromosome `c`\n        miss_chroms = [c not in ds for ds in initialized_data_sources]\n\n        if sum(miss_chroms) &gt; 0:\n            # If the chromosome data only exists for some data sources but not others, remove the chromosome\n            # from all data source.\n            # Is this the best way to handle the missingness? Should we just post a warning?\n            logger.debug(f\"Chromosome {c} is missing in some data sources. \"\n                         f\"Removing it from all data sources.\")\n            for ds in initialized_data_sources:\n                if c in ds:\n                    del ds[c]\n\n        else:\n\n            # Find the set of SNPs that are shared across all data sources (exclude missing values):\n            common_snps = intersect_multiple_arrays([ds[c].snps for ds in initialized_data_sources])\n\n            # If necessary, filter the data sources to only have the common SNPs:\n            for ds in initialized_data_sources:\n                if ds[c].n_snps != len(common_snps):\n                    ds[c].filter_snps(extract_snps=common_snps)\n\n            # Harmonize the summary statistics data with either genotype or LD reference.\n            # This procedure checks for flips in the effect allele between data sources.\n            if self.sumstats_table is not None:\n\n                id_cols = self.sumstats_table[c].identifier_cols\n\n                if self.genotype is not None:\n                    self.sumstats_table[c].match(self.genotype[c].get_snp_table(\n                        col_subset=id_cols + ['A1', 'A2']\n                    ))\n                elif self.ld is not None:\n                    self.sumstats_table[c].match(self.ld[c].to_snp_table(\n                        col_subset=id_cols + ['A1', 'A2']\n                    ))\n\n                # If during the allele matching process we discover incompatibilities,\n                # we filter those SNPs:\n                for ds in initialized_data_sources:\n                    if ds[c].n_snps != self.sumstats_table[c].n_snps:\n                        ds[c].filter_snps(extract_snps=self.sumstats_table[c].snps)\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.load_ld","title":"<code>load_ld()</code>","text":"<p>A utility method to load the LD matrices to memory from on-disk storage.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def load_ld(self):\n    \"\"\"\n    A utility method to load the LD matrices to memory from on-disk storage.\n    \"\"\"\n    if self.ld is not None:\n        for ld in self.ld.values():\n            ld.load()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.perform_gwas","title":"<code>perform_gwas(**gwa_kwargs)</code>","text":"<p>Perform genome-wide association testing of all variants against the phenotype. This is a utility function that calls the <code>.perform_gwas()</code> method of the <code>GenotypeMatrix</code> objects associated with GWADataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>gwa_kwargs</code> <p>Keyword arguments to pass to the GWA functions. Consult stats.gwa.utils for relevant keyword arguments for each backend.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def perform_gwas(self, **gwa_kwargs):\n    \"\"\"\n    Perform genome-wide association testing of all variants against the phenotype.\n    This is a utility function that calls the `.perform_gwas()` method of the\n    `GenotypeMatrix` objects associated with GWADataLoader.\n\n    :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult stats.gwa.utils\n    for relevant keyword arguments for each backend.\n    \"\"\"\n\n    logger.info(\"&gt; Performing GWAS...\")\n\n    self.sumstats_table = {\n        c: g.perform_gwas(**gwa_kwargs)\n        for c, g in tqdm(sorted(self.genotype.items(), key=lambda x: x[0]),\n                         total=len(self.genotype),\n                         desc='Performing GWAS')\n    }\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.predict","title":"<code>predict(beta=None)</code>","text":"<p>Predict the phenotype for the genotyped samples using the provided effect size estimates <code>beta</code>. For quantitative traits, this is equivalent to performing linear scoring. For binary phenotypes, we transform the output using probit link function.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A dictionary where the keys are the chromosome numbers and the values are a vector of effect sizes for each variant on that chromosome. If the betas are not provided, we use the marginal betas by default (if those are available).</p> <code>None</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def predict(self, beta=None):\n    \"\"\"\n    Predict the phenotype for the genotyped samples using the provided effect size\n    estimates `beta`. For quantitative traits, this is equivalent to performing\n    linear scoring. For binary phenotypes, we transform the output using probit link function.\n\n    :param beta: A dictionary where the keys are the chromosome numbers and the\n    values are a vector of effect sizes for each variant on that chromosome. If the\n    betas are not provided, we use the marginal betas by default (if those are available).\n    \"\"\"\n\n    # Perform linear scoring:\n    pgs = self.score(beta)\n\n    if self.phenotype_likelihood == 'binomial':\n        # Apply probit link function:\n        from scipy.stats import norm\n        pgs = norm.cdf(pgs)\n\n    return pgs\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_annotations","title":"<code>read_annotations(annot_path, annot_format='magenpy', parser=None, **parse_kwargs)</code>","text":"<p>Read the annotation matrix from file. Annotations are a set of features associated with each SNP and are generally represented in table format. Consult the documentation for <code>AnnotationMatrix</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>annot_path</code> <p>The path to the annotation file(s). The path may contain a wildcard.</p> required <code>annot_format</code> <p>The format for the summary statistics. Currently, supports the following formats: <code>magenpy</code>, <code>ldsc</code>.</p> <code>'magenpy'</code> <code>parser</code> <p>If the annotation file does not follow any of the formats above, you can create your own parser by inheriting from the base <code>AnnotationMatrixParser</code> class and passing it here as an argument.</p> <code>None</code> <code>parse_kwargs</code> <p>keyword arguments for the parser. These are mainly parameters that will be passed to <code>pandas.read_csv</code> function, such as the delimiter, header information, etc.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_annotations(self, annot_path,\n                     annot_format='magenpy',\n                     parser=None,\n                     **parse_kwargs):\n    \"\"\"\n    Read the annotation matrix from file. Annotations are a set of features associated\n    with each SNP and are generally represented in table format.\n    Consult the documentation for `AnnotationMatrix` for more details.\n\n    :param annot_path: The path to the annotation file(s). The path may contain a wildcard.\n    :param annot_format: The format for the summary statistics. Currently, supports the following\n     formats: `magenpy`, `ldsc`.\n    :param parser: If the annotation file does not follow any of the formats above, you can create\n    your own parser by inheriting from the base `AnnotationMatrixParser` class and passing it here as an argument.\n    :param parse_kwargs: keyword arguments for the parser. These are mainly parameters that will be passed to\n    `pandas.read_csv` function, such as the delimiter, header information, etc.\n    \"\"\"\n\n    if annot_path is None:\n        return\n\n    # Find all the relevant files in the path passed by the user:\n    if not iterable(annot_path):\n        annot_files = get_filenames(annot_path, extension='.annot')\n    else:\n        annot_files = annot_path\n\n    if len(annot_files) &lt; 1:\n        logger.warning(f\"No annotation files were found at: {annot_path}\")\n        return\n\n    logger.info(\"&gt; Reading annotation file...\")\n\n    self.annotation = {}\n\n    for annot_file in tqdm(annot_files,\n                           total=len(annot_files),\n                           desc=\"Reading annotation files\"):\n        annot_mat = AnnotationMatrix.from_file(annot_file,\n                                               annot_format=annot_format,\n                                               annot_parser=parser,\n                                               **parse_kwargs)\n        self.annotation[annot_mat.chromosome] = annot_mat\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_covariates","title":"<code>read_covariates(covariates_file, **read_csv_kwargs)</code>","text":"<p>Read the covariates file and integrate it with the sample tables and genotype matrices.</p> <p>Parameters:</p> Name Type Description Default <code>covariates_file</code> <p>The path to the covariates file (Default: tab-separated file starting with the <code>FID IID ...</code> columns and followed by the covariate columns).</p> required <code>read_csv_kwargs</code> <p>keyword arguments for the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_covariates(self, covariates_file, **read_csv_kwargs):\n    \"\"\"\n    Read the covariates file and integrate it with the sample tables and genotype matrices.\n\n    :param covariates_file: The path to the covariates file\n    (Default: tab-separated file starting with the `FID IID ...` columns and followed by the covariate columns).\n    :param read_csv_kwargs: keyword arguments for the `read_csv` function of `pandas`.\n    \"\"\"\n\n    if covariates_file is None:\n        return\n\n    logger.info(\"&gt; Reading covariates file...\")\n\n    assert self.sample_table is not None\n\n    self.sample_table.read_covariates_file(covariates_file, **read_csv_kwargs)\n    self.sync_sample_tables()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_genotypes","title":"<code>read_genotypes(bed_paths, keep_samples=None, keep_file=None, extract_snps=None, extract_file=None, min_maf=None, min_mac=1, drop_duplicated=True)</code>","text":"<p>Read the genotype matrix and/or associated metadata from plink's BED file format. Consult the documentation for <code>GenotypeMatrix</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>bed_paths</code> <p>The path to the BED file(s). You may use a wildcard here to read files for multiple chromosomes.</p> required <code>keep_samples</code> <p>A vector or list of sample IDs to keep when filtering the genotype matrix.</p> <code>None</code> <code>keep_file</code> <p>A path to a plink-style file containing sample IDs to keep.</p> <code>None</code> <code>extract_snps</code> <p>A vector or list of SNP IDs to keep when filtering the genotype matrix.</p> <code>None</code> <code>extract_file</code> <p>A path to a plink-style file containing SNP IDs to keep.</p> <code>None</code> <code>min_maf</code> <p>The minimum minor allele frequency cutoff.</p> <code>None</code> <code>min_mac</code> <p>The minimum minor allele count cutoff.</p> <code>1</code> <code>drop_duplicated</code> <p>If True, drop SNPs with duplicated rsID.</p> <code>True</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_genotypes(self,\n                   bed_paths,\n                   keep_samples=None,\n                   keep_file=None,\n                   extract_snps=None,\n                   extract_file=None,\n                   min_maf=None,\n                   min_mac=1,\n                   drop_duplicated=True):\n    \"\"\"\n    Read the genotype matrix and/or associated metadata from plink's BED file format.\n    Consult the documentation for `GenotypeMatrix` for more details.\n\n    :param bed_paths: The path to the BED file(s). You may use a wildcard here to read files for multiple\n    chromosomes.\n    :param keep_samples: A vector or list of sample IDs to keep when filtering the genotype matrix.\n    :param keep_file: A path to a plink-style file containing sample IDs to keep.\n    :param extract_snps: A vector or list of SNP IDs to keep when filtering the genotype matrix.\n    :param extract_file: A path to a plink-style file containing SNP IDs to keep.\n    :param min_maf: The minimum minor allele frequency cutoff.\n    :param min_mac: The minimum minor allele count cutoff.\n    :param drop_duplicated: If True, drop SNPs with duplicated rsID.\n    \"\"\"\n\n    if bed_paths is None:\n        return\n\n    # Find all the relevant files in the path passed by the user:\n    if not iterable(bed_paths):\n        bed_files = get_filenames(bed_paths, extension='.bed')\n    else:\n        bed_files = bed_paths\n\n    if len(bed_files) &lt; 1:\n        logger.warning(f\"No BED files were found at: {bed_paths}\")\n        return\n\n    # Depending on the backend, select the `GenotypeMatrix` class:\n    if self.backend == 'xarray':\n        gmat_class = xarrayGenotypeMatrix\n    elif self.backend == 'bed-reader':\n        gmat_class = bedReaderGenotypeMatrix\n    else:\n        gmat_class = plinkBEDGenotypeMatrix\n\n    logger.info(\"&gt; Reading genotype metadata...\")\n\n    self.genotype = {}\n\n    for bfile in tqdm(bed_files,\n                      total=len(bed_files),\n                      desc=\"Reading genotype metadata\"):\n        # Read BED file and update the genotypes dictionary:\n        self.genotype.update(gmat_class.from_file(bfile,\n                                                  temp_dir=self.temp_dir,\n                                                  threads=self.threads).split_by_chromosome())\n\n    # After reading the genotype matrices, apply some standard filters:\n    for i, (c, g) in enumerate(self.genotype.items()):\n\n        # Filter the genotype matrix to keep certain subsample:\n        if keep_samples or keep_file:\n            g.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n        # Filter the genotype matrix to keep certain SNPs\n        if extract_snps or extract_file:\n            g.filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n        # Drop duplicated SNP IDs\n        if drop_duplicated:\n            g.drop_duplicated_snps()\n\n        # Filter SNPs by minor allele frequency and/or count:\n        g.filter_by_allele_frequency(min_maf=min_maf, min_mac=min_mac)\n\n        if i == 0:\n            self.sample_table = g.sample_table\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_ld","title":"<code>read_ld(ld_store_paths)</code>","text":"<p>Read the LD matrix files stored on-disk in Zarr array format.</p> <p>Parameters:</p> Name Type Description Default <code>ld_store_paths</code> <p>The path to the LD matrices. This may be a wildcard to accommodate reading data for multiple chromosomes.</p> required Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_ld(self, ld_store_paths):\n    \"\"\"\n    Read the LD matrix files stored on-disk in Zarr array format.\n    :param ld_store_paths: The path to the LD matrices. This may be a wildcard to accommodate reading data\n    for multiple chromosomes.\n    \"\"\"\n\n    if ld_store_paths is None:\n        return\n\n    if not iterable(ld_store_paths):\n        if 's3://' in ld_store_paths:\n            from .utils.system_utils import glob_s3_path\n            ld_store_files = glob_s3_path(ld_store_paths)\n        else:\n            ld_store_files = get_filenames(ld_store_paths, extension='.zgroup')\n    else:\n        ld_store_files = ld_store_paths\n\n    if len(ld_store_files) &lt; 1:\n        logger.warning(f\"No LD matrix files were found at: {ld_store_paths}\")\n        return\n\n    logger.info(\"&gt; Reading LD metadata...\")\n\n    self.ld = {}\n\n    for f in tqdm(ld_store_files,\n                  total=len(ld_store_files),\n                  desc=\"Reading LD metadata\"):\n        z = LDMatrix.from_path(f)\n        self.ld[z.chromosome] = z\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_phenotype","title":"<code>read_phenotype(phenotype_file, drop_na=True, **read_csv_kwargs)</code>","text":"<p>Read the phenotype file and integrate it with the sample tables and genotype matrices.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype_file</code> <p>The path to the phenotype file (Default: tab-separated file with <code>FID IID phenotype</code> columns). If different, supply details as additional arguments to this function.</p> required <code>drop_na</code> <p>Drop samples with missing phenotype information.</p> <code>True</code> <code>read_csv_kwargs</code> <p>keyword arguments for the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_phenotype(self, phenotype_file, drop_na=True, **read_csv_kwargs):\n    \"\"\"\n    Read the phenotype file and integrate it with the sample tables and genotype matrices.\n\n    :param phenotype_file: The path to the phenotype file\n    (Default: tab-separated file with `FID IID phenotype` columns). If different, supply\n    details as additional arguments to this function.\n    :param drop_na: Drop samples with missing phenotype information.\n    :param read_csv_kwargs: keyword arguments for the `read_csv` function of `pandas`.\n    \"\"\"\n\n    if phenotype_file is None:\n        return\n\n    logger.info(\"&gt; Reading phenotype file...\")\n\n    assert self.sample_table is not None\n\n    self.sample_table.read_phenotype_file(phenotype_file, drop_na=drop_na, **read_csv_kwargs)\n    self.sync_sample_tables()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.read_summary_statistics","title":"<code>read_summary_statistics(sumstats_path, sumstats_format='magenpy', parser=None, drop_duplicated=True, **parse_kwargs)</code>","text":"<p>Read GWAS summary statistics file(s) and parse them to <code>SumstatsTable</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats_path</code> <p>The path to the summary statistics file(s). The path may be a wildcard.</p> required <code>sumstats_format</code> <p>The format for the summary statistics. Currently supports the following formats: <code>plink1.9</code>, <code>plink2</code>, <code>magenpy</code>, <code>fastGWA</code>, <code>COJO</code>, <code>SAIGE</code>, or <code>GWASCatalog</code> for the standard summary statistics format (also known as <code>ssf</code> or <code>gwas-ssf</code>).</p> <code>'magenpy'</code> <code>parser</code> <p>If the summary statistics file does not follow any of the formats above, you can create your own parser by inheriting from the base <code>SumstatsParser</code> class and passing it here as an argument.</p> <code>None</code> <code>drop_duplicated</code> <p>Drop SNPs with duplicated rsIDs.</p> <code>True</code> <code>parse_kwargs</code> <p>keyword arguments for the parser. These are mainly parameters that will be passed to <code>pandas.read_csv</code> function, such as the delimiter, header information, etc.</p> <code>{}</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def read_summary_statistics(self,\n                            sumstats_path,\n                            sumstats_format='magenpy',\n                            parser=None,\n                            drop_duplicated=True,\n                            **parse_kwargs):\n    \"\"\"\n    Read GWAS summary statistics file(s) and parse them to `SumstatsTable` objects.\n\n    :param sumstats_path: The path to the summary statistics file(s). The path may be a wildcard.\n    :param sumstats_format: The format for the summary statistics. Currently supports the following\n     formats: `plink1.9`, `plink2`, `magenpy`, `fastGWA`, `COJO`, `SAIGE`, or `GWASCatalog` for the standard\n     summary statistics format (also known as `ssf` or `gwas-ssf`).\n    :param parser: If the summary statistics file does not follow any of the formats above, you can create\n    your own parser by inheriting from the base `SumstatsParser` class and passing it here as an argument.\n    :param drop_duplicated: Drop SNPs with duplicated rsIDs.\n    :param parse_kwargs: keyword arguments for the parser. These are mainly parameters that will be passed to\n    `pandas.read_csv` function, such as the delimiter, header information, etc.\n    \"\"\"\n\n    if sumstats_path is None:\n        return\n\n    if not iterable(sumstats_path):\n        sumstats_files = get_filenames(sumstats_path)\n\n        from .utils.system_utils import valid_url\n        if len(sumstats_files) &lt; 1 and valid_url(sumstats_path):\n            sumstats_files = [sumstats_path]\n    else:\n        sumstats_files = sumstats_path\n\n    if len(sumstats_files) &lt; 1:\n        logger.warning(f\"No summary statistics files were found at: {sumstats_path}\")\n        return\n\n    logger.info(\"&gt; Reading summary statistics...\")\n\n    self.sumstats_table = {}\n\n    for f in tqdm(sumstats_files,\n                  total=len(sumstats_files),\n                  desc=\"Reading summary statistics\"):\n\n        ss_tab = SumstatsTable.from_file(f,\n                                         sumstats_format=sumstats_format,\n                                         parser=parser,\n                                         **parse_kwargs)\n\n        if drop_duplicated:\n            ss_tab.drop_duplicates()\n\n        if 'CHR' in ss_tab.table.columns:\n            self.sumstats_table.update(ss_tab.split_by_chromosome())\n        else:\n            if self.genotype is not None:\n                ref_table = {c: g.snps for c, g in self.genotype.items()}\n            elif self.ld is not None:\n                ref_table = {c: ld.snps for c, ld in self.ld.items()}\n            else:\n                raise ValueError(\"Cannot index summary statistics tables without chromosome information!\")\n\n            self.sumstats_table.update(ss_tab.split_by_chromosome(snps_per_chrom=ref_table))\n\n    # If SNP information is not present in the sumstats tables, try to impute it\n    # using other reference tables:\n\n    missing_snp = any('SNP' not in ss.table.columns for ss in self.sumstats_table.values())\n\n    if missing_snp and (self.genotype is not None or self.ld is not None):\n\n        ref_table = self.to_snp_table(col_subset=['CHR', 'POS', 'SNP'], per_chromosome=True)\n\n        for c, ss in self.sumstats_table.items():\n            if 'SNP' not in ss.table.columns and c in ref_table:\n                ss.infer_snp_id(ref_table[c], allow_na=True)\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.release_ld","title":"<code>release_ld()</code>","text":"<p>A utility function to release the LD matrices from memory.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def release_ld(self):\n    \"\"\"\n    A utility function to release the LD matrices from memory.\n    \"\"\"\n    if self.ld is not None:\n        for ld in self.ld.values():\n            ld.release()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.score","title":"<code>score(beta=None, standardize_genotype=False)</code>","text":"<p>Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, <code>beta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A dictionary where the keys are the chromosome numbers and the values are a vector of effect sizes for each variant on that chromosome. If the betas are not provided, we use the marginal betas by default (if those are available).</p> <code>None</code> <code>standardize_genotype</code> <p>If True, standardize the genotype matrix before scoring.</p> <code>False</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def score(self, beta=None, standardize_genotype=False):\n    \"\"\"\n    Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, `beta`.\n\n    :param beta: A dictionary where the keys are the chromosome numbers and the\n    values are a vector of effect sizes for each variant on that chromosome. If the\n    betas are not provided, we use the marginal betas by default (if those are available).\n    :param standardize_genotype: If True, standardize the genotype matrix before scoring.\n    \"\"\"\n\n    if beta is None:\n        try:\n            beta = {c: s.marginal_beta or s.get_snp_pseudo_corr() for c, s in self.sumstats_table.items()}\n        except Exception:\n            raise ValueError(\"To perform linear scoring, you must \"\n                             \"provide effect size estimates (BETA)!\")\n\n    # Here, we have a very ugly way of accounting for\n    # the fact that the chromosomes may be coded differently between the genotype\n    # and the beta dictionary. Maybe we can find a better solution in the future.\n    common_chr_g, common_chr_b = match_chromosomes(self.genotype.keys(), beta.keys(), return_both=True)\n\n    if len(common_chr_g) &lt; 1:\n        raise ValueError(\"No common chromosomes found between \"\n                         \"the genotype and the effect size estimates!\")\n\n    logger.info(\"&gt; Generating polygenic scores...\")\n\n    pgs = None\n\n    for c_g, c_b in tqdm(zip(common_chr_g, common_chr_b),\n                         total=len(common_chr_g),\n                         desc='Generating polygenic scores'):\n\n        if pgs is None:\n            pgs = self.genotype[c_g].score(beta[c_b], standardize_genotype=standardize_genotype)\n        else:\n            pgs += self.genotype[c_g].score(beta[c_b], standardize_genotype=standardize_genotype)\n\n    # If we only have a single set of betas, flatten the PGS vector:\n    if len(pgs.shape) &gt; 1 and pgs.shape[1] == 1:\n        pgs = pgs.flatten()\n\n    return pgs\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.set_phenotype","title":"<code>set_phenotype(new_phenotype, phenotype_likelihood=None)</code>","text":"<p>A convenience method to update the phenotype column for the samples.</p> <p>Parameters:</p> Name Type Description Default <code>new_phenotype</code> <p>A vector or list of phenotype values.</p> required <code>phenotype_likelihood</code> <p>The phenotype likelihood (e.g. <code>binomial</code>, <code>gaussian</code>). Optional.</p> <code>None</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def set_phenotype(self, new_phenotype, phenotype_likelihood=None):\n    \"\"\"\n    A convenience method to update the phenotype column for the samples.\n    :param new_phenotype: A vector or list of phenotype values.\n    :param phenotype_likelihood: The phenotype likelihood (e.g. `binomial`, `gaussian`). Optional.\n    \"\"\"\n\n    self.sample_table.set_phenotype(new_phenotype,\n                                    phenotype_likelihood=phenotype_likelihood or self.phenotype_likelihood)\n    self.sync_sample_tables()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.split_by_chromosome","title":"<code>split_by_chromosome()</code>","text":"<p>A utility method to split a GWADataLoader object by chromosome ID, such that we would have one <code>GWADataLoader</code> object per chromosome. The method returns a dictionary where the key is the chromosome number and the value is the <code>GWADataLoader</code> object corresponding to that chromosome only.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def split_by_chromosome(self):\n    \"\"\"\n    A utility method to split a GWADataLoader object by chromosome ID, such that\n    we would have one `GWADataLoader` object per chromosome. The method returns a dictionary\n    where the key is the chromosome number and the value is the `GWADataLoader` object corresponding\n    to that chromosome only.\n    \"\"\"\n\n    if len(self.chromosomes) == 1:\n        return {self.chromosomes[0]: self}\n\n    else:\n        split_dict = {}\n\n        for c in self.chromosomes:\n            split_dict[c] = copy.copy(self)\n\n            if self.genotype is not None and c in self.genotype:\n                split_dict[c].genotype = {c: self.genotype[c]}\n            if self.sumstats_table is not None and c in self.sumstats_table:\n                split_dict[c].sumstats_table = {c: self.sumstats_table[c]}\n            if self.ld is not None and c in self.ld:\n                split_dict[c].ld = {c: self.ld[c]}\n            if self.annotation is not None and c in self.annotation:\n                split_dict[c].annotation = {c: self.annotation[c]}\n\n        return split_dict\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.split_by_samples","title":"<code>split_by_samples(proportions=None, groups=None, keep_original=True)</code>","text":"<p>Split the <code>GWADataLoader</code> object by samples, if genotype or sample data is available. The user must provide a list or proportion of samples in each split, and the method will return a list of <code>GWADataLoader</code> objects with only the samples designated for each split. This may be a useful utility for training/testing split or some other downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>proportions</code> <p>A list with the proportion of samples in each split. Must add to 1.</p> <code>None</code> <code>groups</code> <p>A list of lists containing the sample IDs in each split.</p> <code>None</code> <code>keep_original</code> <p>If True, keep the original <code>GWADataLoader</code> object and do not transform it in the splitting process.</p> <code>True</code> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def split_by_samples(self, proportions=None, groups=None, keep_original=True):\n    \"\"\"\n    Split the `GWADataLoader` object by samples, if genotype or sample data\n    is available. The user must provide a list or proportion of samples in each split,\n    and the method will return a list of `GWADataLoader` objects with only the samples\n    designated for each split. This may be a useful utility for training/testing split or some\n    other downstream tasks.\n\n    :param proportions: A list with the proportion of samples in each split. Must add to 1.\n    :param groups: A list of lists containing the sample IDs in each split.\n    :param keep_original: If True, keep the original `GWADataLoader` object and do not\n    transform it in the splitting process.\n    \"\"\"\n\n    if self.sample_table is None:\n        raise ValueError(\"The sample table is not set!\")\n\n    if groups is None:\n        if proportions is None:\n            raise ValueError(\"To split a `GWADataloader` object by samples, the user must provide either the list \"\n                             \"or proportion of individuals in each split.\")\n        else:\n\n            # Assign each sample to a different split randomly by drawing from a multinomial:\n            random_split = np.random.multinomial(1, proportions, size=self.sample_size).astype(bool)\n            # Extract the individuals in each group from the multinomial sample:\n            groups = [self.samples[random_split[:, i]] for i in range(random_split.shape[1])]\n\n    gdls = []\n    for i, g in enumerate(groups):\n\n        if len(g) &lt; 1:\n            raise ValueError(f\"Group {i} is empty! Please ensure that all splits have at least one sample.\")\n\n        if (i + 1) == len(groups) and not keep_original:\n            new_gdl = self\n        else:\n            new_gdl = copy.deepcopy(self)\n\n        new_gdl.filter_samples(keep_samples=g)\n\n        gdls.append(new_gdl)\n\n    return gdls\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.sync_sample_tables","title":"<code>sync_sample_tables()</code>","text":"<p>A utility method to sync the sample tables of the <code>GenotypeMatrix</code> objects with the sample table under the <code>GWADataLoader</code> object. This is especially important when setting new phenotypes (from the simulators) or reading covariates files, etc.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def sync_sample_tables(self):\n    \"\"\"\n    A utility method to sync the sample tables of the\n    `GenotypeMatrix` objects with the sample table under\n    the `GWADataLoader` object. This is especially important\n    when setting new phenotypes (from the simulators) or reading\n    covariates files, etc.\n    \"\"\"\n\n    for c, g in self.genotype.items():\n        g.set_sample_table(self.sample_table)\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.to_individual_table","title":"<code>to_individual_table()</code>","text":"<p>Returns:</p> Type Description <p>A plink-style dataframe of individual IDs, in the form of Family ID (FID) and Individual ID (IID).</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def to_individual_table(self):\n    \"\"\"\n    :return: A plink-style dataframe of individual IDs, in the form of\n    Family ID (FID) and Individual ID (IID).\n    \"\"\"\n    assert self.sample_table is not None\n\n    return self.sample_table.get_individual_table()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.to_phenotype_table","title":"<code>to_phenotype_table()</code>","text":"<p>Returns:</p> Type Description <p>A plink-style dataframe with each individual's Family ID (FID), Individual ID (IID), and phenotype value.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def to_phenotype_table(self):\n    \"\"\"\n    :return: A plink-style dataframe with each individual's Family ID (FID),\n    Individual ID (IID), and phenotype value.\n    \"\"\"\n\n    assert self.sample_table is not None\n\n    return self.sample_table.get_phenotype_table()\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.to_snp_table","title":"<code>to_snp_table(col_subset=None, per_chromosome=False, resource='auto')</code>","text":"<p>Get a dataframe of SNP data for all variants across different chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>The subset of columns to obtain.</p> <code>None</code> <code>per_chromosome</code> <p>If True, returns a dictionary where the key is the chromosome number and the value is the SNP table per chromosome.</p> <code>False</code> <code>resource</code> <p>The data source to extract the SNP table from. By default, the method will try to extract the SNP table from the genotype matrix. If the genotype matrix is not available, then it will try to extract the SNP information from the LD matrix or the summary statistics table. Possible values: <code>auto</code>, <code>genotype</code>, <code>ld</code>, <code>sumstats</code>.</p> <code>'auto'</code> <p>Returns:</p> Type Description <p>A dataframe (or dictionary of dataframes) of SNP data.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def to_snp_table(self, col_subset=None, per_chromosome=False, resource='auto'):\n    \"\"\"\n    Get a dataframe of SNP data for all variants\n    across different chromosomes.\n\n    :param col_subset: The subset of columns to obtain.\n    :param per_chromosome: If True, returns a dictionary where the key\n    is the chromosome number and the value is the SNP table per\n    chromosome.\n    :param resource: The data source to extract the SNP table from. By default, the method\n    will try to extract the SNP table from the genotype matrix. If the genotype matrix is not\n    available, then it will try to extract the SNP information from the LD matrix or the summary\n    statistics table. Possible values: `auto`, `genotype`, `ld`, `sumstats`.\n\n    :return: A dataframe (or dictionary of dataframes) of SNP data.\n    \"\"\"\n\n    # Sanity checks:\n    assert resource in ('auto', 'genotype', 'ld', 'sumstats')\n\n    if resource != 'auto':\n        if resource == 'genotype' and self.genotype is None:\n            raise ValueError(\"Genotype matrix is not available!\")\n        if resource == 'ld' and self.ld is None:\n            raise ValueError(\"LD matrix is not available!\")\n        if resource == 'sumstats' and self.sumstats_table is None:\n            raise ValueError(\"Summary statistics table is not available!\")\n    else:\n        if all(ds is None for ds in (self.genotype, self.ld, self.sumstats_table)):\n            raise ValueError(\"No data sources available to extract SNP data from!\")\n\n    # Extract the SNP data:\n\n    snp_tables = {}\n\n    if resource in ('auto', 'genotype') and self.genotype is not None:\n        for c in self.chromosomes:\n            snp_tables[c] = self.genotype[c].get_snp_table(col_subset=col_subset)\n    elif resource in ('auto', 'ld') and self.ld is not None:\n        for c in self.chromosomes:\n            snp_tables[c] = self.ld[c].to_snp_table(col_subset=col_subset)\n    else:\n        return self.to_summary_statistics_table(col_subset=col_subset,\n                                                per_chromosome=per_chromosome)\n\n    if per_chromosome:\n        return snp_tables\n    else:\n        return pd.concat(list(snp_tables.values()))\n</code></pre>"},{"location":"api/GWADataLoader/#magenpy.GWADataLoader.GWADataLoader.to_summary_statistics_table","title":"<code>to_summary_statistics_table(col_subset=None, per_chromosome=False)</code>","text":"<p>Get a dataframe of the GWAS summary statistics for all variants across different chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>The subset of columns (or summary statistics) to obtain.</p> <code>None</code> <code>per_chromosome</code> <p>If True, returns a dictionary where the key is the chromosome number and the value is the summary statistics table per chromosome.</p> <code>False</code> <p>Returns:</p> Type Description <p>A dataframe (or dictionary of dataframes) of summary statistics.</p> Source code in <code>magenpy/GWADataLoader.py</code> <pre><code>def to_summary_statistics_table(self, col_subset=None, per_chromosome=False):\n    \"\"\"\n    Get a dataframe of the GWAS summary statistics for all variants\n    across different chromosomes.\n\n    :param col_subset: The subset of columns (or summary statistics) to obtain.\n    :param per_chromosome: If True, returns a dictionary where the key\n    is the chromosome number and the value is the summary statistics table per\n    chromosome.\n\n    :return: A dataframe (or dictionary of dataframes) of summary statistics.\n    \"\"\"\n\n    assert self.sumstats_table is not None\n\n    snp_tables = {}\n\n    for c in self.chromosomes:\n        snp_tables[c] = self.sumstats_table[c].to_table(col_subset=col_subset)\n\n    if per_chromosome:\n        return snp_tables\n    else:\n        return pd.concat(list(snp_tables.values()))\n</code></pre>"},{"location":"api/GenotypeMatrix/","title":"GenotypeMatrix","text":""},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix","title":"<code>GenotypeMatrix</code>","text":"<p>               Bases: <code>object</code></p> <p>A class to represent a genotype matrix. The genotype matrix is a matrix of where the rows represent samples and the columns represent genetic variants. In general, genotype matrices are assumed to reside on disk and this class provides a convenient interface to interact with and perform computations on the genotype matrix.</p> <p>Currently, we assume that the genotype matrix is stored using plink's BED file format, with associated tables for the samples (i.e. FAM file) and genetic variants (i.e. BIM file). Classes that inherit from this generic class support various backends to access and performing computations on this genotype data.</p> <p>See Also</p> <pre><code>* [xarrayGenotypeMatrix][magenpy.GenotypeMatrix.xarrayGenotypeMatrix]\n* [plinkBEDGenotypeMatrix][magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix]\n</code></pre> <p>Attributes:</p> Name Type Description <code>sample_table</code> <code>Union[DataFrame, SampleTable, None]</code> <p>A table containing information about the samples in the genotype matrix (initially read from the FAM file).</p> <code>snp_table</code> <code>Union[DataFrame, None]</code> <p>A table containing information about the genetic variants in the genotype matrix (initially read from the BIM file).</p> <code>bed_file</code> <p>The path to the plink BED file containing the genotype matrix.</p> <code>_genome_build</code> <p>The genome build or assembly under which the SNP coordinates are defined.</p> <code>temp_dir</code> <p>The directory where temporary files will be stored (if needed).</p> <code>threads</code> <p>The number of threads to use for parallel computations.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>class GenotypeMatrix(object):\n    \"\"\"\n    A class to represent a genotype matrix. The genotype matrix is a matrix of\n    where the rows represent samples and the columns represent genetic variants.\n    In general, genotype matrices are assumed to reside on disk and this class\n    provides a convenient interface to interact with and perform computations\n    on the genotype matrix.\n\n    Currently, we assume that the genotype matrix is stored using plink's BED\n    file format, with associated tables for the samples (i.e. FAM file) and genetic\n    variants (i.e. BIM file). Classes that inherit from this generic class support\n    various backends to access and performing computations on this genotype data.\n\n    !!! seealso \"See Also\"\n            * [xarrayGenotypeMatrix][magenpy.GenotypeMatrix.xarrayGenotypeMatrix]\n            * [plinkBEDGenotypeMatrix][magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix]\n\n    :ivar sample_table: A table containing information about the samples in the genotype matrix\n    (initially read from the FAM file).\n    :ivar snp_table: A table containing information about the genetic variants in the genotype matrix\n    (initially read from the BIM file).\n    :ivar bed_file: The path to the plink BED file containing the genotype matrix.\n    :ivar _genome_build: The genome build or assembly under which the SNP coordinates are defined.\n    :ivar temp_dir: The directory where temporary files will be stored (if needed).\n    :ivar threads: The number of threads to use for parallel computations.\n\n    \"\"\"\n\n    def __init__(self,\n                 sample_table: Union[pd.DataFrame, SampleTable, None] = None,\n                 snp_table: Union[pd.DataFrame, None] = None,\n                 temp_dir: str = 'temp',\n                 bed_file: str = None,\n                 genome_build=None,\n                 threads=1,\n                 **kwargs):\n        \"\"\"\n        Initialize a GenotypeMatrix object.\n\n        :param sample_table: A table containing information about the samples in the genotype matrix.\n        :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n        :param temp_dir: The directory where temporary files will be stored (if needed).\n        :param bed_file: The path to the plink BED file containing the genotype matrix.\n        :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n        :param threads: The number of threads to use for parallel computations.\n        :param kwargs: Additional keyword arguments.\n        \"\"\"\n\n        self.sample_table: Union[pd.DataFrame, SampleTable, None] = None\n        self.snp_table: Union[pd.DataFrame, None] = snp_table\n\n        if sample_table is not None:\n            self.set_sample_table(sample_table)\n\n        if snp_table is not None and 'original_index' not in self.snp_table.columns:\n            self.snp_table['original_index'] = np.arange(len(self.snp_table))\n\n        temp_dir_prefix = 'gmat_'\n\n        if self.chromosome is not None:\n            temp_dir_prefix += f'chr{self.chromosome}_'\n\n        self.temp_dir = temp_dir\n        self.temp_dir_prefix = temp_dir_prefix\n\n        makedir(self.temp_dir)\n\n        self.bed_file = bed_file\n        self._genome_build = genome_build\n\n        self.threads = threads\n\n    @classmethod\n    def from_file(cls, file_path, temp_dir='temp', **kwargs):\n        \"\"\"\n        Initialize a genotype matrix object by passing a file path + other keyword arguments.\n        :param file_path: The path to the plink BED file.\n        :type file_path: str\n        :param temp_dir: The directory where temporary files will be stored.\n        :type temp_dir: str\n        :param kwargs: Additional keyword arguments.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def shape(self):\n        \"\"\"\n        :return: The shape of the genotype matrix. Rows correspond to the\n        number of samples and columns to the number of SNPs.\n        \"\"\"\n        return self.n, self.m\n\n    @property\n    def n(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [sample_size][magenpy.GenotypeMatrix.GenotypeMatrix.sample_size]\n\n        :return: The sample size or number of individuals in the genotype matrix.\n        \"\"\"\n        return self.sample_table.n\n\n    @property\n    def sample_size(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n][magenpy.GenotypeMatrix.GenotypeMatrix.n]\n\n        :return: The sample size or number of individuals in the genotype matrix.\n        \"\"\"\n        return self.n\n\n    @property\n    def samples(self):\n        \"\"\"\n        :return: An array of sample IDs in the genotype matrix.\n        \"\"\"\n        return self.sample_table.iid\n\n    @property\n    def sample_index(self):\n        return self.sample_table.table['original_index'].values\n\n    @property\n    def snp_index(self):\n        return self.snp_table['original_index'].values\n\n    @property\n    def m(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [n_snps][magenpy.GenotypeMatrix.GenotypeMatrix.n_snps]\n\n        :return: The number of variants in the genotype matrix.\n        \"\"\"\n        if self.snp_table is not None:\n            return len(self.snp_table)\n\n    @property\n    def n_snps(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [m][magenpy.GenotypeMatrix.GenotypeMatrix.m]\n\n        :return: The number of variants in the genotype matrix.\n        \"\"\"\n        return self.m\n\n    @property\n    def genome_build(self):\n        \"\"\"\n        :return: The genome build or assembly under which the SNP coordinates are defined.\n        \"\"\"\n        return self._genome_build\n\n    @property\n    def chromosome(self):\n        \"\"\"\n        ..note::\n        This is a convenience method that assumes that the genotype matrix contains variants\n        from a single chromosome. If there are multiple chromosomes, the method will return `None`.\n\n        :return: The chromosome associated with the variants in the genotype matrix.\n        \"\"\"\n        chrom = self.chromosomes\n        if chrom is not None and len(chrom) == 1:\n            return chrom[0]\n\n    @property\n    def chromosomes(self):\n        \"\"\"\n        :return: The unique set of chromosomes comprising the genotype matrix.\n        \"\"\"\n        chrom = self.get_snp_attribute('CHR')\n        if chrom is not None:\n            return np.unique(chrom)\n\n    @property\n    def snps(self):\n        \"\"\"\n        :return: The SNP rsIDs for variants in the genotype matrix.\n        \"\"\"\n        return self.get_snp_attribute('SNP')\n\n    @property\n    def bp_pos(self):\n        \"\"\"\n        :return: The basepair position for the genetic variants in the genotype matrix.\n        \"\"\"\n        return self.get_snp_attribute('POS')\n\n    @property\n    def cm_pos(self):\n        \"\"\"\n        :return: The position of genetic variants in the genotype matrix in units of Centi Morgan.\n        :raises KeyError: If the genetic distance is not set in the genotype file.\n        \"\"\"\n        cm = self.get_snp_attribute('cM')\n        if len(set(cm)) == 1:\n            raise KeyError(\"Genetic distance in centi Morgan (cM) is not \"\n                           \"set in the genotype file!\")\n        return cm\n\n    @property\n    def a1(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [alt_allele][magenpy.GenotypeMatrix.GenotypeMatrix.alt_allele]\n            * [effect_allele][magenpy.GenotypeMatrix.GenotypeMatrix.effect_allele]\n\n        :return: The effect allele `A1` for each genetic variant.\n\n        \"\"\"\n        return self.get_snp_attribute('A1')\n\n    @property\n    def a2(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [ref_allele][magenpy.GenotypeMatrix.GenotypeMatrix.ref_allele]\n\n        :return: The reference allele `A2` for each genetic variant.\n\n        \"\"\"\n        return self.get_snp_attribute('A2')\n\n    @property\n    def ref_allele(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [a2][magenpy.GenotypeMatrix.GenotypeMatrix.a2]\n\n        :return: The reference allele `A2` for each genetic variant.\n        \"\"\"\n        return self.a2\n\n    @property\n    def alt_allele(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [effect_allele][magenpy.GenotypeMatrix.GenotypeMatrix.effect_allele]\n            * [a1][magenpy.GenotypeMatrix.GenotypeMatrix.a1]\n\n        :return: The effect allele `A1` for each genetic variant.\n\n        \"\"\"\n        return self.a1\n\n    @property\n    def effect_allele(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [alt_allele][magenpy.GenotypeMatrix.GenotypeMatrix.alt_allele]\n            * [a1][magenpy.GenotypeMatrix.GenotypeMatrix.a1]\n\n        :return: The effect allele `A1` for each genetic variant.\n\n        \"\"\"\n        return self.a1\n\n    @property\n    def n_per_snp(self):\n        \"\"\"\n        :return: Sample size per genetic variant (accounting for potential missing values).\n        \"\"\"\n        n = self.get_snp_attribute('N')\n        if n is not None:\n            return n\n        else:\n            self.compute_sample_size_per_snp()\n            return self.get_snp_attribute('N')\n\n    @property\n    def maf(self):\n        \"\"\"\n        :return: The minor allele frequency (MAF) of each variant in the genotype matrix.\n        \"\"\"\n        maf = self.get_snp_attribute('MAF')\n        if maf is not None:\n            return maf\n        else:\n            self.compute_allele_frequency()\n            return self.get_snp_attribute('MAF')\n\n    @property\n    def maf_var(self):\n        \"\"\"\n        :return: The variance in minor allele frequency (MAF) of each variant in the genotype matrix.\n        \"\"\"\n        return 2. * self.maf * (1. - self.maf)\n\n    def estimate_memory_allocation(self, dtype=np.float32):\n        \"\"\"\n        :return: An estimate of the memory allocation for the genotype matrix in megabytes.\n        \"\"\"\n        return self.n * self.m * np.dtype(dtype).itemsize / 1024 ** 2\n\n    def get_snp_table(self, col_subset=None):\n        \"\"\"\n        A convenience method to extract SNP-related information from the genotype matrix.\n        :param col_subset: A list of columns to extract from the SNP table.\n\n        :return: A `pandas` DataFrame with the requested columns.\n        \"\"\"\n\n        if col_subset is None:\n            return self.snp_table.copy()\n        else:\n            present_cols = list(set(col_subset).intersection(set(self.snp_table.columns)))\n            non_present_cols = list(set(col_subset) - set(present_cols))\n\n            if len(present_cols) &gt; 0:\n                table = self.snp_table[present_cols].copy()\n            else:\n                table = pd.DataFrame({c: [] for c in non_present_cols})\n\n            for col in non_present_cols:\n\n                if col == 'MAF':\n                    table['MAF'] = self.maf\n                elif col == 'MAF_VAR':\n                    table['MAF_VAR'] = self.maf_var\n                elif col == 'N':\n                    table['N'] = self.n_per_snp\n                else:\n                    raise KeyError(f\"Column '{col}' is not available in the SNP table!\")\n\n            return table[list(col_subset)]\n\n    def get_snp_attribute(self, attr):\n        \"\"\"\n\n        :param attr: The name of the attribute to extract from the SNP table.\n        :return: The values of a specific attribute for each variant in the genotype matrix.\n        \"\"\"\n        if self.snp_table is not None and attr in self.snp_table.columns:\n            return self.snp_table[attr].values\n\n    def compute_ld(self,\n                   estimator,\n                   output_dir,\n                   dtype='int16',\n                   compressor_name='zstd',\n                   compression_level=7,\n                   compute_spectral_properties=False,\n                   **ld_kwargs):\n        \"\"\"\n\n        Compute the Linkage-Disequilibrium (LD) or SNP-by-SNP correlation matrix\n        for the variants defined in the genotype matrix.\n\n        :param estimator: The estimator for the LD matrix. We currently support\n        4 different estimators: `sample`, `windowed`, `shrinkage`, and `block`.\n        :param output_dir: The output directory where the Zarr array containing the\n        entries of the LD matrix will be stored.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compressor to use for the Zarr array.\n        :param compression_level: The compression level for the Zarr array (1-9)\n        :param ld_kwargs: keyword arguments for the various LD estimators. Consult\n        the implementations of `WindowedLD`, `ShrinkageLD`, and `BlockLD` for details.\n        :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n        the LD matrix.\n        \"\"\"\n\n        from .stats.ld.estimator import SampleLD, WindowedLD, ShrinkageLD, BlockLD\n\n        if estimator == 'sample':\n            ld_est = SampleLD(self)\n        elif estimator == 'windowed':\n            ld_est = WindowedLD(self, **ld_kwargs)\n        elif estimator == 'shrinkage':\n            ld_est = ShrinkageLD(self, **ld_kwargs)\n        elif estimator == 'block':\n            ld_est = BlockLD(self, **ld_kwargs)\n        else:\n            raise KeyError(f\"LD estimator {estimator} is not recognized!\")\n\n        return ld_est.compute(output_dir,\n                              dtype=dtype,\n                              compressor_name=compressor_name,\n                              compression_level=compression_level,\n                              compute_spectral_properties=compute_spectral_properties)\n\n    def set_sample_table(self, sample_table):\n        \"\"\"\n        A convenience method set the sample table for the genotype matrix.\n        This may be useful for syncing sample tables across different Genotype matrices\n        corresponding to different chromosomes or genomic regions.\n\n        :param sample_table: An instance of SampleTable or a pandas dataframe containing\n        information about the samples in the genotype matrix.\n\n        \"\"\"\n\n        if isinstance(sample_table, SampleTable):\n            self.sample_table = sample_table\n        elif isinstance(sample_table, pd.DataFrame):\n            self.sample_table = SampleTable(sample_table)\n        else:\n            raise ValueError(\"The sample table is invalid! \"\n                             \"Has to be either an instance of \"\n                             \"SampleTable or pandas DataFrame.\")\n\n    def filter_snps(self, extract_snps=None, extract_file=None):\n        \"\"\"\n        Filter variants from the genotype matrix. User must specify\n        either a list of variants to extract or the path to a plink-style file\n        with the list of variants to extract.\n\n        :param extract_snps: A list (or array) of SNP IDs to keep in the genotype matrix.\n        :param extract_file: The path to a file with the list of variants to extract.\n        \"\"\"\n\n        assert extract_snps is not None or extract_file is not None\n\n        if extract_snps is None:\n            from .parsers.misc_parsers import read_snp_filter_file\n            extract_snps = read_snp_filter_file(extract_file)\n\n        self.snp_table = self.snp_table.merge(pd.DataFrame({'SNP': extract_snps}))\n\n    def filter_by_allele_frequency(self, min_maf=None, min_mac=1):\n        \"\"\"\n        Filter variants by minimum minor allele frequency or allele count cutoffs.\n\n        :param min_maf: Minimum minor allele frequency\n        :param min_mac: Minimum minor allele count (1 by default)\n        \"\"\"\n\n        if min_mac or min_maf:\n\n            maf = self.maf\n            n = self.n_per_snp\n\n            keep_flag = None\n\n            if min_mac:\n                mac = (2*maf*n).astype(np.int64)\n                keep_flag = (mac &gt;= min_mac) &amp; ((2*n - mac) &gt;= min_mac)\n\n            if min_maf:\n\n                maf_cond = (maf &gt;= min_maf) &amp; (1. - maf &gt;= min_maf)\n                if keep_flag is not None:\n                    keep_flag = keep_flag &amp; maf_cond\n                else:\n                    keep_flag = maf_cond\n\n            if keep_flag is not None:\n                self.filter_snps(extract_snps=self.snps[keep_flag])\n\n    def drop_duplicated_snps(self):\n        \"\"\"\n        A convenience method to drop variants with duplicated SNP rsIDs.\n        \"\"\"\n\n        u_snps, counts = np.unique(self.snps, return_counts=True)\n        if len(u_snps) &lt; self.n_snps:\n            # Keep only SNPs which occur once in the sequence:\n            self.filter_snps(u_snps[counts == 1])\n\n    def filter_samples(self, keep_samples=None, keep_file=None):\n        \"\"\"\n        Filter samples from the genotype matrix. User must specify\n        either a list of samples to keep or the path to a plink-style file\n        with the list of samples to keep.\n\n        :param keep_samples: A list (or array) of sample IDs to keep in the genotype matrix.\n        :param keep_file: The path to a file with the list of samples to keep.\n        \"\"\"\n\n        self.sample_table.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n        # IMPORTANT: After filtering samples, update SNP attributes that depend on the\n        # samples, such as MAF and N:\n        if 'N' in self.snp_table:\n            self.compute_sample_size_per_snp()\n        if 'MAF' in self.snp_table:\n            self.compute_allele_frequency()\n\n    def score(self, beta, standardize_genotype=False):\n        \"\"\"\n        Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, `beta`.\n\n        :param beta: A vector of effect sizes for each variant in the genotype matrix.\n        :param standardize_genotype: If True, standardized the genotype matrix when computing the score.\n        \"\"\"\n        raise NotImplementedError\n\n    def perform_gwas(self, **gwa_kwargs):\n        \"\"\"\n        Perform genome-wide association testing of all variants against the phenotype.\n\n        :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult `stats.gwa.utils`\n        for relevant keyword arguments for each backend.\n\n        :raises NotImplementedError: If the method is not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_allele_frequency(self):\n        \"\"\"\n        Compute the allele frequency of each variant or SNP in the genotype matrix.\n\n        :raises NotImplementedError: If the method is not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_sample_size_per_snp(self):\n        \"\"\"\n        Compute the sample size for each variant in the genotype matrix, accounting for\n        potential missing values.\n\n        :raises NotImplementedError: If the method is not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def split_by_chromosome(self):\n        \"\"\"\n        Split the genotype matrix by chromosome, so that we would\n        have a separate `GenotypeMatrix` objects for each chromosome.\n        This method returns a dictionary where the key is the chromosome number\n        and the value is an object of `GenotypeMatrix` for that chromosome.\n\n        :return: A dictionary of `GenotypeMatrix` objects, one for each chromosome.\n        \"\"\"\n\n        chromosome = self.chromosome\n\n        if chromosome:\n            return {chromosome: self}\n        else:\n            chrom_tables = self.snp_table.groupby('CHR')\n\n            return {\n                c: self.__class__(sample_table=self.sample_table,\n                                  snp_table=chrom_tables.get_group(c),\n                                  bed_file=self.bed_file,\n                                  temp_dir=self.temp_dir,\n                                  genome_build=self.genome_build,\n                                  threads=self.threads)\n                for c in chrom_tables.groups\n            }\n\n    def split_by_variants(self, variant_group_dict):\n        \"\"\"\n        Split the genotype matrix by variants into separate `GenotypeMatrix` objects\n        based on the groups defined in `variant_group_dict`. The dictionary should have\n        the group name as the key and the list of SNP rsIDs in that group as the value.\n\n        :param variant_group_dict: A dictionary where the key is the group name and the value\n        is a list of SNP rsIDs to group together.\n\n        :return: A dictionary of `GenotypeMatrix` objects, one for each group.\n        \"\"\"\n\n        if isinstance(variant_group_dict, dict):\n\n            variant_group_dict = pd.concat([\n                pd.DataFrame({'group': group, 'SNP': snps})\n                for group, snps in variant_group_dict.items()\n            ])\n        elif isinstance(variant_group_dict, pd.DataFrame):\n            assert 'SNP' in variant_group_dict.columns and 'group' in variant_group_dict.columns\n        else:\n            raise ValueError(\"The variant group dictionary is invalid!\")\n\n        grouped_table = self.snp_table.merge(variant_group_dict, on='SNP').groupby('group')\n\n        return {\n            group: self.__class__(sample_table=self.sample_table,\n                                  snp_table=grouped_table.get_group(group).drop(columns='group'),\n                                  bed_file=self.bed_file,\n                                  temp_dir=self.temp_dir,\n                                  genome_build=self.genome_build,\n                                  threads=self.threads)\n            for group in grouped_table.groups\n        }\n\n    def cleanup(self):\n        \"\"\"\n        Clean up all temporary files and directories\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.a1","title":"<code>a1</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>alt_allele</li> <li>effect_allele</li> </ul> <p>Returns:</p> Type Description <p>The effect allele <code>A1</code> for each genetic variant.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.a2","title":"<code>a2</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>ref_allele</li> </ul> <p>Returns:</p> Type Description <p>The reference allele <code>A2</code> for each genetic variant.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.alt_allele","title":"<code>alt_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>effect_allele</li> <li>a1</li> </ul> <p>Returns:</p> Type Description <p>The effect allele <code>A1</code> for each genetic variant.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.bp_pos","title":"<code>bp_pos</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The basepair position for the genetic variants in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.chromosome","title":"<code>chromosome</code>  <code>property</code>","text":"<p>..note:: This is a convenience method that assumes that the genotype matrix contains variants from a single chromosome. If there are multiple chromosomes, the method will return <code>None</code>.</p> <p>Returns:</p> Type Description <p>The chromosome associated with the variants in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.chromosomes","title":"<code>chromosomes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The unique set of chromosomes comprising the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.cm_pos","title":"<code>cm_pos</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The position of genetic variants in the genotype matrix in units of Centi Morgan.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the genetic distance is not set in the genotype file.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.effect_allele","title":"<code>effect_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>alt_allele</li> <li>a1</li> </ul> <p>Returns:</p> Type Description <p>The effect allele <code>A1</code> for each genetic variant.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.genome_build","title":"<code>genome_build</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The genome build or assembly under which the SNP coordinates are defined.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.m","title":"<code>m</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_snps</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.maf","title":"<code>maf</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The minor allele frequency (MAF) of each variant in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.maf_var","title":"<code>maf_var</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The variance in minor allele frequency (MAF) of each variant in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.n","title":"<code>n</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>sample_size</li> </ul> <p>Returns:</p> Type Description <p>The sample size or number of individuals in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.n_per_snp","title":"<code>n_per_snp</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>Sample size per genetic variant (accounting for potential missing values).</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>m</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.ref_allele","title":"<code>ref_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>a2</li> </ul> <p>Returns:</p> Type Description <p>The reference allele <code>A2</code> for each genetic variant.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.sample_size","title":"<code>sample_size</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n</li> </ul> <p>Returns:</p> Type Description <p>The sample size or number of individuals in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.samples","title":"<code>samples</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>An array of sample IDs in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The shape of the genotype matrix. Rows correspond to the number of samples and columns to the number of SNPs.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The SNP rsIDs for variants in the genotype matrix.</p>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.__init__","title":"<code>__init__(sample_table=None, snp_table=None, temp_dir='temp', bed_file=None, genome_build=None, threads=1, **kwargs)</code>","text":"<p>Initialize a GenotypeMatrix object.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <code>Union[DataFrame, SampleTable, None]</code> <p>A table containing information about the samples in the genotype matrix.</p> <code>None</code> <code>snp_table</code> <code>Union[DataFrame, None]</code> <p>A table containing information about the genetic variants in the genotype matrix.</p> <code>None</code> <code>temp_dir</code> <code>str</code> <p>The directory where temporary files will be stored (if needed).</p> <code>'temp'</code> <code>bed_file</code> <code>str</code> <p>The path to the plink BED file containing the genotype matrix.</p> <code>None</code> <code>genome_build</code> <p>The genome build or assembly under which the SNP coordinates are defined.</p> <code>None</code> <code>threads</code> <p>The number of threads to use for parallel computations.</p> <code>1</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def __init__(self,\n             sample_table: Union[pd.DataFrame, SampleTable, None] = None,\n             snp_table: Union[pd.DataFrame, None] = None,\n             temp_dir: str = 'temp',\n             bed_file: str = None,\n             genome_build=None,\n             threads=1,\n             **kwargs):\n    \"\"\"\n    Initialize a GenotypeMatrix object.\n\n    :param sample_table: A table containing information about the samples in the genotype matrix.\n    :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n    :param temp_dir: The directory where temporary files will be stored (if needed).\n    :param bed_file: The path to the plink BED file containing the genotype matrix.\n    :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n    :param threads: The number of threads to use for parallel computations.\n    :param kwargs: Additional keyword arguments.\n    \"\"\"\n\n    self.sample_table: Union[pd.DataFrame, SampleTable, None] = None\n    self.snp_table: Union[pd.DataFrame, None] = snp_table\n\n    if sample_table is not None:\n        self.set_sample_table(sample_table)\n\n    if snp_table is not None and 'original_index' not in self.snp_table.columns:\n        self.snp_table['original_index'] = np.arange(len(self.snp_table))\n\n    temp_dir_prefix = 'gmat_'\n\n    if self.chromosome is not None:\n        temp_dir_prefix += f'chr{self.chromosome}_'\n\n    self.temp_dir = temp_dir\n    self.temp_dir_prefix = temp_dir_prefix\n\n    makedir(self.temp_dir)\n\n    self.bed_file = bed_file\n    self._genome_build = genome_build\n\n    self.threads = threads\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up all temporary files and directories</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Clean up all temporary files and directories\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.compute_allele_frequency","title":"<code>compute_allele_frequency()</code>","text":"<p>Compute the allele frequency of each variant or SNP in the genotype matrix.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the subclass.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_allele_frequency(self):\n    \"\"\"\n    Compute the allele frequency of each variant or SNP in the genotype matrix.\n\n    :raises NotImplementedError: If the method is not implemented in the subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.compute_ld","title":"<code>compute_ld(estimator, output_dir, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False, **ld_kwargs)</code>","text":"<p>Compute the Linkage-Disequilibrium (LD) or SNP-by-SNP correlation matrix for the variants defined in the genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>The estimator for the LD matrix. We currently support 4 different estimators: <code>sample</code>, <code>windowed</code>, <code>shrinkage</code>, and <code>block</code>.</p> required <code>output_dir</code> <p>The output directory where the Zarr array containing the entries of the LD matrix will be stored.</p> required <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the Zarr array.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level for the Zarr array (1-9)</p> <code>7</code> <code>ld_kwargs</code> <p>keyword arguments for the various LD estimators. Consult the implementations of <code>WindowedLD</code>, <code>ShrinkageLD</code>, and <code>BlockLD</code> for details.</p> <code>{}</code> <code>compute_spectral_properties</code> <p>If True, compute and store information about the eigenvalues of the LD matrix.</p> <code>False</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_ld(self,\n               estimator,\n               output_dir,\n               dtype='int16',\n               compressor_name='zstd',\n               compression_level=7,\n               compute_spectral_properties=False,\n               **ld_kwargs):\n    \"\"\"\n\n    Compute the Linkage-Disequilibrium (LD) or SNP-by-SNP correlation matrix\n    for the variants defined in the genotype matrix.\n\n    :param estimator: The estimator for the LD matrix. We currently support\n    4 different estimators: `sample`, `windowed`, `shrinkage`, and `block`.\n    :param output_dir: The output directory where the Zarr array containing the\n    entries of the LD matrix will be stored.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor to use for the Zarr array.\n    :param compression_level: The compression level for the Zarr array (1-9)\n    :param ld_kwargs: keyword arguments for the various LD estimators. Consult\n    the implementations of `WindowedLD`, `ShrinkageLD`, and `BlockLD` for details.\n    :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n    the LD matrix.\n    \"\"\"\n\n    from .stats.ld.estimator import SampleLD, WindowedLD, ShrinkageLD, BlockLD\n\n    if estimator == 'sample':\n        ld_est = SampleLD(self)\n    elif estimator == 'windowed':\n        ld_est = WindowedLD(self, **ld_kwargs)\n    elif estimator == 'shrinkage':\n        ld_est = ShrinkageLD(self, **ld_kwargs)\n    elif estimator == 'block':\n        ld_est = BlockLD(self, **ld_kwargs)\n    else:\n        raise KeyError(f\"LD estimator {estimator} is not recognized!\")\n\n    return ld_est.compute(output_dir,\n                          dtype=dtype,\n                          compressor_name=compressor_name,\n                          compression_level=compression_level,\n                          compute_spectral_properties=compute_spectral_properties)\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.compute_sample_size_per_snp","title":"<code>compute_sample_size_per_snp()</code>","text":"<p>Compute the sample size for each variant in the genotype matrix, accounting for potential missing values.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the subclass.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_sample_size_per_snp(self):\n    \"\"\"\n    Compute the sample size for each variant in the genotype matrix, accounting for\n    potential missing values.\n\n    :raises NotImplementedError: If the method is not implemented in the subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.drop_duplicated_snps","title":"<code>drop_duplicated_snps()</code>","text":"<p>A convenience method to drop variants with duplicated SNP rsIDs.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def drop_duplicated_snps(self):\n    \"\"\"\n    A convenience method to drop variants with duplicated SNP rsIDs.\n    \"\"\"\n\n    u_snps, counts = np.unique(self.snps, return_counts=True)\n    if len(u_snps) &lt; self.n_snps:\n        # Keep only SNPs which occur once in the sequence:\n        self.filter_snps(u_snps[counts == 1])\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.estimate_memory_allocation","title":"<code>estimate_memory_allocation(dtype=np.float32)</code>","text":"<p>Returns:</p> Type Description <p>An estimate of the memory allocation for the genotype matrix in megabytes.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def estimate_memory_allocation(self, dtype=np.float32):\n    \"\"\"\n    :return: An estimate of the memory allocation for the genotype matrix in megabytes.\n    \"\"\"\n    return self.n * self.m * np.dtype(dtype).itemsize / 1024 ** 2\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.filter_by_allele_frequency","title":"<code>filter_by_allele_frequency(min_maf=None, min_mac=1)</code>","text":"<p>Filter variants by minimum minor allele frequency or allele count cutoffs.</p> <p>Parameters:</p> Name Type Description Default <code>min_maf</code> <p>Minimum minor allele frequency</p> <code>None</code> <code>min_mac</code> <p>Minimum minor allele count (1 by default)</p> <code>1</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def filter_by_allele_frequency(self, min_maf=None, min_mac=1):\n    \"\"\"\n    Filter variants by minimum minor allele frequency or allele count cutoffs.\n\n    :param min_maf: Minimum minor allele frequency\n    :param min_mac: Minimum minor allele count (1 by default)\n    \"\"\"\n\n    if min_mac or min_maf:\n\n        maf = self.maf\n        n = self.n_per_snp\n\n        keep_flag = None\n\n        if min_mac:\n            mac = (2*maf*n).astype(np.int64)\n            keep_flag = (mac &gt;= min_mac) &amp; ((2*n - mac) &gt;= min_mac)\n\n        if min_maf:\n\n            maf_cond = (maf &gt;= min_maf) &amp; (1. - maf &gt;= min_maf)\n            if keep_flag is not None:\n                keep_flag = keep_flag &amp; maf_cond\n            else:\n                keep_flag = maf_cond\n\n        if keep_flag is not None:\n            self.filter_snps(extract_snps=self.snps[keep_flag])\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.filter_samples","title":"<code>filter_samples(keep_samples=None, keep_file=None)</code>","text":"<p>Filter samples from the genotype matrix. User must specify either a list of samples to keep or the path to a plink-style file with the list of samples to keep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_samples</code> <p>A list (or array) of sample IDs to keep in the genotype matrix.</p> <code>None</code> <code>keep_file</code> <p>The path to a file with the list of samples to keep.</p> <code>None</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def filter_samples(self, keep_samples=None, keep_file=None):\n    \"\"\"\n    Filter samples from the genotype matrix. User must specify\n    either a list of samples to keep or the path to a plink-style file\n    with the list of samples to keep.\n\n    :param keep_samples: A list (or array) of sample IDs to keep in the genotype matrix.\n    :param keep_file: The path to a file with the list of samples to keep.\n    \"\"\"\n\n    self.sample_table.filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n\n    # IMPORTANT: After filtering samples, update SNP attributes that depend on the\n    # samples, such as MAF and N:\n    if 'N' in self.snp_table:\n        self.compute_sample_size_per_snp()\n    if 'MAF' in self.snp_table:\n        self.compute_allele_frequency()\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None)</code>","text":"<p>Filter variants from the genotype matrix. User must specify either a list of variants to extract or the path to a plink-style file with the list of variants to extract.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list (or array) of SNP IDs to keep in the genotype matrix.</p> <code>None</code> <code>extract_file</code> <p>The path to a file with the list of variants to extract.</p> <code>None</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None):\n    \"\"\"\n    Filter variants from the genotype matrix. User must specify\n    either a list of variants to extract or the path to a plink-style file\n    with the list of variants to extract.\n\n    :param extract_snps: A list (or array) of SNP IDs to keep in the genotype matrix.\n    :param extract_file: The path to a file with the list of variants to extract.\n    \"\"\"\n\n    assert extract_snps is not None or extract_file is not None\n\n    if extract_snps is None:\n        from .parsers.misc_parsers import read_snp_filter_file\n        extract_snps = read_snp_filter_file(extract_file)\n\n    self.snp_table = self.snp_table.merge(pd.DataFrame({'SNP': extract_snps}))\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.from_file","title":"<code>from_file(file_path, temp_dir='temp', **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize a genotype matrix object by passing a file path + other keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the plink BED file.</p> required <code>temp_dir</code> <code>str</code> <p>The directory where temporary files will be stored.</p> <code>'temp'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>@classmethod\ndef from_file(cls, file_path, temp_dir='temp', **kwargs):\n    \"\"\"\n    Initialize a genotype matrix object by passing a file path + other keyword arguments.\n    :param file_path: The path to the plink BED file.\n    :type file_path: str\n    :param temp_dir: The directory where temporary files will be stored.\n    :type temp_dir: str\n    :param kwargs: Additional keyword arguments.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.get_snp_attribute","title":"<code>get_snp_attribute(attr)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>attr</code> <p>The name of the attribute to extract from the SNP table.</p> required <p>Returns:</p> Type Description <p>The values of a specific attribute for each variant in the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def get_snp_attribute(self, attr):\n    \"\"\"\n\n    :param attr: The name of the attribute to extract from the SNP table.\n    :return: The values of a specific attribute for each variant in the genotype matrix.\n    \"\"\"\n    if self.snp_table is not None and attr in self.snp_table.columns:\n        return self.snp_table[attr].values\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.get_snp_table","title":"<code>get_snp_table(col_subset=None)</code>","text":"<p>A convenience method to extract SNP-related information from the genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>A list of columns to extract from the SNP table.</p> <code>None</code> <p>Returns:</p> Type Description <p>A <code>pandas</code> DataFrame with the requested columns.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def get_snp_table(self, col_subset=None):\n    \"\"\"\n    A convenience method to extract SNP-related information from the genotype matrix.\n    :param col_subset: A list of columns to extract from the SNP table.\n\n    :return: A `pandas` DataFrame with the requested columns.\n    \"\"\"\n\n    if col_subset is None:\n        return self.snp_table.copy()\n    else:\n        present_cols = list(set(col_subset).intersection(set(self.snp_table.columns)))\n        non_present_cols = list(set(col_subset) - set(present_cols))\n\n        if len(present_cols) &gt; 0:\n            table = self.snp_table[present_cols].copy()\n        else:\n            table = pd.DataFrame({c: [] for c in non_present_cols})\n\n        for col in non_present_cols:\n\n            if col == 'MAF':\n                table['MAF'] = self.maf\n            elif col == 'MAF_VAR':\n                table['MAF_VAR'] = self.maf_var\n            elif col == 'N':\n                table['N'] = self.n_per_snp\n            else:\n                raise KeyError(f\"Column '{col}' is not available in the SNP table!\")\n\n        return table[list(col_subset)]\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.perform_gwas","title":"<code>perform_gwas(**gwa_kwargs)</code>","text":"<p>Perform genome-wide association testing of all variants against the phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>gwa_kwargs</code> <p>Keyword arguments to pass to the GWA functions. Consult <code>stats.gwa.utils</code> for relevant keyword arguments for each backend.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the subclass.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def perform_gwas(self, **gwa_kwargs):\n    \"\"\"\n    Perform genome-wide association testing of all variants against the phenotype.\n\n    :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult `stats.gwa.utils`\n    for relevant keyword arguments for each backend.\n\n    :raises NotImplementedError: If the method is not implemented in the subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.score","title":"<code>score(beta, standardize_genotype=False)</code>","text":"<p>Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, <code>beta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A vector of effect sizes for each variant in the genotype matrix.</p> required <code>standardize_genotype</code> <p>If True, standardized the genotype matrix when computing the score.</p> <code>False</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def score(self, beta, standardize_genotype=False):\n    \"\"\"\n    Perform linear scoring, i.e. multiply the genotype matrix by the vector of effect sizes, `beta`.\n\n    :param beta: A vector of effect sizes for each variant in the genotype matrix.\n    :param standardize_genotype: If True, standardized the genotype matrix when computing the score.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.set_sample_table","title":"<code>set_sample_table(sample_table)</code>","text":"<p>A convenience method set the sample table for the genotype matrix. This may be useful for syncing sample tables across different Genotype matrices corresponding to different chromosomes or genomic regions.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <p>An instance of SampleTable or a pandas dataframe containing information about the samples in the genotype matrix.</p> required Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def set_sample_table(self, sample_table):\n    \"\"\"\n    A convenience method set the sample table for the genotype matrix.\n    This may be useful for syncing sample tables across different Genotype matrices\n    corresponding to different chromosomes or genomic regions.\n\n    :param sample_table: An instance of SampleTable or a pandas dataframe containing\n    information about the samples in the genotype matrix.\n\n    \"\"\"\n\n    if isinstance(sample_table, SampleTable):\n        self.sample_table = sample_table\n    elif isinstance(sample_table, pd.DataFrame):\n        self.sample_table = SampleTable(sample_table)\n    else:\n        raise ValueError(\"The sample table is invalid! \"\n                         \"Has to be either an instance of \"\n                         \"SampleTable or pandas DataFrame.\")\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.split_by_chromosome","title":"<code>split_by_chromosome()</code>","text":"<p>Split the genotype matrix by chromosome, so that we would have a separate <code>GenotypeMatrix</code> objects for each chromosome. This method returns a dictionary where the key is the chromosome number and the value is an object of <code>GenotypeMatrix</code> for that chromosome.</p> <p>Returns:</p> Type Description <p>A dictionary of <code>GenotypeMatrix</code> objects, one for each chromosome.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def split_by_chromosome(self):\n    \"\"\"\n    Split the genotype matrix by chromosome, so that we would\n    have a separate `GenotypeMatrix` objects for each chromosome.\n    This method returns a dictionary where the key is the chromosome number\n    and the value is an object of `GenotypeMatrix` for that chromosome.\n\n    :return: A dictionary of `GenotypeMatrix` objects, one for each chromosome.\n    \"\"\"\n\n    chromosome = self.chromosome\n\n    if chromosome:\n        return {chromosome: self}\n    else:\n        chrom_tables = self.snp_table.groupby('CHR')\n\n        return {\n            c: self.__class__(sample_table=self.sample_table,\n                              snp_table=chrom_tables.get_group(c),\n                              bed_file=self.bed_file,\n                              temp_dir=self.temp_dir,\n                              genome_build=self.genome_build,\n                              threads=self.threads)\n            for c in chrom_tables.groups\n        }\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.GenotypeMatrix.split_by_variants","title":"<code>split_by_variants(variant_group_dict)</code>","text":"<p>Split the genotype matrix by variants into separate <code>GenotypeMatrix</code> objects based on the groups defined in <code>variant_group_dict</code>. The dictionary should have the group name as the key and the list of SNP rsIDs in that group as the value.</p> <p>Parameters:</p> Name Type Description Default <code>variant_group_dict</code> <p>A dictionary where the key is the group name and the value is a list of SNP rsIDs to group together.</p> required <p>Returns:</p> Type Description <p>A dictionary of <code>GenotypeMatrix</code> objects, one for each group.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def split_by_variants(self, variant_group_dict):\n    \"\"\"\n    Split the genotype matrix by variants into separate `GenotypeMatrix` objects\n    based on the groups defined in `variant_group_dict`. The dictionary should have\n    the group name as the key and the list of SNP rsIDs in that group as the value.\n\n    :param variant_group_dict: A dictionary where the key is the group name and the value\n    is a list of SNP rsIDs to group together.\n\n    :return: A dictionary of `GenotypeMatrix` objects, one for each group.\n    \"\"\"\n\n    if isinstance(variant_group_dict, dict):\n\n        variant_group_dict = pd.concat([\n            pd.DataFrame({'group': group, 'SNP': snps})\n            for group, snps in variant_group_dict.items()\n        ])\n    elif isinstance(variant_group_dict, pd.DataFrame):\n        assert 'SNP' in variant_group_dict.columns and 'group' in variant_group_dict.columns\n    else:\n        raise ValueError(\"The variant group dictionary is invalid!\")\n\n    grouped_table = self.snp_table.merge(variant_group_dict, on='SNP').groupby('group')\n\n    return {\n        group: self.__class__(sample_table=self.sample_table,\n                              snp_table=grouped_table.get_group(group).drop(columns='group'),\n                              bed_file=self.bed_file,\n                              temp_dir=self.temp_dir,\n                              genome_build=self.genome_build,\n                              threads=self.threads)\n        for group in grouped_table.groups\n    }\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.bedReaderGenotypeMatrix","title":"<code>bedReaderGenotypeMatrix</code>","text":"<p>               Bases: <code>GenotypeMatrix</code></p> <p>NOTE: Still experimental. Requires more testing and fine-tuning.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>class bedReaderGenotypeMatrix(GenotypeMatrix):\n    \"\"\"\n    NOTE: Still experimental.\n    Requires more testing and fine-tuning.\n    \"\"\"\n\n    def __init__(self,\n                 sample_table=None,\n                 snp_table=None,\n                 bed_file=None,\n                 temp_dir='temp',\n                 bed_reader=None,\n                 genome_build=None,\n                 threads=1):\n\n        super().__init__(sample_table=sample_table,\n                         snp_table=snp_table,\n                         temp_dir=temp_dir,\n                         bed_file=bed_file,\n                         genome_build=genome_build,\n                         threads=threads)\n\n        # The bed_reader object:\n        self.bed_reader = bed_reader\n\n    @classmethod\n    def from_file(cls, file_path, temp_dir='temp', **kwargs):\n\n        from bed_reader import open_bed\n\n        try:\n            bed_reader = open_bed(file_path)\n        except Exception as e:\n            raise e\n\n        # Set the sample table:\n        sample_table = pd.DataFrame({\n            'FID': bed_reader.fid,\n            'IID': bed_reader.iid,\n            'fatherID': bed_reader.father,\n            'motherID': bed_reader.mother,\n            'sex': bed_reader.sex,\n            'phenotype': bed_reader.pheno\n        }).astype({\n            'FID': str,\n            'IID': str,\n            'fatherID': str,\n            'motherID': str,\n            'sex': float,\n            'phenotype': float\n        })\n\n        sample_table['phenotype'] = sample_table['phenotype'].replace({-9.: np.nan})\n        sample_table = sample_table.reset_index()\n\n        # Set the snp table:\n        snp_table = pd.DataFrame({\n            'CHR': bed_reader.chromosome,\n            'SNP': bed_reader.sid,\n            'cM': bed_reader.cm_position,\n            'POS': bed_reader.bp_position,\n            'A1': bed_reader.allele_1,\n            'A2': bed_reader.allele_2\n        }).astype({\n            'CHR': int,\n            'SNP': str,\n            'cM': np.float32,\n            'POS': np.int32,\n            'A1': str,\n            'A2': str\n        })\n\n        g_mat = cls(sample_table=SampleTable(sample_table),\n                    snp_table=snp_table,\n                    temp_dir=temp_dir,\n                    bed_reader=bed_reader,\n                    **kwargs)\n\n        return g_mat\n\n    def score(self, beta, standardize_genotype=False, skip_na=True):\n        \"\"\"\n        Perform linear scoring on the genotype matrix.\n        :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n        :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n        :param skip_na: If True, skip missing values when computing the polygenic score.\n        \"\"\"\n\n        if len(beta.shape) &gt; 1:\n            pgs = np.zeros((self.n, beta.shape[1]))\n        else:\n            pgs = np.zeros(self.n)\n\n        if standardize_genotype:\n            from .stats.transforms.genotype import standardize\n            for (start, end), chunk in self._iter_col_chunks(return_slice=True):\n                pgs += standardize(chunk).dot(beta[start:end])\n        else:\n            for (start, end), chunk in self._iter_col_chunks(return_slice=True):\n                if skip_na:\n                    chunk_pgs = np.nan_to_num(chunk).dot(beta[start:end])\n                else:\n                    chunk_pgs = np.where(np.isnan(chunk), self.maf[start:end], chunk).dot(beta[start:end])\n\n                pgs += chunk_pgs\n\n        return pgs\n\n    def perform_gwas(self, **gwa_kwargs):\n        \"\"\"\n        Perform genome-wide association testing of all variants against the phenotype.\n\n        TODO: Implement this method...\n\n        :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult `stats.gwa.utils`\n        \"\"\"\n\n        raise NotImplementedError\n\n    def compute_allele_frequency(self):\n        \"\"\"\n        Compute the allele frequency of each variant or SNP in the genotype matrix.\n        \"\"\"\n        self.snp_table['MAF'] = (np.concatenate([np.nansum(bed_chunk, axis=0)\n                                                 for bed_chunk in self._iter_col_chunks()]) / (2. * self.n_per_snp))\n\n    def compute_sample_size_per_snp(self):\n        \"\"\"\n        Compute the sample size for each variant in the genotype matrix, accounting for\n        potential missing values.\n        \"\"\"\n\n        self.snp_table['N'] = self.n - np.concatenate([np.sum(np.isnan(bed_chunk), axis=0)\n                                                       for bed_chunk in self._iter_col_chunks()])\n\n    def _iter_row_chunks(self, chunk_size='auto', return_slice=False):\n        \"\"\"\n        Iterate over the genotype matrix by rows.\n\n        :param chunk_size: The size of the chunk to read from the genotype matrix.\n        :param return_slice: If True, return the slice of the genotype matrix corresponding to the chunk.\n\n        :return: A generator that yields chunks of the genotype matrix.\n        \"\"\"\n        if chunk_size == 'auto':\n            matrix_size = self.estimate_memory_allocation()\n            # By default, we allocate 128MB per chunk:\n            chunk_size = int(self.n // (matrix_size // 128))\n\n        for i in range(int(np.ceil(self.n / chunk_size))):\n            start, end = int(i * chunk_size), min(int((i + 1) * chunk_size), self.n)\n            chunk = self.bed_reader.read(np.s_[self.sample_index[start:end], self.snp_index],\n                                         num_threads=self.threads)\n            if return_slice:\n                yield (start, end), chunk\n            else:\n                yield chunk\n\n    def _iter_col_chunks(self, chunk_size='auto', return_slice=False):\n        \"\"\"\n        Iterate over the genotype matrix by columns.\n\n        :param chunk_size: The size of the chunk to read from the genotype matrix.\n        :param return_slice: If True, return the slice of the genotype matrix corresponding to the chunk.\n\n        :return: A generator that yields chunks of the genotype matrix.\n        \"\"\"\n\n        if chunk_size == 'auto':\n            matrix_size = self.estimate_memory_allocation()\n            # By default, we allocate 128MB per chunk:\n            chunk_size = int(self.m // (matrix_size // 128))\n\n        for i in range(int(np.ceil(self.m / chunk_size))):\n            start, end = int(i * chunk_size), min(int((i + 1) * chunk_size), self.m)\n            chunk = self.bed_reader.read(np.s_[self.sample_index, self.snp_index[start:end]],\n                                         num_threads=self.threads)\n            if return_slice:\n                yield (start, end), chunk\n            else:\n                yield chunk\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.bedReaderGenotypeMatrix.compute_allele_frequency","title":"<code>compute_allele_frequency()</code>","text":"<p>Compute the allele frequency of each variant or SNP in the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_allele_frequency(self):\n    \"\"\"\n    Compute the allele frequency of each variant or SNP in the genotype matrix.\n    \"\"\"\n    self.snp_table['MAF'] = (np.concatenate([np.nansum(bed_chunk, axis=0)\n                                             for bed_chunk in self._iter_col_chunks()]) / (2. * self.n_per_snp))\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.bedReaderGenotypeMatrix.compute_sample_size_per_snp","title":"<code>compute_sample_size_per_snp()</code>","text":"<p>Compute the sample size for each variant in the genotype matrix, accounting for potential missing values.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_sample_size_per_snp(self):\n    \"\"\"\n    Compute the sample size for each variant in the genotype matrix, accounting for\n    potential missing values.\n    \"\"\"\n\n    self.snp_table['N'] = self.n - np.concatenate([np.sum(np.isnan(bed_chunk), axis=0)\n                                                   for bed_chunk in self._iter_col_chunks()])\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.bedReaderGenotypeMatrix.perform_gwas","title":"<code>perform_gwas(**gwa_kwargs)</code>","text":"<p>Perform genome-wide association testing of all variants against the phenotype.</p> <p>TODO: Implement this method...</p> <p>Parameters:</p> Name Type Description Default <code>gwa_kwargs</code> <p>Keyword arguments to pass to the GWA functions. Consult <code>stats.gwa.utils</code></p> <code>{}</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def perform_gwas(self, **gwa_kwargs):\n    \"\"\"\n    Perform genome-wide association testing of all variants against the phenotype.\n\n    TODO: Implement this method...\n\n    :param gwa_kwargs: Keyword arguments to pass to the GWA functions. Consult `stats.gwa.utils`\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.bedReaderGenotypeMatrix.score","title":"<code>score(beta, standardize_genotype=False, skip_na=True)</code>","text":"<p>Perform linear scoring on the genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A vector or matrix of effect sizes for each variant in the genotype matrix.</p> required <code>standardize_genotype</code> <p>If True, standardize the genotype when computing the polygenic score.</p> <code>False</code> <code>skip_na</code> <p>If True, skip missing values when computing the polygenic score.</p> <code>True</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def score(self, beta, standardize_genotype=False, skip_na=True):\n    \"\"\"\n    Perform linear scoring on the genotype matrix.\n    :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n    :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n    :param skip_na: If True, skip missing values when computing the polygenic score.\n    \"\"\"\n\n    if len(beta.shape) &gt; 1:\n        pgs = np.zeros((self.n, beta.shape[1]))\n    else:\n        pgs = np.zeros(self.n)\n\n    if standardize_genotype:\n        from .stats.transforms.genotype import standardize\n        for (start, end), chunk in self._iter_col_chunks(return_slice=True):\n            pgs += standardize(chunk).dot(beta[start:end])\n    else:\n        for (start, end), chunk in self._iter_col_chunks(return_slice=True):\n            if skip_na:\n                chunk_pgs = np.nan_to_num(chunk).dot(beta[start:end])\n            else:\n                chunk_pgs = np.where(np.isnan(chunk), self.maf[start:end], chunk).dot(beta[start:end])\n\n            pgs += chunk_pgs\n\n    return pgs\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix","title":"<code>plinkBEDGenotypeMatrix</code>","text":"<p>               Bases: <code>GenotypeMatrix</code></p> <p>A class that defines methods and interfaces for interacting with genotype matrices using <code>plink2</code> software. This class provides a convenient interface to perform various computations on genotype matrices stored in the plink BED format.</p> <p>This class inherits all the attributes of the <code>GenotypeMatrix</code> class.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>class plinkBEDGenotypeMatrix(GenotypeMatrix):\n    \"\"\"\n    A class that defines methods and interfaces for interacting with genotype matrices\n    using `plink2` software. This class provides a convenient interface to perform various\n    computations on genotype matrices stored in the plink BED format.\n\n    This class inherits all the attributes of the `GenotypeMatrix` class.\n    \"\"\"\n\n    def __init__(self,\n                 sample_table=None,\n                 snp_table=None,\n                 temp_dir='temp',\n                 bed_file=None,\n                 genome_build=None,\n                 threads=1):\n        \"\"\"\n        Initialize a `plinkBEDGenotypeMatrix` object.\n\n        :param sample_table: A table containing information about the samples in the genotype matrix.\n        :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n        :param temp_dir: The directory where temporary files will be stored (if needed).\n        :param bed_file: The path to the plink BED file containing the genotype matrix.\n        :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n        :param threads: The number of threads to use for parallel computations.\n        \"\"\"\n\n        super().__init__(sample_table=sample_table,\n                         snp_table=snp_table,\n                         temp_dir=temp_dir,\n                         bed_file=bed_file,\n                         genome_build=genome_build,\n                         threads=threads)\n\n        from .parsers.plink_parsers import parse_fam_file, parse_bim_file\n\n        if self.bed_file is not None:\n            self.bed_file = self.bed_file.replace('.bed', '')\n\n        if self.sample_table is None and self.bed_file:\n            self.sample_table = SampleTable(parse_fam_file(self.bed_file))\n\n        if self.snp_table is None and self.bed_file:\n            self.snp_table = parse_bim_file(self.bed_file)\n            self.snp_table['original_index'] = np.arange(len(self.snp_table))\n\n    @classmethod\n    def from_file(cls, file_path, temp_dir='temp', **kwargs):\n        \"\"\"\n        A convenience method to create a `plinkBEDGenotypeMatrix` object by\n         providing a path to a PLINK BED file.\n\n        :param file_path: The path to the plink BED file.\n        :param temp_dir: The directory where temporary files will be stored.\n        :param kwargs: Additional keyword arguments.\n        \"\"\"\n\n        p_gt = cls(bed_file=file_path, temp_dir=temp_dir, **kwargs)\n\n        return p_gt\n\n    def score(self, beta, standardize_genotype=False):\n        \"\"\"\n        Perform linear scoring on the genotype matrix. This function takes a vector (or matrix) of\n        effect sizes and returns the matrix-vector or matrix-matrix product of the genotype matrix\n        multiplied by the effect sizes.\n\n        This can be used for polygenic score calculation or projecting the genotype matrix.\n\n        :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n        :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n\n        :return: The polygenic score (PGS) for each sample in the genotype matrix.\n        \"\"\"\n\n        from .stats.score.utils import score_plink2\n\n        # Create a temporary directory where we store intermediate results:\n        tmp_score_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                                    prefix=self.temp_dir_prefix + 'score_')\n\n        plink_score = score_plink2(self,\n                                   beta,\n                                   standardize_genotype=standardize_genotype,\n                                   temp_dir=tmp_score_dir.name)\n\n        tmp_score_dir.cleanup()\n\n        return plink_score\n\n    def perform_gwas(self, **gwa_kwargs):\n        \"\"\"\n        Perform genome-wide association testing of all variants against the phenotype.\n        This method calls specialized functions that, in turn, call `plink2` to perform\n        the association testing.\n\n        :return: A Summary statistics table containing the results of the association testing.\n        \"\"\"\n\n        from .stats.gwa.utils import perform_gwa_plink2\n\n        # Create a temporary directory where we store intermediate results:\n        tmp_gwas_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                                   prefix=self.temp_dir_prefix + 'gwas_')\n\n        plink_gwa = perform_gwa_plink2(self, temp_dir=tmp_gwas_dir.name, **gwa_kwargs)\n\n        tmp_gwas_dir.cleanup()\n\n        return plink_gwa\n\n    def compute_allele_frequency(self):\n        \"\"\"\n        Compute the allele frequency of each variant or SNP in the genotype matrix.\n        This method calls specialized functions that, in turn, call `plink2` to compute\n        allele frequency.\n        \"\"\"\n\n        from .stats.variant.utils import compute_allele_frequency_plink2\n\n        # Create a temporary directory where we store intermediate results:\n        tmp_freq_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                                   prefix=self.temp_dir_prefix + 'freq_')\n\n        self.snp_table['MAF'] = compute_allele_frequency_plink2(self, temp_dir=tmp_freq_dir.name)\n\n        tmp_freq_dir.cleanup()\n\n    def compute_sample_size_per_snp(self):\n        \"\"\"\n        Compute the sample size for each variant in the genotype matrix, accounting for\n        potential missing values.\n\n        This method calls specialized functions that, in turn, call `plink2` to compute sample\n        size per variant.\n        \"\"\"\n\n        from .stats.variant.utils import compute_sample_size_per_snp_plink2\n\n        # Create a temporary directory where we store intermediate results:\n        tmp_miss_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                                   prefix=self.temp_dir_prefix + 'miss_')\n\n        self.snp_table['N'] = compute_sample_size_per_snp_plink2(self, temp_dir=tmp_miss_dir.name)\n\n        tmp_miss_dir.cleanup()\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.__init__","title":"<code>__init__(sample_table=None, snp_table=None, temp_dir='temp', bed_file=None, genome_build=None, threads=1)</code>","text":"<p>Initialize a <code>plinkBEDGenotypeMatrix</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <p>A table containing information about the samples in the genotype matrix.</p> <code>None</code> <code>snp_table</code> <p>A table containing information about the genetic variants in the genotype matrix.</p> <code>None</code> <code>temp_dir</code> <p>The directory where temporary files will be stored (if needed).</p> <code>'temp'</code> <code>bed_file</code> <p>The path to the plink BED file containing the genotype matrix.</p> <code>None</code> <code>genome_build</code> <p>The genome build or assembly under which the SNP coordinates are defined.</p> <code>None</code> <code>threads</code> <p>The number of threads to use for parallel computations.</p> <code>1</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def __init__(self,\n             sample_table=None,\n             snp_table=None,\n             temp_dir='temp',\n             bed_file=None,\n             genome_build=None,\n             threads=1):\n    \"\"\"\n    Initialize a `plinkBEDGenotypeMatrix` object.\n\n    :param sample_table: A table containing information about the samples in the genotype matrix.\n    :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n    :param temp_dir: The directory where temporary files will be stored (if needed).\n    :param bed_file: The path to the plink BED file containing the genotype matrix.\n    :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n    :param threads: The number of threads to use for parallel computations.\n    \"\"\"\n\n    super().__init__(sample_table=sample_table,\n                     snp_table=snp_table,\n                     temp_dir=temp_dir,\n                     bed_file=bed_file,\n                     genome_build=genome_build,\n                     threads=threads)\n\n    from .parsers.plink_parsers import parse_fam_file, parse_bim_file\n\n    if self.bed_file is not None:\n        self.bed_file = self.bed_file.replace('.bed', '')\n\n    if self.sample_table is None and self.bed_file:\n        self.sample_table = SampleTable(parse_fam_file(self.bed_file))\n\n    if self.snp_table is None and self.bed_file:\n        self.snp_table = parse_bim_file(self.bed_file)\n        self.snp_table['original_index'] = np.arange(len(self.snp_table))\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.compute_allele_frequency","title":"<code>compute_allele_frequency()</code>","text":"<p>Compute the allele frequency of each variant or SNP in the genotype matrix. This method calls specialized functions that, in turn, call <code>plink2</code> to compute allele frequency.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_allele_frequency(self):\n    \"\"\"\n    Compute the allele frequency of each variant or SNP in the genotype matrix.\n    This method calls specialized functions that, in turn, call `plink2` to compute\n    allele frequency.\n    \"\"\"\n\n    from .stats.variant.utils import compute_allele_frequency_plink2\n\n    # Create a temporary directory where we store intermediate results:\n    tmp_freq_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                               prefix=self.temp_dir_prefix + 'freq_')\n\n    self.snp_table['MAF'] = compute_allele_frequency_plink2(self, temp_dir=tmp_freq_dir.name)\n\n    tmp_freq_dir.cleanup()\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.compute_sample_size_per_snp","title":"<code>compute_sample_size_per_snp()</code>","text":"<p>Compute the sample size for each variant in the genotype matrix, accounting for potential missing values.</p> <p>This method calls specialized functions that, in turn, call <code>plink2</code> to compute sample size per variant.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_sample_size_per_snp(self):\n    \"\"\"\n    Compute the sample size for each variant in the genotype matrix, accounting for\n    potential missing values.\n\n    This method calls specialized functions that, in turn, call `plink2` to compute sample\n    size per variant.\n    \"\"\"\n\n    from .stats.variant.utils import compute_sample_size_per_snp_plink2\n\n    # Create a temporary directory where we store intermediate results:\n    tmp_miss_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                               prefix=self.temp_dir_prefix + 'miss_')\n\n    self.snp_table['N'] = compute_sample_size_per_snp_plink2(self, temp_dir=tmp_miss_dir.name)\n\n    tmp_miss_dir.cleanup()\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.from_file","title":"<code>from_file(file_path, temp_dir='temp', **kwargs)</code>  <code>classmethod</code>","text":"<p>A convenience method to create a <code>plinkBEDGenotypeMatrix</code> object by  providing a path to a PLINK BED file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>The path to the plink BED file.</p> required <code>temp_dir</code> <p>The directory where temporary files will be stored.</p> <code>'temp'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>@classmethod\ndef from_file(cls, file_path, temp_dir='temp', **kwargs):\n    \"\"\"\n    A convenience method to create a `plinkBEDGenotypeMatrix` object by\n     providing a path to a PLINK BED file.\n\n    :param file_path: The path to the plink BED file.\n    :param temp_dir: The directory where temporary files will be stored.\n    :param kwargs: Additional keyword arguments.\n    \"\"\"\n\n    p_gt = cls(bed_file=file_path, temp_dir=temp_dir, **kwargs)\n\n    return p_gt\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.perform_gwas","title":"<code>perform_gwas(**gwa_kwargs)</code>","text":"<p>Perform genome-wide association testing of all variants against the phenotype. This method calls specialized functions that, in turn, call <code>plink2</code> to perform the association testing.</p> <p>Returns:</p> Type Description <p>A Summary statistics table containing the results of the association testing.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def perform_gwas(self, **gwa_kwargs):\n    \"\"\"\n    Perform genome-wide association testing of all variants against the phenotype.\n    This method calls specialized functions that, in turn, call `plink2` to perform\n    the association testing.\n\n    :return: A Summary statistics table containing the results of the association testing.\n    \"\"\"\n\n    from .stats.gwa.utils import perform_gwa_plink2\n\n    # Create a temporary directory where we store intermediate results:\n    tmp_gwas_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                               prefix=self.temp_dir_prefix + 'gwas_')\n\n    plink_gwa = perform_gwa_plink2(self, temp_dir=tmp_gwas_dir.name, **gwa_kwargs)\n\n    tmp_gwas_dir.cleanup()\n\n    return plink_gwa\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.plinkBEDGenotypeMatrix.score","title":"<code>score(beta, standardize_genotype=False)</code>","text":"<p>Perform linear scoring on the genotype matrix. This function takes a vector (or matrix) of effect sizes and returns the matrix-vector or matrix-matrix product of the genotype matrix multiplied by the effect sizes.</p> <p>This can be used for polygenic score calculation or projecting the genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A vector or matrix of effect sizes for each variant in the genotype matrix.</p> required <code>standardize_genotype</code> <p>If True, standardize the genotype when computing the polygenic score.</p> <code>False</code> <p>Returns:</p> Type Description <p>The polygenic score (PGS) for each sample in the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def score(self, beta, standardize_genotype=False):\n    \"\"\"\n    Perform linear scoring on the genotype matrix. This function takes a vector (or matrix) of\n    effect sizes and returns the matrix-vector or matrix-matrix product of the genotype matrix\n    multiplied by the effect sizes.\n\n    This can be used for polygenic score calculation or projecting the genotype matrix.\n\n    :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n    :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n\n    :return: The polygenic score (PGS) for each sample in the genotype matrix.\n    \"\"\"\n\n    from .stats.score.utils import score_plink2\n\n    # Create a temporary directory where we store intermediate results:\n    tmp_score_dir = tempfile.TemporaryDirectory(dir=self.temp_dir,\n                                                prefix=self.temp_dir_prefix + 'score_')\n\n    plink_score = score_plink2(self,\n                               beta,\n                               standardize_genotype=standardize_genotype,\n                               temp_dir=tmp_score_dir.name)\n\n    tmp_score_dir.cleanup()\n\n    return plink_score\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix","title":"<code>xarrayGenotypeMatrix</code>","text":"<p>               Bases: <code>GenotypeMatrix</code></p> <p>A class that defines methods and interfaces for interacting with genotype matrices using the <code>xarray</code> library. In particular, the class leverages functionality provided by the <code>pandas-plink</code> package to represent on-disk genotype matrices as chunked multidimensional arrays that can be queried and manipulated efficiently and in parallel.</p> <p>This class inherits all the attributes of the <code>GenotypeMatrix</code> class.</p> <p>Attributes:</p> Name Type Description <code>xr_mat</code> <p>The <code>xarray</code> object representing the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>class xarrayGenotypeMatrix(GenotypeMatrix):\n    \"\"\"\n    A class that defines methods and interfaces for interacting with genotype matrices\n    using the `xarray` library. In particular, the class leverages functionality provided by\n    the `pandas-plink` package to represent on-disk genotype matrices as chunked multidimensional\n    arrays that can be queried and manipulated efficiently and in parallel.\n\n    This class inherits all the attributes of the `GenotypeMatrix` class.\n\n    :ivar xr_mat: The `xarray` object representing the genotype matrix.\n\n    \"\"\"\n\n    def __init__(self,\n                 sample_table=None,\n                 snp_table=None,\n                 bed_file=None,\n                 temp_dir='temp',\n                 xr_mat=None,\n                 genome_build=None,\n                 threads=1):\n        \"\"\"\n        Initialize an xarrayGenotypeMatrix object.\n\n        :param sample_table: A table containing information about the samples in the genotype matrix.\n        :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n        :param bed_file: The path to the plink BED file containing the genotype matrix.\n        :param temp_dir: The directory where temporary files will be stored (if needed).\n        :param xr_mat: The xarray object representing the genotype matrix.\n        :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n        :param threads: The number of threads to use for parallel computations.\n        \"\"\"\n\n        super().__init__(sample_table=sample_table,\n                         snp_table=snp_table,\n                         temp_dir=temp_dir,\n                         bed_file=bed_file,\n                         genome_build=genome_build,\n                         threads=threads)\n\n        # xarray matrix object, as defined by pandas-plink:\n        self.xr_mat = xr_mat\n\n    @classmethod\n    def from_file(cls, file_path, temp_dir='temp', **kwargs):\n        \"\"\"\n        Create a GenotypeMatrix object using a PLINK BED file with the help\n        of the data structures defined in `pandas_plink`. The genotype matrix\n        will be represented implicitly in an `xarray` object, and we will use it\n        to perform various computations. This method is a utility function to\n        construct the genotype matrix object from a plink BED file.\n\n        :param file_path: Path to the plink BED file.\n        :param temp_dir: The directory where the temporary files will be stored.\n        :param kwargs: Additional keyword arguments.\n        \"\"\"\n\n        from pandas_plink import read_plink1_bin\n        import warnings\n\n        def convert_string_python_to_numpy(ds):\n            \"\"\"\n            Convert PandasExtension string[python] coordinates to numpy arrays of\n            objects instead. This is a hack to get around issues with copying\n            xarray datasets that contain string coordinates.\n\n            See here: https://github.com/pydata/xarray/issues/9742\n            \"\"\"\n\n            coord_updates = {}\n\n            for coord_name, coord in ds.coords.items():\n                if hasattr(coord, 'data') and hasattr(coord.data, 'dtype'):\n                    if str(coord.data.dtype) == 'string' or 'string[python]' in str(coord.data.dtype):\n                        try:\n                            # Convert to numpy unicode strings\n                            numpy_strings = coord.data.to_numpy(dtype=None, na_value='')\n                            # This will be a U&lt;n&gt; dtype where n is auto-determined\n                            coord_updates[coord_name] = (coord.dims, numpy_strings, coord.attrs)\n                        except Exception as e:\n                            print(f\"Failed to convert {coord_name}: {e}\")\n\n            if coord_updates:\n                return ds.assign_coords(coord_updates)\n\n            return ds\n\n        # Ignore FutureWarning for now\n        with warnings.catch_warnings():\n\n            warnings.simplefilter(\"ignore\")\n\n            try:\n                xr_gt = read_plink1_bin(file_path + \".bed\", ref=\"a0\", verbose=False)\n            except ValueError:\n                xr_gt = read_plink1_bin(file_path, ref=\"a0\", verbose=False)\n\n            xr_gt = convert_string_python_to_numpy(xr_gt)\n\n        # Set the sample table:\n        sample_table = xr_gt.sample.coords.to_dataset().to_dataframe()\n        sample_table.columns = ['FID', 'IID', 'fatherID', 'motherID', 'sex', 'phenotype']\n        sample_table.reset_index(inplace=True, drop=True)\n        sample_table = sample_table.astype({\n            'FID': str,\n            'IID': str,\n            'fatherID': str,\n            'motherID': str,\n            'sex': float,\n            'phenotype': float\n        })\n\n        sample_table['phenotype'] = sample_table['phenotype'].replace({-9.: np.nan})\n\n        # Set the snp table:\n        snp_table = xr_gt.variant.coords.to_dataset().to_dataframe()\n        snp_table.columns = ['CHR', 'SNP', 'cM', 'POS', 'A1', 'A2']\n        snp_table.reset_index(inplace=True, drop=True)\n        snp_table = snp_table.astype({\n            'CHR': int,\n            'SNP': str,\n            'cM': np.float32,\n            'POS': np.int32,\n            'A1': str,\n            'A2': str\n        })\n\n        g_mat = cls(sample_table=SampleTable(sample_table),\n                    snp_table=snp_table,\n                    temp_dir=temp_dir,\n                    bed_file=file_path,\n                    xr_mat=xr_gt,\n                    **kwargs)\n\n        return g_mat\n\n    def set_sample_table(self, sample_table):\n        \"\"\"\n        A convenience method set the sample table for the genotype matrix.\n        This is useful for cases when we need to sync the sample table across chromosomes.\n\n        :param sample_table: An instance of SampleTable or a pandas dataframe containing\n        information about the samples in the genotype matrix.\n        \"\"\"\n\n        super().set_sample_table(sample_table)\n\n        try:\n            if self.n != self.xr_mat.shape[0]:\n                self.xr_mat = self.xr_mat.sel(sample=self.samples)\n        except AttributeError:\n            pass\n\n    def filter_snps(self, extract_snps=None, extract_file=None):\n        \"\"\"\n        Filter variants from the genotype matrix. User must specify either a list of variants to\n        extract or the path to a file with the list of variants to extract.\n\n        :param extract_snps: A list or array of SNP rsIDs to keep in the genotype matrix.\n        :param extract_file: The path to a file with the list of variants to extract.\n        \"\"\"\n\n        super().filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n        from .utils.compute_utils import intersect_arrays\n\n        idx = intersect_arrays(self.xr_mat.variant.coords['snp'].values, self.snps, return_index=True)\n\n        self.xr_mat = self.xr_mat.isel(variant=idx)\n\n    def filter_samples(self, keep_samples=None, keep_file=None):\n        \"\"\"\n        Filter samples from the genotype matrix.\n        User must specify either a list of samples to keep or the path to a file with the list of samples to keep.\n\n        :param keep_samples: A list (or array) of sample IDs to keep in the genotype matrix.\n        :param keep_file: The path to a file with the list of samples to keep.\n        \"\"\"\n\n        super().filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n        self.xr_mat = self.xr_mat.sel(sample=self.samples)\n\n    def to_numpy(self, dtype=np.int8):\n        \"\"\"\n        Convert the genotype matrix to a numpy array.\n        :param dtype: The data type of the numpy array. Default: Int8\n\n        :return: A numpy array representation of the genotype matrix.\n        \"\"\"\n\n        return self.xr_mat.data.astype(dtype).compute()\n\n    def to_csr(self, dtype=np.int8):\n        \"\"\"\n        Convert the genotype matrix to a scipy sparse CSR matrix.\n        :param dtype: The data type of the scipy array. Default: Int8\n\n        :return: A `scipy` sparse CSR matrix representation of the genotype matrix.\n        \"\"\"\n\n        mat = self.to_numpy(dtype=dtype)\n\n        from scipy.sparse import csr_matrix\n\n        return csr_matrix(mat)\n\n    def score(self, beta, standardize_genotype=False, skip_na=True):\n        \"\"\"\n        Perform linear scoring on the genotype matrix.\n        :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n        :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n        :param skip_na: If True, skip missing values when computing the polygenic score.\n\n        :return: The polygenic score(s) (PGS) for each sample in the genotype matrix.\n\n        \"\"\"\n\n        import dask.array as da\n\n        mat = self.xr_mat.data\n\n        chunked_beta = da.from_array(beta, chunks=mat.chunksize[1])\n\n        if standardize_genotype:\n            from .stats.transforms.genotype import standardize\n            mat = standardize(mat)\n            mat = da.nan_to_num(mat)\n            pgs = da.dot(mat, chunked_beta).compute()\n        else:\n            if skip_na:\n                pgs = da.dot(da.nan_to_num(mat), chunked_beta).compute()\n            else:\n                pgs = da.dot(self.xr_mat.fillna(self.maf).data, chunked_beta).compute()\n\n        return pgs\n\n    def perform_gwas(self, **gwa_kwargs):\n        \"\"\"\n        A convenience method that calls specialized utility functions that perform\n        genome-wide association testing of all variants against the phenotype.\n\n        :return: A Summary statistics table containing the results of the association testing.\n        \"\"\"\n\n        from .stats.gwa.utils import perform_gwa_xarray\n        return perform_gwa_xarray(self, **gwa_kwargs)\n\n    def compute_allele_frequency(self):\n        \"\"\"\n        A convenience method that calls specialized utility functions that\n        compute the allele frequency of each variant or SNP in the genotype matrix.\n        \"\"\"\n        self.snp_table['MAF'] = (self.xr_mat.sum(axis=0) / (2. * self.n_per_snp)).compute().values\n\n    def compute_sample_size_per_snp(self):\n        \"\"\"\n        A convenience method that calls specialized utility functions that compute\n        the sample size for each variant in the genotype matrix, accounting for\n        potential missing values.\n        \"\"\"\n        self.snp_table['N'] = self.xr_mat.shape[0] - self.xr_mat.isnull().sum(axis=0).compute().values\n\n    def split_by_chromosome(self):\n        \"\"\"\n        Split the genotype matrix by chromosome.\n        :return: A dictionary of `xarrayGenotypeMatrix` objects, one for each chromosome.\n        \"\"\"\n        split = super().split_by_chromosome()\n\n        for c, gt in split.items():\n            gt.xr_mat = self.xr_mat\n            if len(split) &gt; 1:\n                gt.filter_snps(extract_snps=gt.snps)\n\n        return split\n\n    def split_by_variants(self, variant_group_dict):\n        \"\"\"\n        Split the genotype matrix by variants into separate `xarrayGenotypeMatrix` objects\n        based on the groups defined in `variant_group_dict`. The dictionary should have\n        the group name as the key and the list of SNP rsIDs in that group as the value.\n\n        :param variant_group_dict: A dictionary where the key is the group name and the value\n        is a list of SNP rsIDs to group together.\n\n        :return: A dictionary of `xarrayGenotypeMatrix` objects, one for each group.\n        \"\"\"\n\n        split = super().split_by_variants(variant_group_dict)\n\n        for g, gt in split.items():\n            gt.xr_mat = self.xr_mat\n            gt.filter_snps(extract_snps=gt.snps)\n\n        return split\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.__init__","title":"<code>__init__(sample_table=None, snp_table=None, bed_file=None, temp_dir='temp', xr_mat=None, genome_build=None, threads=1)</code>","text":"<p>Initialize an xarrayGenotypeMatrix object.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <p>A table containing information about the samples in the genotype matrix.</p> <code>None</code> <code>snp_table</code> <p>A table containing information about the genetic variants in the genotype matrix.</p> <code>None</code> <code>bed_file</code> <p>The path to the plink BED file containing the genotype matrix.</p> <code>None</code> <code>temp_dir</code> <p>The directory where temporary files will be stored (if needed).</p> <code>'temp'</code> <code>xr_mat</code> <p>The xarray object representing the genotype matrix.</p> <code>None</code> <code>genome_build</code> <p>The genome build or assembly under which the SNP coordinates are defined.</p> <code>None</code> <code>threads</code> <p>The number of threads to use for parallel computations.</p> <code>1</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def __init__(self,\n             sample_table=None,\n             snp_table=None,\n             bed_file=None,\n             temp_dir='temp',\n             xr_mat=None,\n             genome_build=None,\n             threads=1):\n    \"\"\"\n    Initialize an xarrayGenotypeMatrix object.\n\n    :param sample_table: A table containing information about the samples in the genotype matrix.\n    :param snp_table: A table containing information about the genetic variants in the genotype matrix.\n    :param bed_file: The path to the plink BED file containing the genotype matrix.\n    :param temp_dir: The directory where temporary files will be stored (if needed).\n    :param xr_mat: The xarray object representing the genotype matrix.\n    :param genome_build: The genome build or assembly under which the SNP coordinates are defined.\n    :param threads: The number of threads to use for parallel computations.\n    \"\"\"\n\n    super().__init__(sample_table=sample_table,\n                     snp_table=snp_table,\n                     temp_dir=temp_dir,\n                     bed_file=bed_file,\n                     genome_build=genome_build,\n                     threads=threads)\n\n    # xarray matrix object, as defined by pandas-plink:\n    self.xr_mat = xr_mat\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.compute_allele_frequency","title":"<code>compute_allele_frequency()</code>","text":"<p>A convenience method that calls specialized utility functions that compute the allele frequency of each variant or SNP in the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_allele_frequency(self):\n    \"\"\"\n    A convenience method that calls specialized utility functions that\n    compute the allele frequency of each variant or SNP in the genotype matrix.\n    \"\"\"\n    self.snp_table['MAF'] = (self.xr_mat.sum(axis=0) / (2. * self.n_per_snp)).compute().values\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.compute_sample_size_per_snp","title":"<code>compute_sample_size_per_snp()</code>","text":"<p>A convenience method that calls specialized utility functions that compute the sample size for each variant in the genotype matrix, accounting for potential missing values.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def compute_sample_size_per_snp(self):\n    \"\"\"\n    A convenience method that calls specialized utility functions that compute\n    the sample size for each variant in the genotype matrix, accounting for\n    potential missing values.\n    \"\"\"\n    self.snp_table['N'] = self.xr_mat.shape[0] - self.xr_mat.isnull().sum(axis=0).compute().values\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.filter_samples","title":"<code>filter_samples(keep_samples=None, keep_file=None)</code>","text":"<p>Filter samples from the genotype matrix. User must specify either a list of samples to keep or the path to a file with the list of samples to keep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_samples</code> <p>A list (or array) of sample IDs to keep in the genotype matrix.</p> <code>None</code> <code>keep_file</code> <p>The path to a file with the list of samples to keep.</p> <code>None</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def filter_samples(self, keep_samples=None, keep_file=None):\n    \"\"\"\n    Filter samples from the genotype matrix.\n    User must specify either a list of samples to keep or the path to a file with the list of samples to keep.\n\n    :param keep_samples: A list (or array) of sample IDs to keep in the genotype matrix.\n    :param keep_file: The path to a file with the list of samples to keep.\n    \"\"\"\n\n    super().filter_samples(keep_samples=keep_samples, keep_file=keep_file)\n    self.xr_mat = self.xr_mat.sel(sample=self.samples)\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None)</code>","text":"<p>Filter variants from the genotype matrix. User must specify either a list of variants to extract or the path to a file with the list of variants to extract.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list or array of SNP rsIDs to keep in the genotype matrix.</p> <code>None</code> <code>extract_file</code> <p>The path to a file with the list of variants to extract.</p> <code>None</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None):\n    \"\"\"\n    Filter variants from the genotype matrix. User must specify either a list of variants to\n    extract or the path to a file with the list of variants to extract.\n\n    :param extract_snps: A list or array of SNP rsIDs to keep in the genotype matrix.\n    :param extract_file: The path to a file with the list of variants to extract.\n    \"\"\"\n\n    super().filter_snps(extract_snps=extract_snps, extract_file=extract_file)\n\n    from .utils.compute_utils import intersect_arrays\n\n    idx = intersect_arrays(self.xr_mat.variant.coords['snp'].values, self.snps, return_index=True)\n\n    self.xr_mat = self.xr_mat.isel(variant=idx)\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.from_file","title":"<code>from_file(file_path, temp_dir='temp', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a GenotypeMatrix object using a PLINK BED file with the help of the data structures defined in <code>pandas_plink</code>. The genotype matrix will be represented implicitly in an <code>xarray</code> object, and we will use it to perform various computations. This method is a utility function to construct the genotype matrix object from a plink BED file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>Path to the plink BED file.</p> required <code>temp_dir</code> <p>The directory where the temporary files will be stored.</p> <code>'temp'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>@classmethod\ndef from_file(cls, file_path, temp_dir='temp', **kwargs):\n    \"\"\"\n    Create a GenotypeMatrix object using a PLINK BED file with the help\n    of the data structures defined in `pandas_plink`. The genotype matrix\n    will be represented implicitly in an `xarray` object, and we will use it\n    to perform various computations. This method is a utility function to\n    construct the genotype matrix object from a plink BED file.\n\n    :param file_path: Path to the plink BED file.\n    :param temp_dir: The directory where the temporary files will be stored.\n    :param kwargs: Additional keyword arguments.\n    \"\"\"\n\n    from pandas_plink import read_plink1_bin\n    import warnings\n\n    def convert_string_python_to_numpy(ds):\n        \"\"\"\n        Convert PandasExtension string[python] coordinates to numpy arrays of\n        objects instead. This is a hack to get around issues with copying\n        xarray datasets that contain string coordinates.\n\n        See here: https://github.com/pydata/xarray/issues/9742\n        \"\"\"\n\n        coord_updates = {}\n\n        for coord_name, coord in ds.coords.items():\n            if hasattr(coord, 'data') and hasattr(coord.data, 'dtype'):\n                if str(coord.data.dtype) == 'string' or 'string[python]' in str(coord.data.dtype):\n                    try:\n                        # Convert to numpy unicode strings\n                        numpy_strings = coord.data.to_numpy(dtype=None, na_value='')\n                        # This will be a U&lt;n&gt; dtype where n is auto-determined\n                        coord_updates[coord_name] = (coord.dims, numpy_strings, coord.attrs)\n                    except Exception as e:\n                        print(f\"Failed to convert {coord_name}: {e}\")\n\n        if coord_updates:\n            return ds.assign_coords(coord_updates)\n\n        return ds\n\n    # Ignore FutureWarning for now\n    with warnings.catch_warnings():\n\n        warnings.simplefilter(\"ignore\")\n\n        try:\n            xr_gt = read_plink1_bin(file_path + \".bed\", ref=\"a0\", verbose=False)\n        except ValueError:\n            xr_gt = read_plink1_bin(file_path, ref=\"a0\", verbose=False)\n\n        xr_gt = convert_string_python_to_numpy(xr_gt)\n\n    # Set the sample table:\n    sample_table = xr_gt.sample.coords.to_dataset().to_dataframe()\n    sample_table.columns = ['FID', 'IID', 'fatherID', 'motherID', 'sex', 'phenotype']\n    sample_table.reset_index(inplace=True, drop=True)\n    sample_table = sample_table.astype({\n        'FID': str,\n        'IID': str,\n        'fatherID': str,\n        'motherID': str,\n        'sex': float,\n        'phenotype': float\n    })\n\n    sample_table['phenotype'] = sample_table['phenotype'].replace({-9.: np.nan})\n\n    # Set the snp table:\n    snp_table = xr_gt.variant.coords.to_dataset().to_dataframe()\n    snp_table.columns = ['CHR', 'SNP', 'cM', 'POS', 'A1', 'A2']\n    snp_table.reset_index(inplace=True, drop=True)\n    snp_table = snp_table.astype({\n        'CHR': int,\n        'SNP': str,\n        'cM': np.float32,\n        'POS': np.int32,\n        'A1': str,\n        'A2': str\n    })\n\n    g_mat = cls(sample_table=SampleTable(sample_table),\n                snp_table=snp_table,\n                temp_dir=temp_dir,\n                bed_file=file_path,\n                xr_mat=xr_gt,\n                **kwargs)\n\n    return g_mat\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.perform_gwas","title":"<code>perform_gwas(**gwa_kwargs)</code>","text":"<p>A convenience method that calls specialized utility functions that perform genome-wide association testing of all variants against the phenotype.</p> <p>Returns:</p> Type Description <p>A Summary statistics table containing the results of the association testing.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def perform_gwas(self, **gwa_kwargs):\n    \"\"\"\n    A convenience method that calls specialized utility functions that perform\n    genome-wide association testing of all variants against the phenotype.\n\n    :return: A Summary statistics table containing the results of the association testing.\n    \"\"\"\n\n    from .stats.gwa.utils import perform_gwa_xarray\n    return perform_gwa_xarray(self, **gwa_kwargs)\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.score","title":"<code>score(beta, standardize_genotype=False, skip_na=True)</code>","text":"<p>Perform linear scoring on the genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <p>A vector or matrix of effect sizes for each variant in the genotype matrix.</p> required <code>standardize_genotype</code> <p>If True, standardize the genotype when computing the polygenic score.</p> <code>False</code> <code>skip_na</code> <p>If True, skip missing values when computing the polygenic score.</p> <code>True</code> <p>Returns:</p> Type Description <p>The polygenic score(s) (PGS) for each sample in the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def score(self, beta, standardize_genotype=False, skip_na=True):\n    \"\"\"\n    Perform linear scoring on the genotype matrix.\n    :param beta: A vector or matrix of effect sizes for each variant in the genotype matrix.\n    :param standardize_genotype: If True, standardize the genotype when computing the polygenic score.\n    :param skip_na: If True, skip missing values when computing the polygenic score.\n\n    :return: The polygenic score(s) (PGS) for each sample in the genotype matrix.\n\n    \"\"\"\n\n    import dask.array as da\n\n    mat = self.xr_mat.data\n\n    chunked_beta = da.from_array(beta, chunks=mat.chunksize[1])\n\n    if standardize_genotype:\n        from .stats.transforms.genotype import standardize\n        mat = standardize(mat)\n        mat = da.nan_to_num(mat)\n        pgs = da.dot(mat, chunked_beta).compute()\n    else:\n        if skip_na:\n            pgs = da.dot(da.nan_to_num(mat), chunked_beta).compute()\n        else:\n            pgs = da.dot(self.xr_mat.fillna(self.maf).data, chunked_beta).compute()\n\n    return pgs\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.set_sample_table","title":"<code>set_sample_table(sample_table)</code>","text":"<p>A convenience method set the sample table for the genotype matrix. This is useful for cases when we need to sync the sample table across chromosomes.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <p>An instance of SampleTable or a pandas dataframe containing information about the samples in the genotype matrix.</p> required Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def set_sample_table(self, sample_table):\n    \"\"\"\n    A convenience method set the sample table for the genotype matrix.\n    This is useful for cases when we need to sync the sample table across chromosomes.\n\n    :param sample_table: An instance of SampleTable or a pandas dataframe containing\n    information about the samples in the genotype matrix.\n    \"\"\"\n\n    super().set_sample_table(sample_table)\n\n    try:\n        if self.n != self.xr_mat.shape[0]:\n            self.xr_mat = self.xr_mat.sel(sample=self.samples)\n    except AttributeError:\n        pass\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.split_by_chromosome","title":"<code>split_by_chromosome()</code>","text":"<p>Split the genotype matrix by chromosome.</p> <p>Returns:</p> Type Description <p>A dictionary of <code>xarrayGenotypeMatrix</code> objects, one for each chromosome.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def split_by_chromosome(self):\n    \"\"\"\n    Split the genotype matrix by chromosome.\n    :return: A dictionary of `xarrayGenotypeMatrix` objects, one for each chromosome.\n    \"\"\"\n    split = super().split_by_chromosome()\n\n    for c, gt in split.items():\n        gt.xr_mat = self.xr_mat\n        if len(split) &gt; 1:\n            gt.filter_snps(extract_snps=gt.snps)\n\n    return split\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.split_by_variants","title":"<code>split_by_variants(variant_group_dict)</code>","text":"<p>Split the genotype matrix by variants into separate <code>xarrayGenotypeMatrix</code> objects based on the groups defined in <code>variant_group_dict</code>. The dictionary should have the group name as the key and the list of SNP rsIDs in that group as the value.</p> <p>Parameters:</p> Name Type Description Default <code>variant_group_dict</code> <p>A dictionary where the key is the group name and the value is a list of SNP rsIDs to group together.</p> required <p>Returns:</p> Type Description <p>A dictionary of <code>xarrayGenotypeMatrix</code> objects, one for each group.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def split_by_variants(self, variant_group_dict):\n    \"\"\"\n    Split the genotype matrix by variants into separate `xarrayGenotypeMatrix` objects\n    based on the groups defined in `variant_group_dict`. The dictionary should have\n    the group name as the key and the list of SNP rsIDs in that group as the value.\n\n    :param variant_group_dict: A dictionary where the key is the group name and the value\n    is a list of SNP rsIDs to group together.\n\n    :return: A dictionary of `xarrayGenotypeMatrix` objects, one for each group.\n    \"\"\"\n\n    split = super().split_by_variants(variant_group_dict)\n\n    for g, gt in split.items():\n        gt.xr_mat = self.xr_mat\n        gt.filter_snps(extract_snps=gt.snps)\n\n    return split\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.to_csr","title":"<code>to_csr(dtype=np.int8)</code>","text":"<p>Convert the genotype matrix to a scipy sparse CSR matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>The data type of the scipy array. Default: Int8</p> <code>int8</code> <p>Returns:</p> Type Description <p>A <code>scipy</code> sparse CSR matrix representation of the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def to_csr(self, dtype=np.int8):\n    \"\"\"\n    Convert the genotype matrix to a scipy sparse CSR matrix.\n    :param dtype: The data type of the scipy array. Default: Int8\n\n    :return: A `scipy` sparse CSR matrix representation of the genotype matrix.\n    \"\"\"\n\n    mat = self.to_numpy(dtype=dtype)\n\n    from scipy.sparse import csr_matrix\n\n    return csr_matrix(mat)\n</code></pre>"},{"location":"api/GenotypeMatrix/#magenpy.GenotypeMatrix.xarrayGenotypeMatrix.to_numpy","title":"<code>to_numpy(dtype=np.int8)</code>","text":"<p>Convert the genotype matrix to a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>The data type of the numpy array. Default: Int8</p> <code>int8</code> <p>Returns:</p> Type Description <p>A numpy array representation of the genotype matrix.</p> Source code in <code>magenpy/GenotypeMatrix.py</code> <pre><code>def to_numpy(self, dtype=np.int8):\n    \"\"\"\n    Convert the genotype matrix to a numpy array.\n    :param dtype: The data type of the numpy array. Default: Int8\n\n    :return: A numpy array representation of the genotype matrix.\n    \"\"\"\n\n    return self.xr_mat.data.astype(dtype).compute()\n</code></pre>"},{"location":"api/LDMatrix/","title":"LDMatrix","text":"<p>               Bases: <code>object</code></p> <p>A class that represents Linkage-Disequilibrium (LD) matrices, which record the SNP-by-SNP pairwise correlations in a sample of genetic data. The class provides various functionalities for initializing, storing, loading, and performing computations with LD matrices. The LD matrices are stored in a hierarchical format using the <code>Zarr</code> library, which allows for efficient storage and retrieval of the data.</p> <p>The class provides the following functionalities:</p> <ul> <li>Initialize an <code>LDMatrix</code> object from plink's LD table files.</li> <li>Initialize an <code>LDMatrix</code> object from a sparse CSR matrix.</li> <li>Initialize an <code>LDMatrix</code> object from a Zarr array store.</li> <li>Compute LD scores for each SNP in the LD matrix.</li> <li>Filter the LD matrix based on SNP indices or ranges.</li> <li>Perform linear algebra operations on LD matrices, including SVD, estimating extremal eigenvalues, and efficient matrix-vector multiplication.</li> </ul> <p>The Zarr hierarchy is structured as follows:</p> <ul> <li><code>chr_22.zarr</code>: The Zarr group.<ul> <li><code>matrix</code>: The subgroup containing the data of the LD matrix in Scipy Sparse CSR matrix format.<ul> <li><code>data</code>: The array containing the non-zero entries of the LD matrix.</li> <li><code>indptr</code>: The array containing the index pointers for the CSR matrix.</li> </ul> </li> <li><code>metadata</code>: The subgroup containing the metadata for variants included in the LD matrix.<ul> <li><code>snps</code>: The array containing the SNP rsIDs.</li> <li><code>a1</code>: The array containing the alternative alleles.</li> <li><code>a2</code>: The array containing the reference alleles.</li> <li><code>maf</code>: The array containing the minor allele frequencies.</li> <li><code>bp</code>: The array containing the base pair positions.</li> <li><code>cm</code>: The array containing the centi Morgan distance along the chromosome.</li> <li><code>ldscore</code>: The array containing the LD scores.</li> </ul> </li> <li><code>attrs</code>: A JSON-style metadata object containing general information about how the LD matrix was calculated, including the chromosome number, sample size, genome build, LD estimator, and estimator properties.</li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>_zg</code> <code>Group</code> <p>The Zarr group object containing the LD matrix and its metadata.</p> <code>_cached_lop</code> <code>Union[LDLinearOperator, None]</code> <p>A cached <code>LDLinearOperator</code> object for performing linear algebra operations.</p> <code>index</code> <code>int</code> <p>An integer index for the current SNP in the LD matrix (useful for iterators).</p> <code>_mask</code> <p>A boolean mask for filtering the LD matrix.</p> <code>_n_masked</code> <p>The number of SNPs masked (i.e. discarded) by the current mask.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>class LDMatrix(object):\n    \"\"\"\n    A class that represents Linkage-Disequilibrium (LD) matrices, which record\n    the SNP-by-SNP pairwise correlations in a sample of genetic data. The class\n    provides various functionalities for initializing, storing, loading, and\n    performing computations with LD matrices. The LD matrices are stored in a\n    hierarchical format using the `Zarr` library, which allows for efficient\n    storage and retrieval of the data.\n\n    The class provides the following functionalities:\n\n    * Initialize an `LDMatrix` object from plink's LD table files.\n    * Initialize an `LDMatrix` object from a sparse CSR matrix.\n    * Initialize an `LDMatrix` object from a Zarr array store.\n    * Compute LD scores for each SNP in the LD matrix.\n    * Filter the LD matrix based on SNP indices or ranges.\n    * Perform linear algebra operations on LD matrices, including SVD, estimating extremal eigenvalues,\n    and efficient matrix-vector multiplication.\n\n    The Zarr hierarchy is structured as follows:\n\n    * `chr_22.zarr`: The Zarr group.\n        * `matrix`: The subgroup containing the data of the LD matrix in Scipy Sparse CSR matrix format.\n            * `data`: The array containing the non-zero entries of the LD matrix.\n            * `indptr`: The array containing the index pointers for the CSR matrix.\n        * `metadata`: The subgroup containing the metadata for variants included in the LD matrix.\n            * `snps`: The array containing the SNP rsIDs.\n            * `a1`: The array containing the alternative alleles.\n            * `a2`: The array containing the reference alleles.\n            * `maf`: The array containing the minor allele frequencies.\n            * `bp`: The array containing the base pair positions.\n            * `cm`: The array containing the centi Morgan distance along the chromosome.\n            * `ldscore`: The array containing the LD scores.\n        * `attrs`: A JSON-style metadata object containing general information about how the LD matrix\n        was calculated, including the chromosome number, sample size, genome build, LD estimator,\n        and estimator properties.\n\n    :ivar _zg: The Zarr group object containing the LD matrix and its metadata.\n    :ivar _cached_lop: A cached `LDLinearOperator` object for performing linear algebra operations.\n    :ivar index: An integer index for the current SNP in the LD matrix (useful for iterators).\n    :ivar _mask: A boolean mask for filtering the LD matrix.\n    :ivar _n_masked: The number of SNPs masked (i.e. discarded) by the current mask.\n\n    \"\"\"\n\n    def __init__(self, zarr_group):\n        \"\"\"\n        Initialize an `LDMatrix` object from a Zarr group store.\n\n        :param zarr_group: The Zarr group object that stores the LD matrix.\n        \"\"\"\n\n        # Checking the input for correct formatting:\n        # First, it has to be a Zarr group:\n        assert isinstance(zarr_group, zarr.hierarchy.Group)\n        # Second, it has to have a group called `matrix`:\n        assert 'matrix' in list(zarr_group.group_keys())\n\n        # Third, all the sparse array keys must be present:\n        arr_keys = list(zarr_group['matrix'].array_keys())\n        assert all([arr in arr_keys\n                    for arr in ('data', 'indptr')])\n\n        # The Zarr storage hierarchy:\n        self._zg: zarr.hierarchy.Group = zarr_group\n\n        # Caching loaded/processed data of the LD matrix:\n        self._cached_lop: Union[LDLinearOperator, None] = None\n\n        # To support dynamically filtering the LD matrix:\n        self._mask = None\n        self._n_masked = 0\n\n        # To support iteration over the LD matrix:\n        self.index: int = 0\n\n    @classmethod\n    def from_path(cls, ld_store_path, cache_size=None):\n        \"\"\"\n        Initialize an `LDMatrix` object from a pre-computed Zarr group store. This is a genetic method\n        that can work with both cloud-based stores (e.g. s3 storage) or local filesystems.\n\n        :param ld_store_path: The path to the Zarr array store.\n        :param cache_size: The size of the cache for the Zarr store (in bytes). Default is `None` (no caching).\n\n        !!! seealso \"See Also\"\n            * [from_directory][magenpy.LDMatrix.LDMatrix.from_directory]\n            * [from_s3][magenpy.LDMatrix.LDMatrix.from_s3]\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        if 's3://' in ld_store_path:\n            return cls.from_s3(ld_store_path, cache_size)\n        else:\n            return cls.from_directory(ld_store_path, cache_size)\n\n    @classmethod\n    def from_s3(cls, s3_path, cache_size=None):\n        \"\"\"\n        Initialize an `LDMatrix` object from a Zarr group store hosted on AWS s3 storage.\n\n        :param s3_path: The path to the Zarr group store on AWS s3. s3 paths are expected\n        to be of the form `s3://bucket-name/path/to/zarr-store`.\n        :param cache_size: The size of the cache for the Zarr store (in bytes). Default is 16MB.\n\n        .. note::\n            Requires installing the `s3fs` package to access the Zarr store on AWS s3.\n\n        !!! seealso \"See Also\"\n            * [from_path][magenpy.LDMatrix.LDMatrix.from_path]\n            * [from_directory][magenpy.LDMatrix.LDMatrix.from_directory]\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        import s3fs\n\n        s3 = s3fs.S3FileSystem(anon=True, client_kwargs=dict(region_name='us-east-2'))\n        store = s3fs.S3Map(root=s3_path.replace('s3://', ''), s3=s3, check=False)\n        if cache_size is not None:\n            store = zarr.LRUStoreCache(store, max_size=cache_size)\n        ld_group = zarr.open_group(store=store, mode='r')\n\n        return cls(ld_group)\n\n    @classmethod\n    def from_directory(cls, dir_path, cache_size=None):\n        \"\"\"\n        Initialize an `LDMatrix` object from a Zarr array store.\n        :param dir_path: The path to the Zarr array store on the local filesystem.\n        :param cache_size: The size of the cache for the Zarr store (in bytes).  Default is `None` (no caching).\n\n        !!! seealso \"See Also\"\n            * [from_s3][magenpy.LDMatrix.LDMatrix.from_s3]\n            * [from_path][magenpy.LDMatrix.LDMatrix.from_path]\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        for level in range(2):\n            try:\n                dir_store = zarr.storage.DirectoryStore(dir_path)\n                if cache_size is not None:\n                    dir_store = zarr.LRUStoreCache(dir_store, max_size=cache_size)\n                ld_group = zarr.open_group(dir_store, mode='r')\n                return cls(ld_group)\n            except zarr.hierarchy.GroupNotFoundError as e:\n                if level &lt; 1:\n                    dir_path = osp.dirname(dir_path)\n                else:\n                    raise e\n\n    @classmethod\n    def from_csr(cls,\n                 csr_mat,\n                 store_path,\n                 overwrite=False,\n                 dtype='int16',\n                 compressor_name='zstd',\n                 compression_level=7):\n        \"\"\"\n        Initialize an LDMatrix object from a sparse CSR matrix.\n\n        TODO: Determine the chunksize based on the avg neighborhood size?\n\n        :param csr_mat: The sparse CSR matrix.\n        :param store_path: The path to the Zarr LD store where the data will be stored.\n        :param overwrite: If True, it overwrites the LD store at `store_path`.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n        :param compression_level: The compression level to use with the compressor (1-9).\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        from scipy.sparse import triu\n\n        dtype = np.dtype(dtype)\n\n        # Get the upper triangular part of the matrix:\n        triu_mat = triu(csr_mat, k=1, format='csr')\n\n        # Check that the non-zeros are contiguous around the diagonal with no gaps.\n        # If there are gaps, eliminate them or raise an error.\n        if np.diff(triu_mat.indices).max() &gt; 1:\n            # TODO: Figure out a way to fix this automatically for the user?\n            raise ValueError(\"The non-zero entries of the LD matrix are not contiguous around the diagonal.\")\n\n        # Create hierarchical storage with zarr groups:\n        store = zarr.DirectoryStore(store_path)\n        z = zarr.group(store=store, overwrite=overwrite)\n\n        # Create a compressor object:\n        compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n        # First sub-hierarchy stores the information for the sparse LD matrix:\n        mat = z.create_group('matrix')\n        if np.issubdtype(dtype, np.integer):\n            mat.array('data', quantize(triu_mat.data, int_dtype=dtype), dtype=dtype, compressor=compressor)\n        else:\n            mat.array('data', triu_mat.data.astype(dtype), dtype=dtype, compressor=compressor_name)\n\n        # Store the index pointer:\n        mat.array('indptr', triu_mat.indptr, dtype=np.int64, compressor=compressor)\n\n        return cls(z)\n\n    @classmethod\n    def from_plink_table(cls,\n                         plink_ld_file,\n                         snps,\n                         store_path,\n                         ld_boundaries=None,\n                         pandas_chunksize=None,\n                         overwrite=False,\n                         dtype='int16',\n                         compressor_name='zstd',\n                         compression_level=7):\n        \"\"\"\n        Construct a Zarr LD matrix using LD tables generated by plink1.9.\n\n        TODO: Determine the chunksize based on the avg neighborhood size?\n\n        :param plink_ld_file: The path to the plink LD table file.\n        :param snps: An iterable containing the list of ordered SNP rsIDs to be included in the LD matrix.\n        :param store_path: The path to the Zarr LD store.\n        :param ld_boundaries: The LD boundaries for each SNP in the LD matrix (delineates the indices of\n        the leftmost and rightmost neighbors of each SNP). If not provided, the LD matrix will be constructed\n        using the full LD table from plink.\n        :param pandas_chunksize: If the LD table is large, provide chunk size\n        (i.e. number of rows to process at each step) to keep memory footprint manageable.\n        :param overwrite: If True, it overwrites the LD store at `store_path`.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n        :param compression_level: The compression level to use with the compressor (1-9).\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        dtype = np.dtype(dtype)\n\n        # Create hierarchical storage with zarr groups:\n        store = zarr.DirectoryStore(store_path)\n        z = zarr.group(store=store, overwrite=overwrite)\n\n        # Create a compressor object:\n        compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n        # First sub-hierarchy stores the information for the sparse LD matrix:\n        mat = z.create_group('matrix')\n        mat.empty('data', shape=len(snps)**2, dtype=dtype, compressor=compressor)\n\n        if ld_boundaries is not None:\n            use_cols = ['SNP_A', 'SNP_B', 'R']\n            bounds_df = pd.DataFrame(np.column_stack((np.arange(len(snps)).reshape(-1, 1),\n                                                      ld_boundaries[1:, :].T)),\n                                     columns=['SNP_idx', 'end'])\n        else:\n            use_cols = ['SNP_A', 'R']\n\n        # Create a chunked iterator with pandas:\n        # Chunk size will correspond to the average chunk size for the Zarr array:\n        ld_chunks = pd.read_csv(plink_ld_file,\n                                sep=r'\\s+',\n                                usecols=use_cols,\n                                engine='c',\n                                chunksize=pandas_chunksize,\n                                dtype={'SNP_A': str, 'R': np.float32})\n\n        if pandas_chunksize is None:\n            ld_chunks = [ld_chunks]\n\n        # Create a dictionary mapping SNPs to their indices:\n        snp_idx = pd.Series(np.arange(len(snps), dtype=np.int32), index=snps)\n\n        indptr_counts = np.zeros(len(snps), dtype=np.int32)\n\n        total_len = 0\n\n        # For each chunk in the LD file:\n        for ld_chunk in ld_chunks:\n\n            # Fill N/A in R before storing it:\n            ld_chunk.fillna({'R': 0.}, inplace=True)\n\n            # If LD boundaries are provided, filter the LD table accordingly:\n            if ld_boundaries is not None:\n\n                row_index = snp_idx[ld_chunk['SNP_A'].values]\n\n                ld_chunk['SNP_A_index'] = snp_idx[ld_chunk['SNP_A'].values].values\n                ld_chunk['SNP_B_index'] = snp_idx[ld_chunk['SNP_B'].values].values\n\n                ld_chunk = ld_chunk.merge(bounds_df, left_on='SNP_A_index', right_on='SNP_idx')\n                ld_chunk = ld_chunk.loc[(ld_chunk['SNP_B_index'] &gt;= ld_chunk['SNP_A_index'] + 1) &amp;\n                                        (ld_chunk['SNP_B_index'] &lt; ld_chunk['end'])]\n\n            # Create an indexed LD chunk:\n            row_index = snp_idx[ld_chunk['SNP_A'].values]\n\n            # Add LD data to the zarr array:\n            if np.issubdtype(dtype, np.integer):\n                mat['data'][total_len:total_len + len(ld_chunk)] = quantize(ld_chunk['R'].values, int_dtype=dtype)\n            else:\n                mat['data'][total_len:total_len + len(ld_chunk)] = ld_chunk['R'].values.astype(dtype)\n\n            total_len += len(ld_chunk)\n\n            # Count the number of occurrences of each SNP in the chunk:\n            snp_counts = row_index.value_counts()\n\n            # Add the number of entries to indptr_counts:\n            indptr_counts[snp_counts.index] += snp_counts.values\n\n        # Get the final indptr by computing cumulative sum:\n        indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n        # Store indptr in the zarr group:\n        mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n        # Resize the data array:\n        mat['data'].resize(total_len)\n\n        return cls(z)\n\n    @classmethod\n    def from_dense_zarr_matrix(cls,\n                               dense_zarr,\n                               ld_boundaries,\n                               store_path,\n                               overwrite=False,\n                               delete_original=False,\n                               dtype='int16',\n                               compressor_name='zstd',\n                               compression_level=7):\n        \"\"\"\n         Initialize a new LD matrix object using a Zarr array object. This method is\n         useful for converting a dense LD matrix computed using Dask (or other distributed computing\n         software) to a sparse or banded one.\n\n         TODO: Determine the chunksize based on the avg neighborhood size?\n\n         :param dense_zarr: The path to the dense Zarr array object.\n         :param ld_boundaries: The LD boundaries for each SNP in the LD matrix (delineates the indices of\n            the leftmost and rightmost neighbors of each SNP).\n         :param store_path: The path where to store the new LD matrix.\n         :param overwrite: If True, it overwrites the LD store at `store_path`.\n         :param delete_original: If True, it deletes the original dense LD matrix.\n         :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n            and integer quantized data types int8 and int16).\n         :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n         :param compression_level: The compression level to use with the compressor (1-9).\n\n         :return: An `LDMatrix` object.\n        \"\"\"\n\n        dtype = np.dtype(dtype)\n\n        # If dense_zarr is a path, rather than a Zarr Array object, then\n        # open it as a Zarr array object before proceeding:\n        if isinstance(dense_zarr, str):\n            if osp.isfile(osp.join(dense_zarr, '.zarray')):\n                dense_zarr = zarr.open(dense_zarr)\n            else:\n                raise FileNotFoundError\n\n        # Create hierarchical storage with zarr groups:\n        store = zarr.DirectoryStore(store_path)\n        z = zarr.group(store=store, overwrite=overwrite)\n\n        # Create a compressor object:\n        compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n        # First sub-hierarchy stores the information for the sparse LD matrix:\n        mat = z.create_group('matrix')\n        mat.empty('data', shape=dense_zarr.shape[0]**2, dtype=dtype, compressor=compressor)\n\n        num_rows = dense_zarr.shape[0]\n        chunk_size = dense_zarr.chunks[0]\n\n        indptr_counts = np.zeros(num_rows, dtype=np.int32)\n\n        total_len = 0\n\n        for chunk_idx in range(int(np.ceil(num_rows / chunk_size))):\n\n            chunk_start = chunk_idx * chunk_size\n            chunk_end = min((chunk_idx + 1) * chunk_size, num_rows)\n\n            z_chunk = dense_zarr[chunk_start: chunk_end]\n\n            data = []\n\n            chunk_len = 0\n\n            for j in range(chunk_start, chunk_end):\n\n                data.append(\n                    z_chunk[j - chunk_start][j + 1:ld_boundaries[1, j]]\n                )\n                indptr_counts[j] = len(data[-1])\n                chunk_len += int(ld_boundaries[1, j] - (j+1))\n\n            # Add data + columns indices to zarr array:\n            concat_data = np.concatenate(data)\n\n            if np.issubdtype(dtype, np.integer):\n                mat['data'][total_len:total_len + chunk_len] = quantize(concat_data, int_dtype=dtype)\n            else:\n                mat['data'][total_len:total_len + chunk_len] = concat_data.astype(dtype)\n\n            total_len += chunk_len\n\n        # Get the final indptr by computing cumulative sum:\n        indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n        # Store indptr in the zarr array:\n        mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n        # Resize the data and indices arrays:\n        mat['data'].resize(total_len)\n\n        if delete_original:\n            from .stats.ld.utils import delete_ld_store\n            delete_ld_store(dense_zarr)\n\n        return cls(z)\n\n    @classmethod\n    def from_ragged_zarr_matrix(cls,\n                                ragged_zarr,\n                                store_path,\n                                overwrite=False,\n                                delete_original=False,\n                                dtype='int16',\n                                compressor_name='zstd',\n                                compression_level=7):\n        \"\"\"\n        Initialize a new LD matrix object using a Zarr array object\n        conforming to the old LD Matrix format from magenpy v&lt;=0.0.12.\n\n        This utility function will also copy some of the stored attributes\n        associated with the matrix in the old format.\n\n        TODO: Determine the chunksize based on the avg neighborhood size?\n\n        :param ragged_zarr: The path to the ragged Zarr array object.\n        :param store_path: The path where to store the new LD matrix.\n        :param overwrite: If True, it overwrites the LD store at `store_path`.\n        :param delete_original: If True, it deletes the original ragged LD matrix.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n        :param compression_level: The compression level to use with the compressor (1-9).\n\n        :return: An `LDMatrix` object.\n        \"\"\"\n\n        dtype = np.dtype(dtype)\n\n        # If ragged_zarr is a path, rather than a Zarr Array object, then\n        # open it as a Zarr array object before proceeding:\n        if isinstance(ragged_zarr, str):\n            if osp.isfile(osp.join(ragged_zarr, '.zarray')):\n                ragged_zarr = zarr.open(ragged_zarr)\n            else:\n                raise FileNotFoundError\n\n        num_rows = ragged_zarr.shape[0]\n        chunk_size = ragged_zarr.chunks[0]\n\n        # Create hierarchical storage with zarr groups:\n        store = zarr.DirectoryStore(store_path)\n        z = zarr.group(store=store, overwrite=overwrite)\n\n        # Create a compressor object:\n        compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n        # First sub-hierarchy stores the information for the sparse LD matrix:\n        mat = z.create_group('matrix')\n        mat.empty('data', shape=num_rows ** 2, dtype=dtype, compressor=compressor)\n\n        indptr_counts = np.zeros(num_rows, dtype=np.int64)\n\n        # Get the LD boundaries from the Zarr array attributes:\n        ld_boundaries = np.array(ragged_zarr.attrs['LD boundaries'])\n\n        total_len = 0\n\n        for chunk_idx in range(int(np.ceil(num_rows / chunk_size))):\n\n            chunk_start = chunk_idx * chunk_size\n            chunk_end = min((chunk_idx + 1) * chunk_size, num_rows)\n\n            z_chunk = ragged_zarr[chunk_start: chunk_end]\n\n            data = []\n            chunk_len = 0\n\n            for j in range(chunk_start, chunk_end):\n\n                start, end = ld_boundaries[:, j]\n                new_start = (j - start) + 1\n\n                data.append(\n                    z_chunk[j - chunk_start][new_start:]\n                )\n                indptr_counts[j] = end - (j + 1)\n                chunk_len += int(end - (j + 1))\n\n            # Add data + columns indices to zarr array:\n            concat_data = np.concatenate(data)\n\n            if np.issubdtype(dtype, np.integer):\n                mat['data'][total_len:total_len + chunk_len] = quantize(concat_data, int_dtype=dtype)\n            else:\n                mat['data'][total_len:total_len + chunk_len] = concat_data.astype(dtype)\n\n            total_len += chunk_len\n\n        # Get the final indptr by computing cumulative sum:\n        indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n        # Store indptr in the zarr array:\n        mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n        # Resize the data and indices arrays:\n        mat['data'].resize(total_len)\n\n        # ============================================================\n        # Transfer the attributes/metadata from the old matrix format:\n\n        ld_mat = cls(z)\n\n        ld_mat.set_metadata('snps', np.array(ragged_zarr.attrs['SNP']))\n        ld_mat.set_metadata('a1', np.array(ragged_zarr.attrs['A1']))\n        ld_mat.set_metadata('a2', np.array(ragged_zarr.attrs['A2']))\n        ld_mat.set_metadata('maf', np.array(ragged_zarr.attrs['MAF']))\n        ld_mat.set_metadata('bp', np.array(ragged_zarr.attrs['BP']))\n        ld_mat.set_metadata('cm', np.array(ragged_zarr.attrs['cM']))\n\n        try:\n            ld_mat.set_metadata('ldscore', np.array(ragged_zarr.attrs['LDScore']))\n        except KeyError:\n            pass\n\n        # Set matrix attributes:\n        ld_mat.set_store_attr('Chromosome', ragged_zarr.attrs['Chromosome'])\n        ld_mat.set_store_attr('LD estimator', ragged_zarr.attrs['LD estimator'])\n        ld_mat.set_store_attr('Estimator properties', ragged_zarr.attrs['Estimator properties'])\n        ld_mat.set_store_attr('Sample size', ragged_zarr.attrs['Sample size'])\n\n        if delete_original:\n            from .stats.ld.utils import delete_ld_store\n            delete_ld_store(ragged_zarr)\n\n        return ld_mat\n\n    @property\n    def n_snps(self):\n        \"\"\"\n        :return: The number of variants in the LD matrix. If a mask is set, we return the\n        number of variants included in the mask.\n        \"\"\"\n        if self._cached_lop is not None:\n            return self._cached_lop.shape[0]\n        else:\n            return self.stored_n_snps - self._n_masked\n\n    @property\n    def shape(self):\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [n_snps][magenpy.LDMatrix.LDMatrix.n_snps]\n\n        :return: The shape of the square LD matrix.\n        \"\"\"\n        return self.n_snps, self.n_snps\n\n    @property\n    def in_memory(self):\n        \"\"\"\n        :return: A boolean flag indicating whether the LD matrix is in memory.\n        \"\"\"\n        return self._cached_lop is not None\n\n    @property\n    def is_symmetric(self):\n        \"\"\"\n        :return: A boolean flag indicating whether the loaded LD matrix is symmetric.\n        \"\"\"\n        if self.in_memory:\n            return self._cached_lop.symmetric\n        else:\n            return False\n\n    @property\n    def store(self):\n        \"\"\"\n        :return: The Zarr group store object.\n        \"\"\"\n        return self._zg.store\n\n    @property\n    def compressor(self):\n        \"\"\"\n        :return: The `numcodecs` compressor object for the LD data.\n        \"\"\"\n        return self._zg['matrix/data'].compressor\n\n    @property\n    def zarr_group(self):\n        \"\"\"\n        :return: The Zarr group object that stores the LD matrix and its metadata.\n        \"\"\"\n        return self._zg\n\n    @property\n    def chunks(self):\n        \"\"\"\n        :return: The chunks for the data array of the LD matrix.\n        \"\"\"\n        return self._zg['matrix/data'].chunks\n\n    @property\n    def chunk_size(self):\n        \"\"\"\n        :return: The chunk size for the data array of the LD matrix.\n        \"\"\"\n        return self.chunks[0]\n\n    @property\n    def stored_n_snps(self):\n        \"\"\"\n        :return: The number of variants stored in the LD matrix (irrespective of any masks / filters).\n        \"\"\"\n        return self._zg['matrix/indptr'].shape[0] - 1\n\n    @property\n    def stored_dtype(self):\n        \"\"\"\n        :return: The data type for the stored entries of `data` array of the LD matrix.\n        \"\"\"\n        return self._zg['matrix/data'].dtype\n\n    @property\n    def stored_shape(self):\n        \"\"\"\n        :return: The shape of the stored LD matrix (irrespective of any masks / filters).\n        \"\"\"\n        n_snps = self.stored_n_snps\n        return n_snps, n_snps\n\n    @property\n    def dtype(self):\n        \"\"\"\n        :return: The data type for the entries of the `data` array of the LD matrix. If the matrix is\n        in memory, return the dtype of the CSR matrix. Otherwise, return the\n        dtype of the entries in the Zarr array.\n        \"\"\"\n        if self.in_memory:\n            return self._cached_lop.ld_data_type\n        else:\n            return self.stored_dtype\n\n    @property\n    def chromosome(self):\n        \"\"\"\n        :return: The chromosome for which this LD matrix was calculated.\n        \"\"\"\n        return self.get_store_attr('Chromosome')\n\n    @property\n    def ld_estimator(self):\n        \"\"\"\n        :return: The LD estimator used to compute the LD matrix. Examples include: `block`, `windowed`, `shrinkage`.\n        \"\"\"\n        return self.get_store_attr('LD estimator')\n\n    @property\n    def estimator_properties(self):\n        \"\"\"\n        :return: The properties of the LD estimator used to compute the LD matrix.\n        \"\"\"\n        return self.get_store_attr('Estimator properties')\n\n    @property\n    def sample_size(self):\n        \"\"\"\n        :return: The sample size used to compute the LD matrix.\n        \"\"\"\n        return self.get_store_attr('Sample size')\n\n    @property\n    def dequantization_scale(self):\n        \"\"\"\n        :return: The dequantization scale for the quantized LD matrix. If the matrix is not quantized, returns 1.\n        \"\"\"\n        if np.issubdtype(self.stored_dtype, np.integer):\n            return 1./np.iinfo(self.stored_dtype).max\n        else:\n            return 1.\n\n    @property\n    def genome_build(self):\n        \"\"\"\n        :return: The genome build based on which the base pair coordinates are defined.\n        \"\"\"\n        return self.get_store_attr('Genome build')\n\n    @property\n    def snps(self):\n        \"\"\"\n        :return: rsIDs of the variants included in the LD matrix.\n        \"\"\"\n        return self.get_metadata('snps')\n\n    @property\n    def a1(self):\n        \"\"\"\n        :return: The alternative alleles of the variants included in the LD matrix.\n        \"\"\"\n        return self.get_metadata('a1')\n\n    @property\n    def a2(self):\n        \"\"\"\n        :return: The reference alleles of the variants included in the LD matrix.\n        \"\"\"\n        return self.get_metadata('a2')\n\n    @property\n    def maf(self):\n        \"\"\"\n        :return: The minor allele frequency (MAF) of the alternative allele (A1) in the LD matrix.\n        \"\"\"\n        try:\n            return self.get_metadata('maf')\n        except KeyError:\n            return None\n\n    @property\n    def bp_position(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [genome_build][magenpy.LDMatrix.LDMatrix.genome_build]\n\n        :return: The base pair position of each SNP in the LD matrix.\n        \"\"\"\n        return self.get_metadata('bp')\n\n    @property\n    def cm_position(self):\n        \"\"\"\n        :return: The centi Morgan (cM) position of each variant in the LD matrix.\n        \"\"\"\n        try:\n            return self.get_metadata('cm')\n        except KeyError:\n            return None\n\n    @property\n    def ld_score(self):\n        \"\"\"\n        :return: The LD score of each variant in the LD matrix.\n        \"\"\"\n        try:\n            return self.get_metadata('ldscore')\n        except KeyError:\n\n            ld_score = self.compute_ld_scores()\n\n            if self._mask is None:\n                self.set_metadata('ldscore', ld_score, overwrite=True)\n\n            return ld_score\n\n    @property\n    def ld_boundaries(self):\n        \"\"\"\n        The LD boundaries associated with each variant.\n        The LD boundaries are defined as the index of the leftmost neighbor\n        (lower boundary) and the rightmost neighbor (upper boundary) of for each variant.\n        If the LD matrix is upper triangular, then the boundaries for variant `i` go from `i + 1` to `i + k_i`,\n        where `k_i` is the number of neighbors that SNP `i` is in LD with.\n\n        :return: A matrix of shape `(2, n_snps)` where the first row contains the lower boundaries and the second row\n        contains the upper boundaries.\n\n        \"\"\"\n\n        indptr = self.indptr\n        leftmost_idx = self.leftmost_index\n\n        return np.vstack([leftmost_idx, leftmost_idx + np.diff(indptr)]).astype(np.int32)\n\n    @property\n    def window_size(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n_neighbors][magenpy.LDMatrix.LDMatrix.n_neighbors]\n\n        :return: The number of variants in the LD window for each SNP.\n\n        \"\"\"\n\n        if self.in_memory and self.is_symmetric:\n            indptr = self.indptr\n        else:\n            from .stats.ld.c_utils import get_symmetrized_indptr\n            indptr, _ = get_symmetrized_indptr(self.indptr[:])\n\n        return np.diff(indptr)\n\n    @property\n    def n_neighbors(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [window_size][magenpy.LDMatrix.LDMatrix.window_size]\n\n        :return: The number of variants in the LD window for each SNP.\n\n        \"\"\"\n        return self.window_size\n\n    @property\n    def data(self):\n        \"\"\"\n        :return: The `data` array of the sparse `CSR` matrix, containing the entries of the LD matrix.\n        \"\"\"\n        if self.in_memory:\n            return self._cached_lop.ld_data\n        else:\n            return self._zg['matrix/data']\n\n    @property\n    def leftmost_index(self):\n        \"\"\"\n        :return: The index of the leftmost neighbor of each variant in the LD matrix.\n        \"\"\"\n        if self.in_memory:\n            return self._cached_lop.leftmost_idx\n        else:\n            return np.arange(1, len(self.indptr) - 1)\n\n    @property\n    def indices(self):\n        \"\"\"\n        :return: The column indices of the non-zero elements of the sparse, CSR representation of the LD matrix.\n        \"\"\"\n\n        ld_bounds = self.ld_boundaries\n\n        from .stats.ld.c_utils import expand_ranges\n\n        return expand_ranges(ld_bounds[0], ld_bounds[1], self.data.shape[0])\n\n    @property\n    def row_indices(self):\n        \"\"\"\n        :return: The row indices of the non-zero elements of the sparse, CSR representation of the LD matrix\n        \"\"\"\n        indptr = self.indptr\n        return np.repeat(np.arange(len(indptr) - 1), np.diff(indptr))\n\n    @property\n    def indptr(self):\n        \"\"\"\n        :return: The index pointers `indptr` delineating where the data for each row of the flattened,\n        sparse CSR representation of the lD matrix.\n        \"\"\"\n        if self.in_memory:\n            return self._cached_lop.ld_indptr\n        else:\n            return self._zg['matrix/indptr']\n\n    @property\n    def is_mask_set(self):\n        \"\"\"\n        :return: A boolean flag indicating whether a mask is set for the LD matrix.\n        \"\"\"\n        return self._mask is not None\n\n    def get_long_range_ld_variants(self, return_value='snps'):\n        \"\"\"\n        A utility method to exclude variants that are in long-range LD regions. The\n        boundaries of those regions are derived from here:\n\n        https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n\n        Which is based on the work of\n\n        &gt; Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\"\n        Nature protocols 5.9 (2010): 1564-1573.\n\n        :param return_value: The value to return. Options are 'mask', 'index', 'snps'. If `mask`, then\n        it returns a boolean array of which variants are in long-range LD regions. If `index`, then it returns\n        the index of those variants. If `snps`, then it returns the rsIDs of those variants.\n\n        :return: An array of the variants that are in long-range LD regions.\n        \"\"\"\n\n        assert return_value in ('mask', 'index', 'snps')\n\n        from .parsers.annotation_parsers import parse_annotation_bed_file\n        from .utils.data_utils import lrld_path\n\n        bed_df = parse_annotation_bed_file(lrld_path())\n\n        # Filter to only regions specific to the chromosome of this matrix:\n        bed_df = bed_df.loc[bed_df['CHR'] == self.chromosome]\n\n        bp_pos = self.bp_position\n        snp_mask = np.zeros(len(bp_pos), dtype=bool)\n\n        # Loop over the LRLD region on this chromosome and include the SNPs in these regions:\n        for _, row in bed_df.iterrows():\n            start, end = row['Start'], row['End']\n            snp_mask |= ((bp_pos &gt;= start) &amp; (bp_pos &lt;= end))\n\n        if return_value == 'mask':\n            return snp_mask\n        elif return_value == 'index':\n            return np.where(snp_mask)[0]\n        else:\n            return self.snps[snp_mask]\n\n    def filter_long_range_ld_regions(self):\n        \"\"\"\n        A utility method to exclude variants that are in long-range LD regions. The\n        boundaries of those regions are derived from here:\n\n        https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n\n        Which is based on the work of\n\n        &gt; Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\"\n        Nature protocols 5.9 (2010): 1564-1573.\n        \"\"\"\n\n        # Filter the SNP to only those not in the LRLD regions:\n        self.filter_snps(self.snps[~self.get_long_range_ld_variants(return_value='mask')])\n\n    def filter_snps(self, extract_snps=None, extract_file=None):\n        \"\"\"\n        Filter the LDMatrix to keep a subset of variants. This mainly sets\n        the mask for the LD matrix, which is used to hide/remove some SNPs from the LD matrix,\n        without altering the stored objects on-disk.\n\n        :param extract_snps: A list or array of SNP rsIDs to keep.\n        :param extract_file: A plink-style file containing the SNP rsIDs to keep.\n        \"\"\"\n\n        assert extract_snps is not None or extract_file is not None\n\n        if extract_snps is None:\n            from .parsers.misc_parsers import read_snp_filter_file\n            extract_snps = read_snp_filter_file(extract_file)\n\n        from .utils.compute_utils import intersect_arrays\n\n        new_mask = intersect_arrays(self.get_metadata('snps', apply_mask=False),\n                                    extract_snps,\n                                    return_index=True)\n\n        self.set_mask(new_mask)\n\n    def get_mask(self):\n        \"\"\"\n        :return: The mask (a boolean flag array) used to hide/remove some SNPs from the LD matrix.\n        \"\"\"\n        return self._mask\n\n    def set_mask(self, mask):\n        \"\"\"\n        Set the mask (a boolean array) to hide/remove some SNPs from the LD matrix.\n        :param mask: An array of indices or boolean mask for SNPs to retain.\n        \"\"\"\n\n        # Check that mask is a numpy array:\n        if not isinstance(mask, np.ndarray):\n            raise ValueError(\"Mask must be a numpy array.\")\n\n        # Check that mask is either a boolean array or an array of indices:\n        if mask.dtype != bool and not np.issubdtype(mask.dtype, np.integer):\n            raise ValueError(\"Mask must be a boolean array or an array of indices.\")\n\n        # If mask is a boolean array, ensure that it matches the number of stored SNPs:\n        if mask.dtype == bool and len(mask) != self.stored_n_snps:\n            raise ValueError(\"Boolean mask must have the same length as the number of stored SNPs.\")\n\n        # If the mask is equivalent to the current mask, return:\n        if np.array_equal(mask, self._mask):\n            return\n\n        # If the mask consists of indices, convert to boolean mask:\n        if mask.dtype != bool:\n            self._mask = np.zeros(self.stored_n_snps, dtype=bool)\n            self._mask[mask] = True\n        else:\n            self._mask = mask\n\n        self._n_masked = np.sum(~self._mask)\n\n        # If the data has already been loaded to memory, reload:\n        if self.in_memory:\n            self.load(force_reload=True,\n                      return_symmetric=self.is_symmetric,\n                      dtype=self.dtype)\n\n    def reset_mask(self):\n        \"\"\"\n        Reset the mask to its default value (None).\n        \"\"\"\n\n        self._mask = None\n        self._n_masked = 0\n\n        if self.in_memory:\n            self.load(force_reload=True,\n                      return_symmetric=self.is_symmetric,\n                      dtype=self.dtype)\n\n    def prune(self, threshold):\n        \"\"\"\n        Perform LD pruning to remove variants that are in high LD with other variants.\n        If two variants are in high LD, this function keeps the variant that occurs\n        earlier in the matrix. This behavior will be updated in the future to allow\n        for arbitrary ordering of variants.\n\n        !!! note\n            Experimental for now. Needs further testing &amp; improvement.\n\n        :param threshold: The absolute value of the Pearson correlation coefficient above which to prune variants. A\n        positive floating point number between 0. and 1.\n        :return: A boolean array indicating whether a variant is kept after pruning.\n        \"\"\"\n\n        from .stats.ld.c_utils import prune_ld_ut\n\n        assert 0. &lt; threshold &lt;= 1.\n\n        if np.issubdtype(self.dtype, np.integer):\n            threshold = quantize(np.array([threshold]), int_dtype=self.dtype)[0]\n\n        return prune_ld_ut(self.indptr[:], self.data[:], threshold)\n\n    def to_snp_table(self, col_subset=None, use_original_index=False):\n        \"\"\"\n        :param col_subset: The subset of columns to add to the table. If None, it returns\n        all available columns.\n        :param use_original_index: If True, it uses the original index of the SNPs in the LD matrix (\n        before applying any filters).\n\n        :return: A `pandas` dataframe of the SNP attributes and metadata for variants\n        included in the LD matrix.\n        \"\"\"\n\n        col_subset = col_subset or ['CHR', 'SNP', 'POS', 'A1', 'A2', 'MAF', 'LDScore']\n\n        # Create the index according to the original SNP order:\n        if use_original_index:\n            original_index = np.arange(self.stored_n_snps)\n            if self._mask is not None:\n                original_index = original_index[self._mask]\n        else:\n            original_index = None\n\n        table = pd.DataFrame({'SNP': self.snps}, index=original_index)\n\n        for col in col_subset:\n            if col == 'CHR':\n                table['CHR'] = self.chromosome\n            if col == 'POS':\n                table['POS'] = self.bp_position\n            if col == 'cM':\n                table['cM'] = self.cm_position\n            if col == 'A1':\n                table['A1'] = self.a1\n            if col == 'A2':\n                table['A2'] = self.a2\n            if col == 'MAF':\n                table['MAF'] = self.maf\n            if col == 'LDScore':\n                table['LDScore'] = self.ld_score\n            if col == 'WindowSize':\n                table['WindowSize'] = self.window_size\n\n        return table[list(col_subset)]\n\n    def compute_ld_scores(self,\n                          annotation_matrix=None,\n                          corrected=True,\n                          chunk_size=10_000):\n        \"\"\"\n\n        Computes the LD scores for variants in the LD matrix. LD Scores are defined\n        as the sum of the squared pairwise Pearson Correlation coefficient between the focal SNP and\n        all its neighboring SNPs. See Bulik-Sullivan et al. (2015) for details.\n\n        :param annotation_matrix: A matrix of annotations for each variant for which to aggregate the LD scores.\n        :param corrected: Use the sample-size corrected estimator for the squared Pearson correlation coefficient.\n            See Bulik-Sullivan et al. (2015).\n        :param chunk_size: Specify the number of rows (i.e. SNPs) to compute the LD scores for simultaneously.\n            Smaller chunk sizes should require less memory resources. If set to None, we compute LD scores\n            for all SNPs in the LD matrix in one go.\n\n        :return: An array of LD scores for each variant in the LD matrix.\n        \"\"\"\n\n        if chunk_size is None:\n            chunk_size = self.n_snps\n\n        if annotation_matrix is None:\n            annotation_matrix = np.ones((self.n_snps, 1), dtype=np.float32)\n        else:\n            assert annotation_matrix.shape[0] == self.n_snps, (\"Annotation matrix must have the same \"\n                                                               \"number of rows as the LD matrix.\")\n\n        ld_scores = np.zeros((self.n_snps, annotation_matrix.shape[1]), dtype=np.float32)\n\n        for chunk_idx in range(int(np.ceil(self.n_snps / chunk_size))):\n\n            start_row = chunk_idx*chunk_size\n            end_row = min((chunk_idx + 1)*chunk_size, self.n_snps)\n\n            csr_mat = self.load_data(start_row=start_row,\n                                     end_row=end_row,\n                                     return_symmetric=False,\n                                     return_square=False,\n                                     keep_original_shape=True,\n                                     return_as_csr=True,\n                                     dtype=np.float32)\n\n            mat_sq = csr_mat.power(2)\n\n            if corrected:\n                mat_sq.data -= (1. - mat_sq.data) / (self.sample_size - 2)\n\n            ld_scores += mat_sq.dot(annotation_matrix)\n            ld_scores += mat_sq.T.dot(annotation_matrix)\n\n        from scipy.sparse import identity\n\n        # Add the contribution of the diagonal:\n        ld_scores += identity(self.n_snps, dtype=np.float32).dot(annotation_matrix)\n\n        # Set floating type to float32:\n        ld_scores = ld_scores.astype(np.float32)\n\n        if ld_scores.shape[1] == 1:\n            return ld_scores.flatten()\n        else:\n            return ld_scores\n\n    def multiply(self, vec):\n        \"\"\"\n        Multiply the LD matrix with an input vector `vec`.\n\n        :param vec: The input vector to multiply with the LD matrix.\n\n        !!! seealso \"See Also\"\n            * [dot][magenpy.LDMatrix.LDMatrix.dot]\n\n        :return: The product of the LD matrix with the input vector.\n        \"\"\"\n\n        if self.in_memory:\n            return self._cached_lop.dot(vec)\n        else:\n            ld_opr = self.to_linear_operator()\n            return ld_opr.dot(vec)\n\n    def dot(self, vec):\n        \"\"\"\n        Multiply the LD matrix with an input vector `vec`.\n\n        :param vec: The input vector to multiply with the LD matrix.\n\n        !!! seealso \"See Also\"\n            * [multiply][magenpy.LDMatrix.LDMatrix.multiply]\n\n        :return: A numpy array that is the result of the matrix multiplication between\n        the LD matrix with the input vector.\n\n        \"\"\"\n        return self.multiply(vec)\n\n    def perform_svd(self, **svds_kwargs):\n        \"\"\"\n        Perform the Singular Value Decomposition (SVD) on the LD matrix.\n        This method is a wrapper around the `scipy.sparse.linalg.svds` function and provides\n        utilities to perform SVD with a LinearOperator for large-scale LD matrix, so that\n        the matrices don't need to be fully represented in memory.\n\n        :param svds_kwargs: Additional keyword arguments to pass to the `scipy.sparse.linalg.svds` function.\n\n        :return: The result of the SVD decomposition of the LD matrix.\n        \"\"\"\n\n        from scipy.sparse.linalg import svds\n\n        if self.in_memory:\n            mat = self._cached_lop\n        else:\n            mat = self.to_linear_operator()\n\n        return svds(mat, **svds_kwargs)\n\n    def estimate_extremal_eigenvalues(self,\n                                      block_size=None,\n                                      block_size_cm=None,\n                                      block_size_kb=None,\n                                      blocks=None,\n                                      which='both',\n                                      return_block_boundaries=False,\n                                      assign_to_variants=False):\n        \"\"\"\n        Estimate the smallest/largest algebraic eigenvalues of the LD matrix. This is useful for\n        analyzing the spectral properties of the LD matrix and detecting potential\n        issues for downstream applications that leverage the LD matrix. For instance, many LD\n        matrices are not positive semi-definite (PSD) and this manifests in having negative eigenvalues.\n        This function can be used to detect such issues.\n\n        To perform this computation efficiently, we leverage fast ARPACK routines provided by `scipy` to\n        compute only the extremal eigenvalues of the LD matrix. Another advantage of this implementation\n        is that it doesn't require symmetric or dequantized LD matrices. The `LDLinearOperator` class\n        can be used to perform all the computations without symmetrizing or dequantizing the matrix beforehand,\n        which should make it more efficient in terms of memory and CPU resources.\n\n        Furthermore, this function supports computing eigenvalues for sub-blocks of the LD matrix,\n        by simply providing one of the following parameters:\n            * `block_size`: Number of variants per block\n            * `block_size_cm`: Block size in centi-Morgans\n            * `block_size_kb` Block size in kilobases\n\n        :param block_size: An integer specifying the block size or number of variants to\n        compute the minimum eigenvalue for. If provided, we compute minimum eigenvalues for each block in the\n        LD matrix separately, instead of the minimum eigenvalue for the entire matrix. This can be useful for\n        large LD matrices that don't fit in memory or in cases where information about local blocks is needed.\n        :param block_size_cm: The block size in centi-Morgans (cM) to compute the minimum eigenvalue for.\n        :param block_size_kb: The block size in kilo-base pairs (kb) to compute the minimum eigenvalue for.\n        :param blocks: An array or list specifying the block boundaries to compute the minimum eigenvalue for.\n        If there are B blocks, then the array should be of size B + 1, with the entries specifying the start position\n        of each block.\n        :param which: The extremal eigenvalues to compute. Options are 'min', 'max', or 'both'.\n        :param return_block_boundaries: If True, return the block boundaries used to compute the minimum eigenvalue.\n        :param assign_to_variants: If True, assign the minimum eigenvalue to the variants used to compute it.\n\n        :return: The extremal eigenvalue(s) of the LD matrix or sub-blocks of the LD matrix. If `assign_to_variants`\n        is set to True, then return an array of size `n_snps` mapping the extremal eigenvalues to each variant.\n        \"\"\"\n\n        assert which in ('min', 'max', 'both')\n\n        if assign_to_variants:\n            if which == 'both':\n                eigs_per_var = np.zeros((self.stored_n_snps, 2), dtype=np.float32)\n            else:\n                eigs_per_var = np.zeros(self.stored_n_snps, dtype=np.float32)\n        else:\n            eigs = []\n\n        block_boundaries = []\n\n        from .stats.ld.utils import compute_extremal_eigenvalues\n\n        for mat, (start, end) in self.iter_blocks(block_size=block_size,\n                                                  block_size_cm=block_size_cm,\n                                                  block_size_kb=block_size_kb,\n                                                  blocks=blocks,\n                                                  return_type='linop',\n                                                  return_block_boundaries=True):\n\n            block_boundaries.append({'block_start': start, 'block_end': end})\n            eig = compute_extremal_eigenvalues(mat, which=which)\n\n            if assign_to_variants:\n                if which == 'both':\n                    eigs_per_var[start:end, 0] = eig['min']\n                    eigs_per_var[start:end, 1] = eig['max']\n                else:\n                    eigs_per_var[start:end] = eig\n            else:\n                eigs.append(eig)\n\n        block_boundaries = pd.DataFrame(block_boundaries).to_dict(orient='list')\n\n        if assign_to_variants:\n\n            if self._mask is not None:\n                eigs_per_var = eigs_per_var[self._mask, :]\n\n            if which == 'both':\n                eigs_per_var = {\n                    'min': eigs_per_var[:, 0],\n                    'max': eigs_per_var[:, 1]\n                }\n\n            if return_block_boundaries:\n                return eigs_per_var, block_boundaries\n            else:\n                return eigs_per_var\n\n        elif return_block_boundaries:\n            if which == 'both':\n                return pd.DataFrame(eigs).to_dict(orient='list'), block_boundaries\n            else:\n                return eigs, block_boundaries\n        else:\n            if len(eigs) == 1:\n                return eigs[0]\n            else:\n                if which == 'both':\n                    return pd.DataFrame(eigs).to_dict(orient='list')\n                else:\n                    return eigs\n\n    def get_lambda_min(self, aggregate=None, min_max_ratio=0.):\n        \"\"\"\n        A utility method to compute the `lambda_min` value for the LD matrix. `lambda_min` is the smallest\n        algebraic eigenvalue of the LD matrix. This quantity is useful to know in some applications.\n        The function retrieves minimum eigenvalue (if pre-computed and stored) per block and maps it\n        to each variant in the corresponding block. If minimum eigenvalues per block are not available,\n         we use global minimum eigenvalue (either from matrix attributes or we compute it on the spot).\n\n        Before returning the `lambda_min` value to the user, we apply the following transformation:\n\n        abs(min(lambda_min, 0.))\n\n        This implies that if the minimum eigenvalue is non-negative, we just return 0. for `lambda_min`. We are mainly\n        interested in negative eigenvalues here (if they exist).\n\n        :param aggregate: A summary of the minimum eigenvalue across variants or across blocks (if available).\n        Supported aggregation functions are `min_block` and `min`. If `min` is selected,\n        we return the minimum eigenvalue for the entire matrix (rather than sub-blocks of it). If `min_block` is\n        selected, we return the minimum eigenvalue for each block separately (mapped to variants within that block).\n\n        :param min_max_ratio: The ratio between the absolute values of the minimum and maximum eigenvalues.\n        This could be used to target a particular threshold for the minimum eigenvalue.\n\n        :return: The absolute value of the minimum eigenvalue for the LD matrix. If the minimum\n        eigenvalue is non-negative, we return zero.\n        \"\"\"\n\n        if aggregate is not None:\n            assert aggregate in ('min_block', 'min')\n\n        # Get the attributes of the LD store:\n        store_attrs = self.list_store_attributes()\n\n        def threshold_lambda_min(eigs):\n            return np.abs(np.minimum(eigs['min'] + min_max_ratio*eigs['max'], 0.)) / (1. + min_max_ratio)\n\n        lambda_min = 0.\n\n        if 'Spectral properties' not in store_attrs:\n            if aggregate in ('mean_block', 'median_block', 'min_block'):\n                raise ValueError('Aggregating lambda_min across blocks '\n                                 'requires that these blocks are pre-defined.')\n            else:\n                lambda_min = threshold_lambda_min(self.estimate_extremal_eigenvalues())\n\n        else:\n\n            spectral_props = self.get_store_attr('Spectral properties')\n\n            if aggregate == 'min_block':\n                assert 'Eigenvalues per block' in spectral_props, (\n                    'Aggregating lambda_min across blocks '\n                    'requires that these blocks are pre-defined.')\n\n            if aggregate == 'min' or 'Eigenvalues per block' not in spectral_props:\n\n                if 'Extremal' in spectral_props:\n                    lambda_min = threshold_lambda_min(spectral_props['Extremal'])\n                else:\n                    lambda_min = threshold_lambda_min(self.estimate_extremal_eigenvalues())\n\n            elif 'Eigenvalues per block' in spectral_props:\n\n                # If we have eigenvalues per block, map the block value to each variant:\n                block_eigs = spectral_props['Eigenvalues per block']\n\n                if aggregate is None:\n\n                    # Create a dataframe with the block information:\n                    block_df = pd.DataFrame(block_eigs)\n                    block_df['add_lam'] = block_df.apply(threshold_lambda_min, axis=1)\n\n                    merged_df = pd.merge_asof(pd.DataFrame({'SNP_idx': np.arange(self.stored_n_snps)}),\n                                              block_df,\n                                              right_on='block_start', left_on='SNP_idx', direction='backward')\n                    # Filter merged_df to only include variants that were matched properly with a block:\n                    merged_df = merged_df.loc[\n                        (merged_df.SNP_idx &gt;= merged_df.block_start) &amp; (merged_df.SNP_idx &lt; merged_df.block_end)\n                    ]\n\n                    if len(merged_df) &lt; 1:\n                        raise ValueError('No variants were matched to blocks. '\n                                         'This could be due to incorrect block boundaries.')\n\n                    lambda_min = np.zeros(self.stored_n_snps)\n                    lambda_min[merged_df['SNP_idx'].values] = merged_df['add_lam'].values\n\n                    if self.is_mask_set:\n                        lambda_min = lambda_min[self._mask]\n\n                elif aggregate == 'min_block':\n                    lambda_min = np.min(block_eigs['min'])\n\n        return lambda_min\n\n    def estimate_uncompressed_size(self, dtype=None):\n        \"\"\"\n        Provide an estimate of size of the uncompressed LD matrix in megabytes (MB).\n        This is only a rough estimate. Depending on how the LD matrix is loaded, actual memory\n        usage may be larger than this estimate.\n\n        :param dtype: The data type for the entries of the LD matrix. If None, the stored data type is used\n        to determine the size of the data in memory.\n\n        :return: The estimated size of the uncompressed LD matrix in MB.\n\n        \"\"\"\n\n        if dtype is None:\n            dtype = self.stored_dtype\n\n        return 2.*self._zg['matrix/data'].shape[0]*np.dtype(dtype).itemsize / 1024 ** 2\n\n    def get_total_storage_size(self):\n        \"\"\"\n        Estimate the storage size for all elements of the `LDMatrix` hierarchy,\n        including the LD data arrays, metadata arrays, and attributes.\n\n        :return: The estimated size of the stored and compressed LDMatrix object in MB.\n        \"\"\"\n\n        total_bytes = 0\n\n        # Estimate contribution of matrix arrays\n        for arr_name, array in self.zarr_group.matrix.arrays():\n            total_bytes += array.nbytes_stored\n\n        # Estimate contribution of metadata arrays\n        for arr_name, array in self.zarr_group.metadata.arrays():\n            total_bytes += array.nbytes_stored\n\n        # Estimate the contribution of the attributes:\n        if hasattr(self.zarr_group, 'attrs'):\n            total_bytes += len(str(dict(self.zarr_group.attrs)).encode('utf-8'))\n\n        return total_bytes / 1024**2\n\n    def get_metadata(self, key, apply_mask=True):\n        \"\"\"\n        Get the metadata associated with each variant in the LD matrix.\n\n        :param key: The key for the metadata item.\n        :param apply_mask: If True, apply the mask (e.g. filter) to the metadata.\n\n        :return: The metadata item for each variant in the LD matrix.\n        :raises KeyError: if the metadata item is not set.\n        \"\"\"\n        try:\n            metadata = self._zg[f'metadata/{key}'][:]\n        except KeyError:\n            raise KeyError(f\"LD matrix metadata item {key} is not set!\")\n\n        if self.is_mask_set and apply_mask:\n            metadata = metadata[self._mask]\n\n        return metadata\n\n    def get_store_attr(self, attr):\n        \"\"\"\n        Get the attribute or metadata `attr` associated with the LD matrix.\n        :param attr: The attribute name.\n\n        :return: The value for the attribute.\n        :raises KeyError: if the attribute is not set.\n        \"\"\"\n        return self._zg.attrs[attr]\n\n    def list_store_attributes(self):\n        \"\"\"\n        Get all the attributes associated with the LD matrix.\n        :return: A list of all the attributes.\n        \"\"\"\n        return list(self._zg.attrs.keys())\n\n    def set_store_attr(self, attr, value):\n        \"\"\"\n        Set the attribute `attr` associated with the LD matrix. This is used\n        to set high-level information, such as information about the sample from which\n        the matrix was computed, the LD estimator used, its properties, etc.\n\n        :param attr: The attribute name.\n        :param value: The value for the attribute.\n        \"\"\"\n\n        self._zg.attrs[attr] = value\n\n    def set_metadata(self, key, value, overwrite=False):\n        \"\"\"\n        Set the metadata field associated with variants in the LD matrix.\n        :param key: The key for the metadata item.\n        :param value: The value for the metadata item (an array with the same length as the number of variants).\n        :param overwrite: If True, overwrite the metadata item if it already exists.\n        \"\"\"\n\n        if 'metadata' not in list(self._zg.group_keys()):\n            meta = self._zg.create_group('metadata')\n        else:\n            meta = self._zg['metadata']\n\n        value = np.array(value)\n\n        if np.issubdtype(value.dtype, np.floating):\n            dtype = np.float32\n        elif np.issubdtype(value.dtype, np.integer):\n            dtype = np.int32\n        else:\n            dtype = str\n\n        meta.array(key,\n                   value,\n                   overwrite=overwrite,\n                   dtype=dtype,\n                   compressor=self.compressor)\n\n    def update_rows_inplace(self, new_csr, start_row=None, end_row=None):\n        \"\"\"\n        A utility function to perform partial updates to a subset of rows in the\n        LD matrix. The function takes a new CSR matrix and, optionally, a start\n        and end row delimiting the chunk of the LD matrix to update with the `new_csr`.\n\n        !!! note\n            Current implementation assumes that the update does not change the sparsity\n            structure of the original matrix. Updating the matrix with new sparsity structure\n            is a harder problem that we will try to tackle later on.\n\n        !!! note\n            Current implementation assumes `new_csr` is upper triangular.\n\n        :param new_csr: A sparse CSR matrix (`scipy.sparse.csr_matrix`) where the column dimension\n        matches the column dimension of the LD matrix.\n        :param start_row: The start row for the chunk to update.\n        :param end_row: The end row for the chunk to update.\n\n        :raises AssertionError: if the column dimension of `new_csr` does not match the column dimension\n        \"\"\"\n\n        assert new_csr.shape[1] == self.stored_n_snps\n\n        start_row = start_row or 0\n        end_row = end_row or self.stored_n_snps\n\n        # Sanity checking:\n        assert start_row &gt;= 0\n        assert end_row &lt;= self.stored_n_snps\n\n        indptr = self._zg['matrix/indptr'][:]\n\n        data_start = indptr[start_row]\n        data_end = indptr[end_row]\n\n        # TODO: Check that this covers most cases and would not result in unexpected behavior\n        if np.issubdtype(self.stored_dtype, np.integer) and np.issubdtype(new_csr.dtype, np.floating):\n            self._zg['matrix/data'][data_start:data_end] = quantize(new_csr.data, int_dtype=self.stored_dtype)\n        else:\n            self._zg['matrix/data'][data_start:data_end] = new_csr.data.astype(self.stored_dtype, copy=False)\n\n    def to_linear_operator(self, **load_kwargs):\n        \"\"\"\n        Get the LD data as a `LDLinearOperator` object. This is useful for performing\n        linear algebra operations on the LD matrix efficiently.\n\n        !!! seealso \"See Also\"\n        * [LDLinearOperator][magenpy.LDMatrix.LDLinearOperator]\n\n        :param load_kwargs: Additional keyword arguments to pass to the `load_data` method.\n\n        :return: An `LDLinearOperator` object containing the LD data.\n        \"\"\"\n\n        if self.in_memory:\n            return self._cached_lop\n        else:\n            return self.load_data(**load_kwargs)\n\n    def to_csr(self, **load_kwargs):\n        \"\"\"\n        Get the LD data as a `scipy.csr_matrix` object.\n\n        :param load_kwargs: Additional keyword arguments to pass to the `load_data` method.\n        :return: A `scipy.csr_matrix` object representing the LD matrix.\n        \"\"\"\n\n        if self.in_memory:\n            return self._cached_lop.to_csr()\n        else:\n            return self.load_data(return_as_csr=True, **load_kwargs)\n\n    def load_data(self,\n                  start_row=None,\n                  end_row=None,\n                  dtype=None,\n                  return_square=True,\n                  keep_original_shape=False,\n                  return_symmetric=False,\n                  return_as_csr=False):\n        \"\"\"\n        A utility function to load and process the LD matrix data.\n        This function is particularly useful for filtering, symmetrizing, and dequantizing the LD matrix\n        after it's loaded to memory.\n\n        .. note ::\n            Start and end row positions are with reference to the stored on-disk LD matrix. This means\n            that the mask is not considered when defining the boundaries\n            based on the start and end row positions.\n\n        :param start_row: The start row to load to memory (if loading a subset of the matrix).\n        :param end_row: The end row (not inclusive) to load to memory (if loading a subset of the matrix).\n        :param dtype: The data type for the entries of the LD matrix.\n        :param return_square: If True, return a square representation of the LD matrix. This flag is used in\n        conjunction with the `start_row` and `end_row` parameters. In particular, if `end_row` is less than the\n        number of variants and `return_square=False`, then we return a rectangular slice of the LD matrix\n        corresponding to the rows requested by the user.\n        :param keep_original_shape: If True, keep the original shape of the LD matrix. This is useful when\n        returning a subset of the matrix, but keeping the original shape.\n        :param return_symmetric: If True, return a full symmetric representation of the LD matrix.\n        :param return_as_csr: If True, return the data in the CSR format.\n\n        :return: An LDLinearOperator object containing the LD data or a scipy CSR matrix (if\n        `return_as_csr` is set to `True`.\n        \"\"\"\n\n        # Sanity checking:\n\n        if start_row is not None:\n            assert 0. &lt;= start_row &lt; self.stored_n_snps\n        if end_row is not None:\n            assert 0. &lt; end_row &lt;= self.stored_n_snps\n\n        if keep_original_shape:\n            assert return_as_csr, \"If keeping the original shape, the data must be returned as a CSR matrix.\"\n\n        # -------------- Step 1: Preparing input data type --------------\n        if dtype is None:\n            dtype = self.stored_dtype\n            dequantize_data = False\n        else:\n            dtype = np.dtype(dtype)\n            if np.issubdtype(self.stored_dtype, np.integer) and np.issubdtype(dtype, np.floating):\n                dequantize_data = True\n            else:\n                dequantize_data = False\n\n        # -------------- Step 2: Pre-process data boundaries (if provided) --------------\n        n_snps = self.stored_n_snps\n\n        start_row = start_row or 0\n        end_row = end_row or n_snps\n\n        end_row = min(end_row, n_snps)\n\n        # -------------- Step 2: Fetch the indptr array --------------\n\n        # Get the index pointer array:\n        indptr = self._zg['matrix/indptr'][start_row:end_row + 1]\n\n        # Determine the start and end positions in the data matrix\n        # based on the requested start and end rows:\n        data_start = indptr[0]\n        data_end = indptr[-1]\n\n        # If the user is requesting a subset of the matrix, then we need to adjust\n        # the index pointer accordingly:\n        if start_row &gt; 0:\n            # Zero out all index pointers before `start_row`:\n            indptr = np.clip(indptr - data_start, a_min=0, a_max=None)\n\n        # -------------- Step 3: Loading and filtering data array --------------\n\n        data = self._zg['matrix/data'][data_start:data_end]\n\n        # Filter the data and index pointer arrays based on the mask (if set):\n        if self.is_mask_set or (end_row &lt; n_snps and return_square):\n\n            mask = np.zeros(n_snps, dtype=np.int8)\n\n            # Two cases to consider:\n\n            # 1) If the mask is not set:\n            if not self.is_mask_set:\n\n                # If the returned matrix should be square:\n                if return_square:\n                    mask[start_row:end_row] = 1\n                else:\n                    mask[start_row:] = 1\n\n                new_nrows = end_row - start_row\n            else:\n                # If the mask is set:\n\n                mask[self._mask] = 1\n\n                # If return square, ensure that elements after end row are set to 0 in the mask:\n                if return_square:\n                    mask[end_row:] = 0\n\n                # Compute new size:\n                new_nrows = mask[start_row:end_row].sum()\n\n            from .stats.ld.c_utils import filter_ut_csr_matrix_inplace\n\n            data, indptr = filter_ut_csr_matrix_inplace(indptr, data, mask[start_row:], new_nrows)\n\n        # -------------- Step 4: Symmetrizing input matrix --------------\n\n        if return_symmetric:\n\n            from .stats.ld.c_utils import symmetrize_ut_csr_matrix\n\n            if np.issubdtype(self.stored_dtype, np.integer):\n                fill_val = np.iinfo(self.stored_dtype).max\n            else:\n                fill_val = 1.\n\n            data, indptr, leftmost_idx = symmetrize_ut_csr_matrix(indptr, data, fill_val)\n        else:\n            leftmost_idx = np.arange(1, indptr.shape[0], dtype=np.int32)\n\n        # -------------- Step 5: Dequantizing/type cast requested data --------------\n\n        if dequantize_data:\n            data = dequantize(data, float_dtype=dtype)\n        else:\n            data = data.astype(dtype, copy=False)\n\n        # ---------------------------------------------------------------------------\n        # Determine the shape of the data matrix:\n\n        if keep_original_shape:\n            shape = (n_snps, n_snps)\n        elif return_square or end_row == n_snps:\n            shape = (indptr.shape[0] - 1, indptr.shape[0] - 1)\n        else:\n            shape = (indptr.shape[0] - 1, n_snps)\n\n        # ---------------------------------------------------------------------------\n        # Return the requested data:\n\n        if return_as_csr:\n            # If the user requested the data as CSR matrix:\n\n            from .stats.ld.c_utils import expand_ranges\n            from scipy.sparse import csr_matrix\n\n            indices = expand_ranges(leftmost_idx,\n                                    (np.diff(indptr) + leftmost_idx).astype(np.int32),\n                                    data.shape[0])\n\n            if keep_original_shape:\n                # TODO: Consider incorporating this in `LDLinearOperator.to_csr`\n                indices += start_row\n                indptr = np.concatenate([np.zeros(start_row, dtype=indptr.dtype),\n                                         indptr,\n                                         np.ones(n_snps - end_row, dtype=indptr.dtype) * indptr[-1]])\n\n            return csr_matrix(\n                (\n                    data,\n                    indices,\n                    indptr\n                ),\n                shape=shape,\n                dtype=dtype\n            )\n\n        else:\n            # Otherwise, return as a linear operator:\n\n            return LDLinearOperator(\n                indptr,\n                data,\n                leftmost_idx,\n                symmetric=return_symmetric,\n                shape=shape\n            )\n\n    def load(self,\n             force_reload=False,\n             return_symmetric=False,\n             dtype=None) -&gt; LDLinearOperator:\n\n        \"\"\"\n        Load the LD matrix from on-disk storage in the form of Zarr arrays to memory,\n        in the form of sparse CSR matrices.\n\n        :param force_reload: If True, it will reload the data even if it is already in memory.\n        :param return_symmetric: If True, return a full symmetric representation of the LD matrix.\n        :param dtype: The data type for the entries of the LD matrix.\n\n        !!! seealso \"See Also\"\n            * [load_data][magenpy.LDMatrix.LDMatrix.load_data]\n\n        :return: The LD matrix as a `scipy` sparse CSR matrix.\n        \"\"\"\n\n        if dtype is not None:\n            dtype = np.dtype(dtype)\n        else:\n            dtype = self.dtype\n\n        if not force_reload and self.in_memory and return_symmetric == self._cached_lop.symmetric:\n            # If the LD matrix is already in memory and the requested symmetry is the same,\n            # then we don't need to reload the matrix. Here, we only transform its entries it to\n            # conform to the requested data types of the user:\n\n            # If the requested data type differs from the stored one, we need to cast the data:\n            if dtype is not None and self._cached_lop.ld_data_type != np.dtype(dtype):\n\n                if np.issubdtype(self._cached_lop.ld_data_type, np.floating) and np.issubdtype(dtype, np.floating):\n                    # The user requested casting the data to different floating point precision:\n                    self._cached_lop.ld_data = self._cached_lop.ld_data.astype(dtype)\n                elif np.issubdtype(self._cached_lop.ld_data_type, np.integer) and np.issubdtype(dtype, np.integer):\n                    # The user requested casting the data to different integer format:\n                    self._cached_lop.ld_data = quantize(dequantize(self._cached_lop.ld_data), int_dtype=dtype)\n                elif np.issubdtype(self._cached_lop.ld_data_type, np.floating) and np.issubdtype(dtype, np.integer):\n                    # The user requested quantizing the data from floats to integers:\n                    self._cached_lop.ld_data = quantize(self._cached_lop.ld_data, int_dtype=dtype)\n                else:\n                    # The user requested dequantizing the data from integers to floats:\n                    self._cached_lop.ld_data = dequantize(self._cached_lop.ld_data, float_dtype=dtype)\n\n        else:\n            # If we are re-loading the matrix, make sure to release the current one:\n            self.release()\n\n            self._cached_lop = self.load_data(return_symmetric=return_symmetric,\n                                              dtype=dtype)\n\n        return self._cached_lop\n\n    def release(self):\n        \"\"\"\n        Release the LD data and associated arrays from memory.\n        \"\"\"\n        self._cached_lop = None\n        self.index = 0\n\n    def validate_ld_matrix(self):\n        \"\"\"\n        Checks that the `LDMatrix` object has correct structure and\n        checks its contents for validity.\n\n        Specifically, we check that:\n        * The dimensions of the matrix and its associated attributes are matching.\n        * The masking is working properly.\n        * Index pointer is valid and its contents make sense.\n\n        :return: True if the matrix has the correct structure.\n        :raises ValueError: If the matrix or some of its entries are not valid.\n        \"\"\"\n\n        class_attrs = ['snps', 'a1', 'a2', 'maf', 'bp_position', 'cm_position', 'ld_score']\n\n        for attr in class_attrs:\n            attribute = getattr(self, attr)\n            if attribute is None:\n                continue\n            if len(attribute) != len(self):\n                raise ValueError(f\"Invalid LD Matrix: Dimensions for attribute {attr} are not aligned!\")\n\n        # -------------------- Index pointer checks --------------------\n        # Check that the entries of the index pointer are all positive or zero:\n        indptr = self.indptr[:]\n\n        if indptr.min() &lt; 0:\n            raise ValueError(\"The index pointer contains negative entries!\")\n\n        # Check that the entries don't decrease:\n        indptr_diff = np.diff(indptr)\n        if indptr_diff.min() &lt; 0:\n            raise ValueError(\"The index pointer entries are not increasing!\")\n\n        # Check that the last entry of the index pointer matches the shape of the data:\n        if indptr[-1] != self.data.shape[0]:\n            raise ValueError(\"The last entry of the index pointer \"\n                             \"does not match the shape of the data!\")\n\n        # TODO: Add other sanity checks here?\n\n        return True\n\n    def iter_blocks(self,\n                    block_size=None,\n                    block_size_cm=None,\n                    block_size_kb=None,\n                    blocks=None,\n                    min_block_size=2,\n                    max_block_size=None,\n                    return_type='csr',\n                    return_block_boundaries=False,\n                    dry_run=False,\n                    **return_type_kwargs\n                    ):\n        \"\"\"\n        Iterator over blocks of the LD matrix.\n\n        This function allows for iterating over blocks of the LD matrix, either based on the number of variants\n        per block, or based on the physical distance between variants. The function yields the requested data\n        in the form of a sparse CSR matrix, a `LinearOperator`, or a dense numpy array.\n\n        .. note::\n            For now, all block-related information is with reference to the original, unfiltered LD matrix.\n            In future releases, we may consider supporting block iterators based on subsets of the\n            matrix.\n\n        :param block_size: An integer specifying the block size in terms of the number of variants.\n        :param block_size_cm: The block size in centi-Morgans (cM).\n        :param block_size_kb: The block size in kilo-base pairs (kb).\n        :param blocks: An array or list specifying the block boundaries to iterate over.\n        :param min_block_size: The minimum block size.\n        :param return_type: The type of data to return. Options are 'csr', 'linop', or 'numpy'.\n        :param return_block_boundaries: If True, return the boundaries of the generated blocks along with the\n        LD data itself.\n        :param dry_run: If True, do not load the data, just return the block boundaries. Useful for debugging.\n        :param return_type_kwargs: Additional keyword arguments to pass to the return type constructor.\n\n        \"\"\"\n\n        # Sanity checks:\n        assert return_type in ('csr', 'linop', 'numpy')\n        assert min_block_size &gt;= 1\n\n        # Determine the block boundaries based on the input parameters:\n        if self.in_memory:\n            n_snps = self.n_snps\n        else:\n            n_snps = self.stored_n_snps\n\n        from .utils.compute_utils import generate_overlapping_windows\n\n        if blocks is not None:\n            block_iter = blocks\n        elif block_size is not None:\n\n            windows = generate_overlapping_windows(np.arange(n_snps),\n                                                   block_size - 1, block_size,\n                                                   min_window_size=min_block_size)\n            block_iter = np.insert(windows[:, 1], 0, 0)\n\n        elif block_size_cm is not None or block_size_kb is not None:\n\n            if block_size_cm is not None:\n                dist = self.get_metadata('cm', apply_mask=self.in_memory)\n                block_size = block_size_cm\n            else:\n                dist = self.get_metadata('bp', apply_mask=self.in_memory) / 1000\n                block_size = block_size_kb\n\n            windows = generate_overlapping_windows(dist,\n                                                   block_size,\n                                                   block_size,\n                                                   min_window_size=min_block_size)\n\n            block_iter = np.insert(windows[:, 1], 0, 0)\n        elif self.ld_estimator == 'windowed':\n\n            est_properties = self.estimator_properties\n\n            if 'Window size (cM)' in est_properties:\n                block_size = est_properties['Window size (cM)']\n                dist = self.get_metadata('cm', apply_mask=self.in_memory)\n            elif 'Window size (kb)' in est_properties:\n                block_size = est_properties['Window size (kb)']\n                dist = self.get_metadata('bp', apply_mask=self.in_memory) / 1000\n            else:\n                block_size = est_properties['Window size']\n                dist = np.arange(n_snps)\n\n            windows = generate_overlapping_windows(dist,\n                                                   block_size,\n                                                   block_size,\n                                                   min_window_size=min_block_size)\n\n            block_iter = np.insert(windows[:, 1], 0, 0)\n\n        elif self.ld_estimator == 'block':\n\n            from .utils.model_utils import map_variants_to_genomic_blocks\n\n            variants_to_blocks = map_variants_to_genomic_blocks(\n                pd.DataFrame({\n                    'POS': self.get_metadata('bp', apply_mask=self.in_memory)\n                }).reset_index(),\n                pd.DataFrame(np.array(self.estimator_properties['LD blocks']),\n                             columns=['block_start', 'block_end'],\n                             dtype=np.int32),\n                filter_unmatched=True\n            )\n\n            block_iter = [0] + list(variants_to_blocks.groupby('block_end')['index'].max().values + 1)\n        else:\n            block_iter = [0, n_snps]\n\n        # If the maximum block size is specified by the user,\n        # then use the `split_block_boundaries` utility function to split\n        # blocks to conform to this constraint:\n        if max_block_size is not None:\n\n            from .utils.compute_utils import split_block_boundaries\n\n            block_iter = split_block_boundaries(\n                block_iter,\n                max_block_size,\n                mask=[self._mask, None][self.in_memory]\n            )\n\n        mat = None\n\n        # Loop over the blocks and yield the requested data:\n        for bidx in range(len(block_iter) - 1):\n\n            start = block_iter[bidx]\n            end = block_iter[bidx + 1]\n\n            if dry_run:\n                yield start, end\n            else:\n                # If the data is in memory, subset the data for the requested block:\n                if self.in_memory:\n\n                    if return_type == 'numpy':\n                        mat = self._cached_lop.to_numpy(start, end)\n                    elif return_type in ('csr', 'linop'):\n                        mat = self._cached_lop[start:end, start:end]\n                        if return_type == 'csr':\n                            mat = mat.to_csr()\n\n                else:\n\n                    mat = self.load_data(start_row=start,\n                                         end_row=end,\n                                         return_as_csr=return_type in ('csr', 'numpy'),\n                                         **return_type_kwargs)\n                    if return_type == 'numpy':\n                        mat = mat.todense()\n\n                if return_block_boundaries:\n                    yield mat, (start, end)\n                else:\n                    yield mat\n\n    def getrow(self, index, symmetric=False, return_indices=False):\n        \"\"\"\n        Extract a single row from the LD matrix.\n\n        # TODO: Support extracting rows from the symmetric LD matrix.\n        # TODO: Verify that this is correct.\n\n        :param index: The index of the row to extract.\n        :param symmetric: If True, return a symmetric representation of the row (i.e. LD with\n        variants before and after the index variant).\n        :param return_indices: If True, return the indices of the non-zero elements of that row.\n\n        :return: The requested row of the LD matrix.\n        \"\"\"\n\n        if symmetric:\n            raise NotImplementedError(\"Symmetric row extraction is not yet supported.\")\n\n        if self.in_memory:\n            return self._cached_lop.getrow(index, symmetric=symmetric, return_indices=return_indices)\n        else:\n            start_idx, end_idx = self.indptr[index:index + 2]\n            data = self.data[start_idx:end_idx]\n\n            if return_indices:\n                return data, np.arange(\n                    index + 1,\n                    index + len(data)\n                )\n            else:\n                return data\n\n    def summary(self):\n        \"\"\"\n        :return: A `pandas` dataframe with summary of the main attributes of the LD matrix.\n        \"\"\"\n\n        return pd.DataFrame([\n            {'LD Matrix property': 'Chromosome', 'Value': self.chromosome},\n            {'LD Matrix property': 'Stored shape', 'Value': self.stored_shape},\n            {'LD Matrix property': 'Stored data type', 'Value': self.stored_dtype},\n            {'LD Matrix property': 'Stored entries', 'Value': self._zg['matrix/data'].shape[0]},\n            {'LD Matrix property': 'Path', 'Value': self.store.path},\n            {'LD Matrix property': 'In memory?', 'Value': self.in_memory},\n            {'LD Matrix property': 'Mask set?', 'Value': self.is_mask_set},\n            {'LD Matrix property': 'On-disk storage', 'Value': f'{self.get_total_storage_size():.3} MB'},\n            {'LD Matrix property': 'Estimated uncompressed size', 'Value': f'{self.estimate_uncompressed_size():.3} MB'}\n        ]).set_index('LD Matrix property')\n\n    def __repr__(self):\n        \"\"\"\n        :return: A summary of the LDMatrix object as a string.\n        \"\"\"\n        return self.summary().to_string()\n\n    def _repr_html_(self):\n        \"\"\"\n        :return: A summary of the LDMatrix object as an HTML table.\n        \"\"\"\n\n        styled_df = self.summary().style.set_table_attributes(\n            'class=\"dataframe\" style=\"width: 50%\"'\n        ).set_properties(**{\n            'text-align': 'left',\n            'white-space': 'normal',  # Allow text wrapping\n            'word-wrap': 'break-word',  # Break words at any character\n            'word-break': 'break-word',  # Allow breaking words when necessary\n            'font-size': '13px',\n            'padding': '10px 15px'\n        }).set_table_styles([\n            # Table border and design\n            {'selector': 'table', 'props': [\n                ('border-collapse', 'separate'),\n                ('border-spacing', '0px'),\n                ('border-radius', '5px'),\n                ('overflow', 'hidden'),\n                ('box-shadow', '0 2px 3px rgba(0,0,0,0.1)'),\n                ('margin', '20px 0'),\n                ('table-layout', 'fixed'),  # Fixed layout helps with word wrapping\n                ('width', '50%')  # Ensure table takes full width\n            ]},\n            # Header styling\n            {'selector': 'thead th', 'props': [\n                ('background-color', '#3b5f9e'),\n                ('color', 'white'),\n                ('font-weight', 'bold'),\n                ('text-align', 'left'),\n                ('padding', '12px 15px')\n            ]},\n            # Property name cells (index)\n            {'selector': 'tbody th', 'props': [\n                ('background-color', '#f8f9fa'),\n                ('color', '#333'),\n                ('font-weight', 'bold'),\n                ('border-bottom', '1px solid #eaeaea'),\n                ('text-align', 'left'),\n                ('padding', '10px 15px'),\n                ('width', '30%'),  # Control width of the property column\n                ('word-wrap', 'break-word'),\n                ('white-space', 'normal')\n            ]},\n            # Value cells\n            {'selector': 'tbody td', 'props': [\n                ('background-color', 'white'),\n                ('border-bottom', '1px solid #eaeaea'),\n                ('color', '#444'),\n                ('padding', '10px 15px'),\n                ('width', '70%'),  # Control width of the value column\n                ('word-wrap', 'break-word'),\n                ('white-space', 'normal'),\n                ('max-width', '0')  # Forces the cell to respect width constraints\n            ]},\n            # Alternating rows\n            {'selector': 'tbody tr:nth-of-type(odd) td', 'props': [\n                ('background-color', '#f8f9fa'),\n            ]},\n            # Hover effect on rows\n            {'selector': 'tbody tr:hover td, tbody tr:hover th', 'props': [\n                ('background-color', '#e8f0fe'),\n            ]}\n        ]).hide(axis='columns')\n\n        return styled_df._repr_html_()\n\n    def __getstate__(self):\n        return self.store.path, self.in_memory, self.is_symmetric, self._mask, self.dtype\n\n    def __setstate__(self, state):\n\n        path, in_mem, is_symmetric, mask, dtype = state\n\n        self._zg = zarr.open_group(path, mode='r')\n        self._cached_lop = None\n        self.index = 0\n        self._mask = None\n\n        if mask is not None:\n            self.set_mask(mask)\n\n        if in_mem:\n            self.load(return_symmetric=is_symmetric, dtype=dtype)\n\n    def __len__(self):\n        return self.n_snps\n\n    def __getitem__(self, item):\n        \"\"\"\n        Access the LD matrix entries via the `[]` operator.\n        This implementation supports the following types of indexing:\n        * Accessing a single row of the LD matrix by specifying numeric index or SNP rsID.\n        * Accessing a single entry of the LD matrix by specifying numeric indices or SNP rsIDs.\n\n        Example usages:\n\n            &gt;&gt;&gt; ldm[0]\n            &gt;&gt;&gt; ldm['rs123']\n            &gt;&gt;&gt; ldm['rs123', 'rs456']\n        \"\"\"\n\n        dq_scale = self.dequantization_scale\n\n        if isinstance(item, tuple):\n            assert len(item) == 2\n            assert type(item[0]) is type(item[1])\n\n            if isinstance(item[0], str):\n\n                # If they're the same variant:\n                if item[0] == item[1]:\n                    return 1.\n\n                # Extract the indices of the two variants:\n                snps = self.snps.tolist()\n\n                try:\n                    index_1 = snps.index(item[0])\n                except ValueError:\n                    raise ValueError(f\"Invalid variant rsID: {item[0]}\")\n\n                try:\n                    index_2 = snps.index(item[1])\n                except ValueError:\n                    raise ValueError(f\"Invalid variant rsID: {item[1]}\")\n\n            else:\n                index_1, index_2 = item\n\n            index_1, index_2 = sorted([index_1, index_2])\n\n            if index_1 == index_2:\n                return 1.\n            if index_2 - index_1 &gt; self.window_size[index_1]:\n                return 0.\n            else:\n                row = self.getrow(index_1)\n                return dq_scale*row[index_2 - index_1 - 1]\n\n        if isinstance(item, int):\n            return dq_scale*self.getrow(item)\n        elif isinstance(item, str):\n            try:\n                index = self.snps.tolist().index(item)\n            except ValueError:\n                raise ValueError(f\"Invalid variant rsID: {item}\")\n\n            return dq_scale*self.getrow(index)\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over the rows of the LD matrix.\n        This iterator supports fetching the rows of the LD matrix one by one from on-disk storage,\n        or, if the data is already loaded, it subsets the data for each row.\n        \"\"\"\n        self.index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"\n        Get the next row of the LD matrix.\n        \"\"\"\n\n        if self.index == len(self):\n            self.index = 0\n            raise StopIteration\n\n        next_item = self.getrow(self.index)\n        self.index += 1\n\n        return next_item\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.a1","title":"<code>a1</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The alternative alleles of the variants included in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.a2","title":"<code>a2</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The reference alleles of the variants included in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.bp_position","title":"<code>bp_position</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>genome_build</li> </ul> <p>Returns:</p> Type Description <p>The base pair position of each SNP in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.chromosome","title":"<code>chromosome</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The chromosome for which this LD matrix was calculated.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.chunk_size","title":"<code>chunk_size</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The chunk size for the data array of the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.chunks","title":"<code>chunks</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The chunks for the data array of the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.cm_position","title":"<code>cm_position</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The centi Morgan (cM) position of each variant in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.compressor","title":"<code>compressor</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The <code>numcodecs</code> compressor object for the LD data.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.data","title":"<code>data</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The <code>data</code> array of the sparse <code>CSR</code> matrix, containing the entries of the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.dequantization_scale","title":"<code>dequantization_scale</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The dequantization scale for the quantized LD matrix. If the matrix is not quantized, returns 1.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The data type for the entries of the <code>data</code> array of the LD matrix. If the matrix is in memory, return the dtype of the CSR matrix. Otherwise, return the dtype of the entries in the Zarr array.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.estimator_properties","title":"<code>estimator_properties</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The properties of the LD estimator used to compute the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.genome_build","title":"<code>genome_build</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The genome build based on which the base pair coordinates are defined.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.in_memory","title":"<code>in_memory</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean flag indicating whether the LD matrix is in memory.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.indices","title":"<code>indices</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The column indices of the non-zero elements of the sparse, CSR representation of the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.indptr","title":"<code>indptr</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The index pointers <code>indptr</code> delineating where the data for each row of the flattened, sparse CSR representation of the lD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.is_mask_set","title":"<code>is_mask_set</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean flag indicating whether a mask is set for the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.is_symmetric","title":"<code>is_symmetric</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean flag indicating whether the loaded LD matrix is symmetric.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.ld_boundaries","title":"<code>ld_boundaries</code>  <code>property</code>","text":"<p>The LD boundaries associated with each variant. The LD boundaries are defined as the index of the leftmost neighbor (lower boundary) and the rightmost neighbor (upper boundary) of for each variant. If the LD matrix is upper triangular, then the boundaries for variant <code>i</code> go from <code>i + 1</code> to <code>i + k_i</code>, where <code>k_i</code> is the number of neighbors that SNP <code>i</code> is in LD with.</p> <p>Returns:</p> Type Description <p>A matrix of shape <code>(2, n_snps)</code> where the first row contains the lower boundaries and the second row contains the upper boundaries.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.ld_estimator","title":"<code>ld_estimator</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The LD estimator used to compute the LD matrix. Examples include: <code>block</code>, <code>windowed</code>, <code>shrinkage</code>.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.ld_score","title":"<code>ld_score</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The LD score of each variant in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.leftmost_index","title":"<code>leftmost_index</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The index of the leftmost neighbor of each variant in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.maf","title":"<code>maf</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The minor allele frequency (MAF) of the alternative allele (A1) in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.n_neighbors","title":"<code>n_neighbors</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>window_size</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the LD window for each SNP.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of variants in the LD matrix. If a mask is set, we return the number of variants included in the mask.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.row_indices","title":"<code>row_indices</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The row indices of the non-zero elements of the sparse, CSR representation of the LD matrix</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.sample_size","title":"<code>sample_size</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The sample size used to compute the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_snps</li> </ul> <p>Returns:</p> Type Description <p>The shape of the square LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>rsIDs of the variants included in the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.store","title":"<code>store</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The Zarr group store object.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.stored_dtype","title":"<code>stored_dtype</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The data type for the stored entries of <code>data</code> array of the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.stored_n_snps","title":"<code>stored_n_snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of variants stored in the LD matrix (irrespective of any masks / filters).</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.stored_shape","title":"<code>stored_shape</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The shape of the stored LD matrix (irrespective of any masks / filters).</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.window_size","title":"<code>window_size</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_neighbors</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the LD window for each SNP.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.zarr_group","title":"<code>zarr_group</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The Zarr group object that stores the LD matrix and its metadata.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Access the LD matrix entries via the <code>[]</code> operator. This implementation supports the following types of indexing: * Accessing a single row of the LD matrix by specifying numeric index or SNP rsID. * Accessing a single entry of the LD matrix by specifying numeric indices or SNP rsIDs.</p> <p>Example usages:</p> <pre><code>&gt;&gt;&gt; ldm[0]\n&gt;&gt;&gt; ldm['rs123']\n&gt;&gt;&gt; ldm['rs123', 'rs456']\n</code></pre> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Access the LD matrix entries via the `[]` operator.\n    This implementation supports the following types of indexing:\n    * Accessing a single row of the LD matrix by specifying numeric index or SNP rsID.\n    * Accessing a single entry of the LD matrix by specifying numeric indices or SNP rsIDs.\n\n    Example usages:\n\n        &gt;&gt;&gt; ldm[0]\n        &gt;&gt;&gt; ldm['rs123']\n        &gt;&gt;&gt; ldm['rs123', 'rs456']\n    \"\"\"\n\n    dq_scale = self.dequantization_scale\n\n    if isinstance(item, tuple):\n        assert len(item) == 2\n        assert type(item[0]) is type(item[1])\n\n        if isinstance(item[0], str):\n\n            # If they're the same variant:\n            if item[0] == item[1]:\n                return 1.\n\n            # Extract the indices of the two variants:\n            snps = self.snps.tolist()\n\n            try:\n                index_1 = snps.index(item[0])\n            except ValueError:\n                raise ValueError(f\"Invalid variant rsID: {item[0]}\")\n\n            try:\n                index_2 = snps.index(item[1])\n            except ValueError:\n                raise ValueError(f\"Invalid variant rsID: {item[1]}\")\n\n        else:\n            index_1, index_2 = item\n\n        index_1, index_2 = sorted([index_1, index_2])\n\n        if index_1 == index_2:\n            return 1.\n        if index_2 - index_1 &gt; self.window_size[index_1]:\n            return 0.\n        else:\n            row = self.getrow(index_1)\n            return dq_scale*row[index_2 - index_1 - 1]\n\n    if isinstance(item, int):\n        return dq_scale*self.getrow(item)\n    elif isinstance(item, str):\n        try:\n            index = self.snps.tolist().index(item)\n        except ValueError:\n            raise ValueError(f\"Invalid variant rsID: {item}\")\n\n        return dq_scale*self.getrow(index)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.__init__","title":"<code>__init__(zarr_group)</code>","text":"<p>Initialize an <code>LDMatrix</code> object from a Zarr group store.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_group</code> <p>The Zarr group object that stores the LD matrix.</p> required Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def __init__(self, zarr_group):\n    \"\"\"\n    Initialize an `LDMatrix` object from a Zarr group store.\n\n    :param zarr_group: The Zarr group object that stores the LD matrix.\n    \"\"\"\n\n    # Checking the input for correct formatting:\n    # First, it has to be a Zarr group:\n    assert isinstance(zarr_group, zarr.hierarchy.Group)\n    # Second, it has to have a group called `matrix`:\n    assert 'matrix' in list(zarr_group.group_keys())\n\n    # Third, all the sparse array keys must be present:\n    arr_keys = list(zarr_group['matrix'].array_keys())\n    assert all([arr in arr_keys\n                for arr in ('data', 'indptr')])\n\n    # The Zarr storage hierarchy:\n    self._zg: zarr.hierarchy.Group = zarr_group\n\n    # Caching loaded/processed data of the LD matrix:\n    self._cached_lop: Union[LDLinearOperator, None] = None\n\n    # To support dynamically filtering the LD matrix:\n    self._mask = None\n    self._n_masked = 0\n\n    # To support iteration over the LD matrix:\n    self.index: int = 0\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the rows of the LD matrix. This iterator supports fetching the rows of the LD matrix one by one from on-disk storage, or, if the data is already loaded, it subsets the data for each row.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over the rows of the LD matrix.\n    This iterator supports fetching the rows of the LD matrix one by one from on-disk storage,\n    or, if the data is already loaded, it subsets the data for each row.\n    \"\"\"\n    self.index = 0\n    return self\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.__next__","title":"<code>__next__()</code>","text":"<p>Get the next row of the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def __next__(self):\n    \"\"\"\n    Get the next row of the LD matrix.\n    \"\"\"\n\n    if self.index == len(self):\n        self.index = 0\n        raise StopIteration\n\n    next_item = self.getrow(self.index)\n    self.index += 1\n\n    return next_item\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Type Description <p>A summary of the LDMatrix object as a string.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    :return: A summary of the LDMatrix object as a string.\n    \"\"\"\n    return self.summary().to_string()\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.compute_ld_scores","title":"<code>compute_ld_scores(annotation_matrix=None, corrected=True, chunk_size=10000)</code>","text":"<p>Computes the LD scores for variants in the LD matrix. LD Scores are defined as the sum of the squared pairwise Pearson Correlation coefficient between the focal SNP and all its neighboring SNPs. See Bulik-Sullivan et al. (2015) for details.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_matrix</code> <p>A matrix of annotations for each variant for which to aggregate the LD scores.</p> <code>None</code> <code>corrected</code> <p>Use the sample-size corrected estimator for the squared Pearson correlation coefficient. See Bulik-Sullivan et al. (2015).</p> <code>True</code> <code>chunk_size</code> <p>Specify the number of rows (i.e. SNPs) to compute the LD scores for simultaneously. Smaller chunk sizes should require less memory resources. If set to None, we compute LD scores for all SNPs in the LD matrix in one go.</p> <code>10000</code> <p>Returns:</p> Type Description <p>An array of LD scores for each variant in the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def compute_ld_scores(self,\n                      annotation_matrix=None,\n                      corrected=True,\n                      chunk_size=10_000):\n    \"\"\"\n\n    Computes the LD scores for variants in the LD matrix. LD Scores are defined\n    as the sum of the squared pairwise Pearson Correlation coefficient between the focal SNP and\n    all its neighboring SNPs. See Bulik-Sullivan et al. (2015) for details.\n\n    :param annotation_matrix: A matrix of annotations for each variant for which to aggregate the LD scores.\n    :param corrected: Use the sample-size corrected estimator for the squared Pearson correlation coefficient.\n        See Bulik-Sullivan et al. (2015).\n    :param chunk_size: Specify the number of rows (i.e. SNPs) to compute the LD scores for simultaneously.\n        Smaller chunk sizes should require less memory resources. If set to None, we compute LD scores\n        for all SNPs in the LD matrix in one go.\n\n    :return: An array of LD scores for each variant in the LD matrix.\n    \"\"\"\n\n    if chunk_size is None:\n        chunk_size = self.n_snps\n\n    if annotation_matrix is None:\n        annotation_matrix = np.ones((self.n_snps, 1), dtype=np.float32)\n    else:\n        assert annotation_matrix.shape[0] == self.n_snps, (\"Annotation matrix must have the same \"\n                                                           \"number of rows as the LD matrix.\")\n\n    ld_scores = np.zeros((self.n_snps, annotation_matrix.shape[1]), dtype=np.float32)\n\n    for chunk_idx in range(int(np.ceil(self.n_snps / chunk_size))):\n\n        start_row = chunk_idx*chunk_size\n        end_row = min((chunk_idx + 1)*chunk_size, self.n_snps)\n\n        csr_mat = self.load_data(start_row=start_row,\n                                 end_row=end_row,\n                                 return_symmetric=False,\n                                 return_square=False,\n                                 keep_original_shape=True,\n                                 return_as_csr=True,\n                                 dtype=np.float32)\n\n        mat_sq = csr_mat.power(2)\n\n        if corrected:\n            mat_sq.data -= (1. - mat_sq.data) / (self.sample_size - 2)\n\n        ld_scores += mat_sq.dot(annotation_matrix)\n        ld_scores += mat_sq.T.dot(annotation_matrix)\n\n    from scipy.sparse import identity\n\n    # Add the contribution of the diagonal:\n    ld_scores += identity(self.n_snps, dtype=np.float32).dot(annotation_matrix)\n\n    # Set floating type to float32:\n    ld_scores = ld_scores.astype(np.float32)\n\n    if ld_scores.shape[1] == 1:\n        return ld_scores.flatten()\n    else:\n        return ld_scores\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.dot","title":"<code>dot(vec)</code>","text":"<p>Multiply the LD matrix with an input vector <code>vec</code>.</p> <p>Parameters:</p> Name Type Description Default <code>vec</code> <p>The input vector to multiply with the LD matrix.  !!! seealso \"See Also\" * multiply</p> required <p>Returns:</p> Type Description <p>A numpy array that is the result of the matrix multiplication between the LD matrix with the input vector.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def dot(self, vec):\n    \"\"\"\n    Multiply the LD matrix with an input vector `vec`.\n\n    :param vec: The input vector to multiply with the LD matrix.\n\n    !!! seealso \"See Also\"\n        * [multiply][magenpy.LDMatrix.LDMatrix.multiply]\n\n    :return: A numpy array that is the result of the matrix multiplication between\n    the LD matrix with the input vector.\n\n    \"\"\"\n    return self.multiply(vec)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.estimate_extremal_eigenvalues","title":"<code>estimate_extremal_eigenvalues(block_size=None, block_size_cm=None, block_size_kb=None, blocks=None, which='both', return_block_boundaries=False, assign_to_variants=False)</code>","text":"<p>Estimate the smallest/largest algebraic eigenvalues of the LD matrix. This is useful for analyzing the spectral properties of the LD matrix and detecting potential issues for downstream applications that leverage the LD matrix. For instance, many LD matrices are not positive semi-definite (PSD) and this manifests in having negative eigenvalues. This function can be used to detect such issues.</p> <p>To perform this computation efficiently, we leverage fast ARPACK routines provided by <code>scipy</code> to compute only the extremal eigenvalues of the LD matrix. Another advantage of this implementation is that it doesn't require symmetric or dequantized LD matrices. The <code>LDLinearOperator</code> class can be used to perform all the computations without symmetrizing or dequantizing the matrix beforehand, which should make it more efficient in terms of memory and CPU resources.</p> <p>Furthermore, this function supports computing eigenvalues for sub-blocks of the LD matrix, by simply providing one of the following parameters:     * <code>block_size</code>: Number of variants per block     * <code>block_size_cm</code>: Block size in centi-Morgans     * <code>block_size_kb</code> Block size in kilobases</p> <p>Parameters:</p> Name Type Description Default <code>block_size</code> <p>An integer specifying the block size or number of variants to compute the minimum eigenvalue for. If provided, we compute minimum eigenvalues for each block in the LD matrix separately, instead of the minimum eigenvalue for the entire matrix. This can be useful for large LD matrices that don't fit in memory or in cases where information about local blocks is needed.</p> <code>None</code> <code>block_size_cm</code> <p>The block size in centi-Morgans (cM) to compute the minimum eigenvalue for.</p> <code>None</code> <code>block_size_kb</code> <p>The block size in kilo-base pairs (kb) to compute the minimum eigenvalue for.</p> <code>None</code> <code>blocks</code> <p>An array or list specifying the block boundaries to compute the minimum eigenvalue for. If there are B blocks, then the array should be of size B + 1, with the entries specifying the start position of each block.</p> <code>None</code> <code>which</code> <p>The extremal eigenvalues to compute. Options are 'min', 'max', or 'both'.</p> <code>'both'</code> <code>return_block_boundaries</code> <p>If True, return the block boundaries used to compute the minimum eigenvalue.</p> <code>False</code> <code>assign_to_variants</code> <p>If True, assign the minimum eigenvalue to the variants used to compute it.</p> <code>False</code> <p>Returns:</p> Type Description <p>The extremal eigenvalue(s) of the LD matrix or sub-blocks of the LD matrix. If <code>assign_to_variants</code> is set to True, then return an array of size <code>n_snps</code> mapping the extremal eigenvalues to each variant.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def estimate_extremal_eigenvalues(self,\n                                  block_size=None,\n                                  block_size_cm=None,\n                                  block_size_kb=None,\n                                  blocks=None,\n                                  which='both',\n                                  return_block_boundaries=False,\n                                  assign_to_variants=False):\n    \"\"\"\n    Estimate the smallest/largest algebraic eigenvalues of the LD matrix. This is useful for\n    analyzing the spectral properties of the LD matrix and detecting potential\n    issues for downstream applications that leverage the LD matrix. For instance, many LD\n    matrices are not positive semi-definite (PSD) and this manifests in having negative eigenvalues.\n    This function can be used to detect such issues.\n\n    To perform this computation efficiently, we leverage fast ARPACK routines provided by `scipy` to\n    compute only the extremal eigenvalues of the LD matrix. Another advantage of this implementation\n    is that it doesn't require symmetric or dequantized LD matrices. The `LDLinearOperator` class\n    can be used to perform all the computations without symmetrizing or dequantizing the matrix beforehand,\n    which should make it more efficient in terms of memory and CPU resources.\n\n    Furthermore, this function supports computing eigenvalues for sub-blocks of the LD matrix,\n    by simply providing one of the following parameters:\n        * `block_size`: Number of variants per block\n        * `block_size_cm`: Block size in centi-Morgans\n        * `block_size_kb` Block size in kilobases\n\n    :param block_size: An integer specifying the block size or number of variants to\n    compute the minimum eigenvalue for. If provided, we compute minimum eigenvalues for each block in the\n    LD matrix separately, instead of the minimum eigenvalue for the entire matrix. This can be useful for\n    large LD matrices that don't fit in memory or in cases where information about local blocks is needed.\n    :param block_size_cm: The block size in centi-Morgans (cM) to compute the minimum eigenvalue for.\n    :param block_size_kb: The block size in kilo-base pairs (kb) to compute the minimum eigenvalue for.\n    :param blocks: An array or list specifying the block boundaries to compute the minimum eigenvalue for.\n    If there are B blocks, then the array should be of size B + 1, with the entries specifying the start position\n    of each block.\n    :param which: The extremal eigenvalues to compute. Options are 'min', 'max', or 'both'.\n    :param return_block_boundaries: If True, return the block boundaries used to compute the minimum eigenvalue.\n    :param assign_to_variants: If True, assign the minimum eigenvalue to the variants used to compute it.\n\n    :return: The extremal eigenvalue(s) of the LD matrix or sub-blocks of the LD matrix. If `assign_to_variants`\n    is set to True, then return an array of size `n_snps` mapping the extremal eigenvalues to each variant.\n    \"\"\"\n\n    assert which in ('min', 'max', 'both')\n\n    if assign_to_variants:\n        if which == 'both':\n            eigs_per_var = np.zeros((self.stored_n_snps, 2), dtype=np.float32)\n        else:\n            eigs_per_var = np.zeros(self.stored_n_snps, dtype=np.float32)\n    else:\n        eigs = []\n\n    block_boundaries = []\n\n    from .stats.ld.utils import compute_extremal_eigenvalues\n\n    for mat, (start, end) in self.iter_blocks(block_size=block_size,\n                                              block_size_cm=block_size_cm,\n                                              block_size_kb=block_size_kb,\n                                              blocks=blocks,\n                                              return_type='linop',\n                                              return_block_boundaries=True):\n\n        block_boundaries.append({'block_start': start, 'block_end': end})\n        eig = compute_extremal_eigenvalues(mat, which=which)\n\n        if assign_to_variants:\n            if which == 'both':\n                eigs_per_var[start:end, 0] = eig['min']\n                eigs_per_var[start:end, 1] = eig['max']\n            else:\n                eigs_per_var[start:end] = eig\n        else:\n            eigs.append(eig)\n\n    block_boundaries = pd.DataFrame(block_boundaries).to_dict(orient='list')\n\n    if assign_to_variants:\n\n        if self._mask is not None:\n            eigs_per_var = eigs_per_var[self._mask, :]\n\n        if which == 'both':\n            eigs_per_var = {\n                'min': eigs_per_var[:, 0],\n                'max': eigs_per_var[:, 1]\n            }\n\n        if return_block_boundaries:\n            return eigs_per_var, block_boundaries\n        else:\n            return eigs_per_var\n\n    elif return_block_boundaries:\n        if which == 'both':\n            return pd.DataFrame(eigs).to_dict(orient='list'), block_boundaries\n        else:\n            return eigs, block_boundaries\n    else:\n        if len(eigs) == 1:\n            return eigs[0]\n        else:\n            if which == 'both':\n                return pd.DataFrame(eigs).to_dict(orient='list')\n            else:\n                return eigs\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.estimate_uncompressed_size","title":"<code>estimate_uncompressed_size(dtype=None)</code>","text":"<p>Provide an estimate of size of the uncompressed LD matrix in megabytes (MB). This is only a rough estimate. Depending on how the LD matrix is loaded, actual memory usage may be larger than this estimate.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>The data type for the entries of the LD matrix. If None, the stored data type is used to determine the size of the data in memory.</p> <code>None</code> <p>Returns:</p> Type Description <p>The estimated size of the uncompressed LD matrix in MB.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def estimate_uncompressed_size(self, dtype=None):\n    \"\"\"\n    Provide an estimate of size of the uncompressed LD matrix in megabytes (MB).\n    This is only a rough estimate. Depending on how the LD matrix is loaded, actual memory\n    usage may be larger than this estimate.\n\n    :param dtype: The data type for the entries of the LD matrix. If None, the stored data type is used\n    to determine the size of the data in memory.\n\n    :return: The estimated size of the uncompressed LD matrix in MB.\n\n    \"\"\"\n\n    if dtype is None:\n        dtype = self.stored_dtype\n\n    return 2.*self._zg['matrix/data'].shape[0]*np.dtype(dtype).itemsize / 1024 ** 2\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.filter_long_range_ld_regions","title":"<code>filter_long_range_ld_regions()</code>","text":"<p>A utility method to exclude variants that are in long-range LD regions. The boundaries of those regions are derived from here:</p> <p>https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)</p> <p>Which is based on the work of</p> <p>Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\" Nature protocols 5.9 (2010): 1564-1573.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def filter_long_range_ld_regions(self):\n    \"\"\"\n    A utility method to exclude variants that are in long-range LD regions. The\n    boundaries of those regions are derived from here:\n\n    https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n\n    Which is based on the work of\n\n    &gt; Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\"\n    Nature protocols 5.9 (2010): 1564-1573.\n    \"\"\"\n\n    # Filter the SNP to only those not in the LRLD regions:\n    self.filter_snps(self.snps[~self.get_long_range_ld_variants(return_value='mask')])\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None)</code>","text":"<p>Filter the LDMatrix to keep a subset of variants. This mainly sets the mask for the LD matrix, which is used to hide/remove some SNPs from the LD matrix, without altering the stored objects on-disk.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list or array of SNP rsIDs to keep.</p> <code>None</code> <code>extract_file</code> <p>A plink-style file containing the SNP rsIDs to keep.</p> <code>None</code> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None):\n    \"\"\"\n    Filter the LDMatrix to keep a subset of variants. This mainly sets\n    the mask for the LD matrix, which is used to hide/remove some SNPs from the LD matrix,\n    without altering the stored objects on-disk.\n\n    :param extract_snps: A list or array of SNP rsIDs to keep.\n    :param extract_file: A plink-style file containing the SNP rsIDs to keep.\n    \"\"\"\n\n    assert extract_snps is not None or extract_file is not None\n\n    if extract_snps is None:\n        from .parsers.misc_parsers import read_snp_filter_file\n        extract_snps = read_snp_filter_file(extract_file)\n\n    from .utils.compute_utils import intersect_arrays\n\n    new_mask = intersect_arrays(self.get_metadata('snps', apply_mask=False),\n                                extract_snps,\n                                return_index=True)\n\n    self.set_mask(new_mask)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_csr","title":"<code>from_csr(csr_mat, store_path, overwrite=False, dtype='int16', compressor_name='zstd', compression_level=7)</code>  <code>classmethod</code>","text":"<p>Initialize an LDMatrix object from a sparse CSR matrix.</p> <p>TODO: Determine the chunksize based on the avg neighborhood size?</p> <p>Parameters:</p> Name Type Description Default <code>csr_mat</code> <p>The sparse CSR matrix.</p> required <code>store_path</code> <p>The path to the Zarr LD store where the data will be stored.</p> required <code>overwrite</code> <p>If True, it overwrites the LD store at <code>store_path</code>.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor or compression algorithm to use with Zarr.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use with the compressor (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_csr(cls,\n             csr_mat,\n             store_path,\n             overwrite=False,\n             dtype='int16',\n             compressor_name='zstd',\n             compression_level=7):\n    \"\"\"\n    Initialize an LDMatrix object from a sparse CSR matrix.\n\n    TODO: Determine the chunksize based on the avg neighborhood size?\n\n    :param csr_mat: The sparse CSR matrix.\n    :param store_path: The path to the Zarr LD store where the data will be stored.\n    :param overwrite: If True, it overwrites the LD store at `store_path`.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n    :param compression_level: The compression level to use with the compressor (1-9).\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    from scipy.sparse import triu\n\n    dtype = np.dtype(dtype)\n\n    # Get the upper triangular part of the matrix:\n    triu_mat = triu(csr_mat, k=1, format='csr')\n\n    # Check that the non-zeros are contiguous around the diagonal with no gaps.\n    # If there are gaps, eliminate them or raise an error.\n    if np.diff(triu_mat.indices).max() &gt; 1:\n        # TODO: Figure out a way to fix this automatically for the user?\n        raise ValueError(\"The non-zero entries of the LD matrix are not contiguous around the diagonal.\")\n\n    # Create hierarchical storage with zarr groups:\n    store = zarr.DirectoryStore(store_path)\n    z = zarr.group(store=store, overwrite=overwrite)\n\n    # Create a compressor object:\n    compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n    # First sub-hierarchy stores the information for the sparse LD matrix:\n    mat = z.create_group('matrix')\n    if np.issubdtype(dtype, np.integer):\n        mat.array('data', quantize(triu_mat.data, int_dtype=dtype), dtype=dtype, compressor=compressor)\n    else:\n        mat.array('data', triu_mat.data.astype(dtype), dtype=dtype, compressor=compressor_name)\n\n    # Store the index pointer:\n    mat.array('indptr', triu_mat.indptr, dtype=np.int64, compressor=compressor)\n\n    return cls(z)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_dense_zarr_matrix","title":"<code>from_dense_zarr_matrix(dense_zarr, ld_boundaries, store_path, overwrite=False, delete_original=False, dtype='int16', compressor_name='zstd', compression_level=7)</code>  <code>classmethod</code>","text":"<p>Initialize a new LD matrix object using a Zarr array object. This method is useful for converting a dense LD matrix computed using Dask (or other distributed computing software) to a sparse or banded one.</p> <p>TODO: Determine the chunksize based on the avg neighborhood size?</p> <p>Parameters:</p> Name Type Description Default <code>dense_zarr</code> <p>The path to the dense Zarr array object.</p> required <code>ld_boundaries</code> <p>The LD boundaries for each SNP in the LD matrix (delineates the indices of the leftmost and rightmost neighbors of each SNP).</p> required <code>store_path</code> <p>The path where to store the new LD matrix.</p> required <code>overwrite</code> <p>If True, it overwrites the LD store at <code>store_path</code>.</p> <code>False</code> <code>delete_original</code> <p>If True, it deletes the original dense LD matrix.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor or compression algorithm to use with Zarr.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use with the compressor (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_dense_zarr_matrix(cls,\n                           dense_zarr,\n                           ld_boundaries,\n                           store_path,\n                           overwrite=False,\n                           delete_original=False,\n                           dtype='int16',\n                           compressor_name='zstd',\n                           compression_level=7):\n    \"\"\"\n     Initialize a new LD matrix object using a Zarr array object. This method is\n     useful for converting a dense LD matrix computed using Dask (or other distributed computing\n     software) to a sparse or banded one.\n\n     TODO: Determine the chunksize based on the avg neighborhood size?\n\n     :param dense_zarr: The path to the dense Zarr array object.\n     :param ld_boundaries: The LD boundaries for each SNP in the LD matrix (delineates the indices of\n        the leftmost and rightmost neighbors of each SNP).\n     :param store_path: The path where to store the new LD matrix.\n     :param overwrite: If True, it overwrites the LD store at `store_path`.\n     :param delete_original: If True, it deletes the original dense LD matrix.\n     :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n     :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n     :param compression_level: The compression level to use with the compressor (1-9).\n\n     :return: An `LDMatrix` object.\n    \"\"\"\n\n    dtype = np.dtype(dtype)\n\n    # If dense_zarr is a path, rather than a Zarr Array object, then\n    # open it as a Zarr array object before proceeding:\n    if isinstance(dense_zarr, str):\n        if osp.isfile(osp.join(dense_zarr, '.zarray')):\n            dense_zarr = zarr.open(dense_zarr)\n        else:\n            raise FileNotFoundError\n\n    # Create hierarchical storage with zarr groups:\n    store = zarr.DirectoryStore(store_path)\n    z = zarr.group(store=store, overwrite=overwrite)\n\n    # Create a compressor object:\n    compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n    # First sub-hierarchy stores the information for the sparse LD matrix:\n    mat = z.create_group('matrix')\n    mat.empty('data', shape=dense_zarr.shape[0]**2, dtype=dtype, compressor=compressor)\n\n    num_rows = dense_zarr.shape[0]\n    chunk_size = dense_zarr.chunks[0]\n\n    indptr_counts = np.zeros(num_rows, dtype=np.int32)\n\n    total_len = 0\n\n    for chunk_idx in range(int(np.ceil(num_rows / chunk_size))):\n\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = min((chunk_idx + 1) * chunk_size, num_rows)\n\n        z_chunk = dense_zarr[chunk_start: chunk_end]\n\n        data = []\n\n        chunk_len = 0\n\n        for j in range(chunk_start, chunk_end):\n\n            data.append(\n                z_chunk[j - chunk_start][j + 1:ld_boundaries[1, j]]\n            )\n            indptr_counts[j] = len(data[-1])\n            chunk_len += int(ld_boundaries[1, j] - (j+1))\n\n        # Add data + columns indices to zarr array:\n        concat_data = np.concatenate(data)\n\n        if np.issubdtype(dtype, np.integer):\n            mat['data'][total_len:total_len + chunk_len] = quantize(concat_data, int_dtype=dtype)\n        else:\n            mat['data'][total_len:total_len + chunk_len] = concat_data.astype(dtype)\n\n        total_len += chunk_len\n\n    # Get the final indptr by computing cumulative sum:\n    indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n    # Store indptr in the zarr array:\n    mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n    # Resize the data and indices arrays:\n    mat['data'].resize(total_len)\n\n    if delete_original:\n        from .stats.ld.utils import delete_ld_store\n        delete_ld_store(dense_zarr)\n\n    return cls(z)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_directory","title":"<code>from_directory(dir_path, cache_size=None)</code>  <code>classmethod</code>","text":"<p>Initialize an <code>LDMatrix</code> object from a Zarr array store.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <p>The path to the Zarr array store on the local filesystem.</p> required <code>cache_size</code> <p>The size of the cache for the Zarr store (in bytes).  Default is <code>None</code> (no caching).  !!! seealso \"See Also\" * from_s3 * from_path</p> <code>None</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_directory(cls, dir_path, cache_size=None):\n    \"\"\"\n    Initialize an `LDMatrix` object from a Zarr array store.\n    :param dir_path: The path to the Zarr array store on the local filesystem.\n    :param cache_size: The size of the cache for the Zarr store (in bytes).  Default is `None` (no caching).\n\n    !!! seealso \"See Also\"\n        * [from_s3][magenpy.LDMatrix.LDMatrix.from_s3]\n        * [from_path][magenpy.LDMatrix.LDMatrix.from_path]\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    for level in range(2):\n        try:\n            dir_store = zarr.storage.DirectoryStore(dir_path)\n            if cache_size is not None:\n                dir_store = zarr.LRUStoreCache(dir_store, max_size=cache_size)\n            ld_group = zarr.open_group(dir_store, mode='r')\n            return cls(ld_group)\n        except zarr.hierarchy.GroupNotFoundError as e:\n            if level &lt; 1:\n                dir_path = osp.dirname(dir_path)\n            else:\n                raise e\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_path","title":"<code>from_path(ld_store_path, cache_size=None)</code>  <code>classmethod</code>","text":"<p>Initialize an <code>LDMatrix</code> object from a pre-computed Zarr group store. This is a genetic method that can work with both cloud-based stores (e.g. s3 storage) or local filesystems.</p> <p>Parameters:</p> Name Type Description Default <code>ld_store_path</code> <p>The path to the Zarr array store.</p> required <code>cache_size</code> <p>The size of the cache for the Zarr store (in bytes). Default is <code>None</code> (no caching).  !!! seealso \"See Also\" * from_directory * from_s3</p> <code>None</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_path(cls, ld_store_path, cache_size=None):\n    \"\"\"\n    Initialize an `LDMatrix` object from a pre-computed Zarr group store. This is a genetic method\n    that can work with both cloud-based stores (e.g. s3 storage) or local filesystems.\n\n    :param ld_store_path: The path to the Zarr array store.\n    :param cache_size: The size of the cache for the Zarr store (in bytes). Default is `None` (no caching).\n\n    !!! seealso \"See Also\"\n        * [from_directory][magenpy.LDMatrix.LDMatrix.from_directory]\n        * [from_s3][magenpy.LDMatrix.LDMatrix.from_s3]\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    if 's3://' in ld_store_path:\n        return cls.from_s3(ld_store_path, cache_size)\n    else:\n        return cls.from_directory(ld_store_path, cache_size)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_plink_table","title":"<code>from_plink_table(plink_ld_file, snps, store_path, ld_boundaries=None, pandas_chunksize=None, overwrite=False, dtype='int16', compressor_name='zstd', compression_level=7)</code>  <code>classmethod</code>","text":"<p>Construct a Zarr LD matrix using LD tables generated by plink1.9.</p> <p>TODO: Determine the chunksize based on the avg neighborhood size?</p> <p>Parameters:</p> Name Type Description Default <code>plink_ld_file</code> <p>The path to the plink LD table file.</p> required <code>snps</code> <p>An iterable containing the list of ordered SNP rsIDs to be included in the LD matrix.</p> required <code>store_path</code> <p>The path to the Zarr LD store.</p> required <code>ld_boundaries</code> <p>The LD boundaries for each SNP in the LD matrix (delineates the indices of the leftmost and rightmost neighbors of each SNP). If not provided, the LD matrix will be constructed using the full LD table from plink.</p> <code>None</code> <code>pandas_chunksize</code> <p>If the LD table is large, provide chunk size (i.e. number of rows to process at each step) to keep memory footprint manageable.</p> <code>None</code> <code>overwrite</code> <p>If True, it overwrites the LD store at <code>store_path</code>.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor or compression algorithm to use with Zarr.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use with the compressor (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_plink_table(cls,\n                     plink_ld_file,\n                     snps,\n                     store_path,\n                     ld_boundaries=None,\n                     pandas_chunksize=None,\n                     overwrite=False,\n                     dtype='int16',\n                     compressor_name='zstd',\n                     compression_level=7):\n    \"\"\"\n    Construct a Zarr LD matrix using LD tables generated by plink1.9.\n\n    TODO: Determine the chunksize based on the avg neighborhood size?\n\n    :param plink_ld_file: The path to the plink LD table file.\n    :param snps: An iterable containing the list of ordered SNP rsIDs to be included in the LD matrix.\n    :param store_path: The path to the Zarr LD store.\n    :param ld_boundaries: The LD boundaries for each SNP in the LD matrix (delineates the indices of\n    the leftmost and rightmost neighbors of each SNP). If not provided, the LD matrix will be constructed\n    using the full LD table from plink.\n    :param pandas_chunksize: If the LD table is large, provide chunk size\n    (i.e. number of rows to process at each step) to keep memory footprint manageable.\n    :param overwrite: If True, it overwrites the LD store at `store_path`.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n    :param compression_level: The compression level to use with the compressor (1-9).\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    dtype = np.dtype(dtype)\n\n    # Create hierarchical storage with zarr groups:\n    store = zarr.DirectoryStore(store_path)\n    z = zarr.group(store=store, overwrite=overwrite)\n\n    # Create a compressor object:\n    compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n    # First sub-hierarchy stores the information for the sparse LD matrix:\n    mat = z.create_group('matrix')\n    mat.empty('data', shape=len(snps)**2, dtype=dtype, compressor=compressor)\n\n    if ld_boundaries is not None:\n        use_cols = ['SNP_A', 'SNP_B', 'R']\n        bounds_df = pd.DataFrame(np.column_stack((np.arange(len(snps)).reshape(-1, 1),\n                                                  ld_boundaries[1:, :].T)),\n                                 columns=['SNP_idx', 'end'])\n    else:\n        use_cols = ['SNP_A', 'R']\n\n    # Create a chunked iterator with pandas:\n    # Chunk size will correspond to the average chunk size for the Zarr array:\n    ld_chunks = pd.read_csv(plink_ld_file,\n                            sep=r'\\s+',\n                            usecols=use_cols,\n                            engine='c',\n                            chunksize=pandas_chunksize,\n                            dtype={'SNP_A': str, 'R': np.float32})\n\n    if pandas_chunksize is None:\n        ld_chunks = [ld_chunks]\n\n    # Create a dictionary mapping SNPs to their indices:\n    snp_idx = pd.Series(np.arange(len(snps), dtype=np.int32), index=snps)\n\n    indptr_counts = np.zeros(len(snps), dtype=np.int32)\n\n    total_len = 0\n\n    # For each chunk in the LD file:\n    for ld_chunk in ld_chunks:\n\n        # Fill N/A in R before storing it:\n        ld_chunk.fillna({'R': 0.}, inplace=True)\n\n        # If LD boundaries are provided, filter the LD table accordingly:\n        if ld_boundaries is not None:\n\n            row_index = snp_idx[ld_chunk['SNP_A'].values]\n\n            ld_chunk['SNP_A_index'] = snp_idx[ld_chunk['SNP_A'].values].values\n            ld_chunk['SNP_B_index'] = snp_idx[ld_chunk['SNP_B'].values].values\n\n            ld_chunk = ld_chunk.merge(bounds_df, left_on='SNP_A_index', right_on='SNP_idx')\n            ld_chunk = ld_chunk.loc[(ld_chunk['SNP_B_index'] &gt;= ld_chunk['SNP_A_index'] + 1) &amp;\n                                    (ld_chunk['SNP_B_index'] &lt; ld_chunk['end'])]\n\n        # Create an indexed LD chunk:\n        row_index = snp_idx[ld_chunk['SNP_A'].values]\n\n        # Add LD data to the zarr array:\n        if np.issubdtype(dtype, np.integer):\n            mat['data'][total_len:total_len + len(ld_chunk)] = quantize(ld_chunk['R'].values, int_dtype=dtype)\n        else:\n            mat['data'][total_len:total_len + len(ld_chunk)] = ld_chunk['R'].values.astype(dtype)\n\n        total_len += len(ld_chunk)\n\n        # Count the number of occurrences of each SNP in the chunk:\n        snp_counts = row_index.value_counts()\n\n        # Add the number of entries to indptr_counts:\n        indptr_counts[snp_counts.index] += snp_counts.values\n\n    # Get the final indptr by computing cumulative sum:\n    indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n    # Store indptr in the zarr group:\n    mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n    # Resize the data array:\n    mat['data'].resize(total_len)\n\n    return cls(z)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_ragged_zarr_matrix","title":"<code>from_ragged_zarr_matrix(ragged_zarr, store_path, overwrite=False, delete_original=False, dtype='int16', compressor_name='zstd', compression_level=7)</code>  <code>classmethod</code>","text":"<p>Initialize a new LD matrix object using a Zarr array object conforming to the old LD Matrix format from magenpy v&lt;=0.0.12.</p> <p>This utility function will also copy some of the stored attributes associated with the matrix in the old format.</p> <p>TODO: Determine the chunksize based on the avg neighborhood size?</p> <p>Parameters:</p> Name Type Description Default <code>ragged_zarr</code> <p>The path to the ragged Zarr array object.</p> required <code>store_path</code> <p>The path where to store the new LD matrix.</p> required <code>overwrite</code> <p>If True, it overwrites the LD store at <code>store_path</code>.</p> <code>False</code> <code>delete_original</code> <p>If True, it deletes the original ragged LD matrix.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor or compression algorithm to use with Zarr.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use with the compressor (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_ragged_zarr_matrix(cls,\n                            ragged_zarr,\n                            store_path,\n                            overwrite=False,\n                            delete_original=False,\n                            dtype='int16',\n                            compressor_name='zstd',\n                            compression_level=7):\n    \"\"\"\n    Initialize a new LD matrix object using a Zarr array object\n    conforming to the old LD Matrix format from magenpy v&lt;=0.0.12.\n\n    This utility function will also copy some of the stored attributes\n    associated with the matrix in the old format.\n\n    TODO: Determine the chunksize based on the avg neighborhood size?\n\n    :param ragged_zarr: The path to the ragged Zarr array object.\n    :param store_path: The path where to store the new LD matrix.\n    :param overwrite: If True, it overwrites the LD store at `store_path`.\n    :param delete_original: If True, it deletes the original ragged LD matrix.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor or compression algorithm to use with Zarr.\n    :param compression_level: The compression level to use with the compressor (1-9).\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    dtype = np.dtype(dtype)\n\n    # If ragged_zarr is a path, rather than a Zarr Array object, then\n    # open it as a Zarr array object before proceeding:\n    if isinstance(ragged_zarr, str):\n        if osp.isfile(osp.join(ragged_zarr, '.zarray')):\n            ragged_zarr = zarr.open(ragged_zarr)\n        else:\n            raise FileNotFoundError\n\n    num_rows = ragged_zarr.shape[0]\n    chunk_size = ragged_zarr.chunks[0]\n\n    # Create hierarchical storage with zarr groups:\n    store = zarr.DirectoryStore(store_path)\n    z = zarr.group(store=store, overwrite=overwrite)\n\n    # Create a compressor object:\n    compressor = zarr.Blosc(cname=compressor_name, clevel=compression_level)\n\n    # First sub-hierarchy stores the information for the sparse LD matrix:\n    mat = z.create_group('matrix')\n    mat.empty('data', shape=num_rows ** 2, dtype=dtype, compressor=compressor)\n\n    indptr_counts = np.zeros(num_rows, dtype=np.int64)\n\n    # Get the LD boundaries from the Zarr array attributes:\n    ld_boundaries = np.array(ragged_zarr.attrs['LD boundaries'])\n\n    total_len = 0\n\n    for chunk_idx in range(int(np.ceil(num_rows / chunk_size))):\n\n        chunk_start = chunk_idx * chunk_size\n        chunk_end = min((chunk_idx + 1) * chunk_size, num_rows)\n\n        z_chunk = ragged_zarr[chunk_start: chunk_end]\n\n        data = []\n        chunk_len = 0\n\n        for j in range(chunk_start, chunk_end):\n\n            start, end = ld_boundaries[:, j]\n            new_start = (j - start) + 1\n\n            data.append(\n                z_chunk[j - chunk_start][new_start:]\n            )\n            indptr_counts[j] = end - (j + 1)\n            chunk_len += int(end - (j + 1))\n\n        # Add data + columns indices to zarr array:\n        concat_data = np.concatenate(data)\n\n        if np.issubdtype(dtype, np.integer):\n            mat['data'][total_len:total_len + chunk_len] = quantize(concat_data, int_dtype=dtype)\n        else:\n            mat['data'][total_len:total_len + chunk_len] = concat_data.astype(dtype)\n\n        total_len += chunk_len\n\n    # Get the final indptr by computing cumulative sum:\n    indptr = np.insert(np.cumsum(indptr_counts, dtype=np.int64), 0, 0)\n    # Store indptr in the zarr array:\n    mat.array('indptr', indptr, dtype=np.int64, compressor=compressor)\n\n    # Resize the data and indices arrays:\n    mat['data'].resize(total_len)\n\n    # ============================================================\n    # Transfer the attributes/metadata from the old matrix format:\n\n    ld_mat = cls(z)\n\n    ld_mat.set_metadata('snps', np.array(ragged_zarr.attrs['SNP']))\n    ld_mat.set_metadata('a1', np.array(ragged_zarr.attrs['A1']))\n    ld_mat.set_metadata('a2', np.array(ragged_zarr.attrs['A2']))\n    ld_mat.set_metadata('maf', np.array(ragged_zarr.attrs['MAF']))\n    ld_mat.set_metadata('bp', np.array(ragged_zarr.attrs['BP']))\n    ld_mat.set_metadata('cm', np.array(ragged_zarr.attrs['cM']))\n\n    try:\n        ld_mat.set_metadata('ldscore', np.array(ragged_zarr.attrs['LDScore']))\n    except KeyError:\n        pass\n\n    # Set matrix attributes:\n    ld_mat.set_store_attr('Chromosome', ragged_zarr.attrs['Chromosome'])\n    ld_mat.set_store_attr('LD estimator', ragged_zarr.attrs['LD estimator'])\n    ld_mat.set_store_attr('Estimator properties', ragged_zarr.attrs['Estimator properties'])\n    ld_mat.set_store_attr('Sample size', ragged_zarr.attrs['Sample size'])\n\n    if delete_original:\n        from .stats.ld.utils import delete_ld_store\n        delete_ld_store(ragged_zarr)\n\n    return ld_mat\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.from_s3","title":"<code>from_s3(s3_path, cache_size=None)</code>  <code>classmethod</code>","text":"<p>Initialize an <code>LDMatrix</code> object from a Zarr group store hosted on AWS s3 storage.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <p>The path to the Zarr group store on AWS s3. s3 paths are expected to be of the form <code>s3://bucket-name/path/to/zarr-store</code>.</p> required <code>cache_size</code> <p>The size of the cache for the Zarr store (in bytes). Default is 16MB.  .. note:: Requires installing the <code>s3fs</code> package to access the Zarr store on AWS s3.  !!! seealso \"See Also\" * from_path * from_directory</p> <code>None</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>@classmethod\ndef from_s3(cls, s3_path, cache_size=None):\n    \"\"\"\n    Initialize an `LDMatrix` object from a Zarr group store hosted on AWS s3 storage.\n\n    :param s3_path: The path to the Zarr group store on AWS s3. s3 paths are expected\n    to be of the form `s3://bucket-name/path/to/zarr-store`.\n    :param cache_size: The size of the cache for the Zarr store (in bytes). Default is 16MB.\n\n    .. note::\n        Requires installing the `s3fs` package to access the Zarr store on AWS s3.\n\n    !!! seealso \"See Also\"\n        * [from_path][magenpy.LDMatrix.LDMatrix.from_path]\n        * [from_directory][magenpy.LDMatrix.LDMatrix.from_directory]\n\n    :return: An `LDMatrix` object.\n    \"\"\"\n\n    import s3fs\n\n    s3 = s3fs.S3FileSystem(anon=True, client_kwargs=dict(region_name='us-east-2'))\n    store = s3fs.S3Map(root=s3_path.replace('s3://', ''), s3=s3, check=False)\n    if cache_size is not None:\n        store = zarr.LRUStoreCache(store, max_size=cache_size)\n    ld_group = zarr.open_group(store=store, mode='r')\n\n    return cls(ld_group)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_lambda_min","title":"<code>get_lambda_min(aggregate=None, min_max_ratio=0.0)</code>","text":"<p>A utility method to compute the <code>lambda_min</code> value for the LD matrix. <code>lambda_min</code> is the smallest algebraic eigenvalue of the LD matrix. This quantity is useful to know in some applications. The function retrieves minimum eigenvalue (if pre-computed and stored) per block and maps it to each variant in the corresponding block. If minimum eigenvalues per block are not available,  we use global minimum eigenvalue (either from matrix attributes or we compute it on the spot).</p> <p>Before returning the <code>lambda_min</code> value to the user, we apply the following transformation:</p> <p>abs(min(lambda_min, 0.))</p> <p>This implies that if the minimum eigenvalue is non-negative, we just return 0. for <code>lambda_min</code>. We are mainly interested in negative eigenvalues here (if they exist).</p> <p>Parameters:</p> Name Type Description Default <code>aggregate</code> <p>A summary of the minimum eigenvalue across variants or across blocks (if available). Supported aggregation functions are <code>min_block</code> and <code>min</code>. If <code>min</code> is selected, we return the minimum eigenvalue for the entire matrix (rather than sub-blocks of it). If <code>min_block</code> is selected, we return the minimum eigenvalue for each block separately (mapped to variants within that block).</p> <code>None</code> <code>min_max_ratio</code> <p>The ratio between the absolute values of the minimum and maximum eigenvalues. This could be used to target a particular threshold for the minimum eigenvalue.</p> <code>0.0</code> <p>Returns:</p> Type Description <p>The absolute value of the minimum eigenvalue for the LD matrix. If the minimum eigenvalue is non-negative, we return zero.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_lambda_min(self, aggregate=None, min_max_ratio=0.):\n    \"\"\"\n    A utility method to compute the `lambda_min` value for the LD matrix. `lambda_min` is the smallest\n    algebraic eigenvalue of the LD matrix. This quantity is useful to know in some applications.\n    The function retrieves minimum eigenvalue (if pre-computed and stored) per block and maps it\n    to each variant in the corresponding block. If minimum eigenvalues per block are not available,\n     we use global minimum eigenvalue (either from matrix attributes or we compute it on the spot).\n\n    Before returning the `lambda_min` value to the user, we apply the following transformation:\n\n    abs(min(lambda_min, 0.))\n\n    This implies that if the minimum eigenvalue is non-negative, we just return 0. for `lambda_min`. We are mainly\n    interested in negative eigenvalues here (if they exist).\n\n    :param aggregate: A summary of the minimum eigenvalue across variants or across blocks (if available).\n    Supported aggregation functions are `min_block` and `min`. If `min` is selected,\n    we return the minimum eigenvalue for the entire matrix (rather than sub-blocks of it). If `min_block` is\n    selected, we return the minimum eigenvalue for each block separately (mapped to variants within that block).\n\n    :param min_max_ratio: The ratio between the absolute values of the minimum and maximum eigenvalues.\n    This could be used to target a particular threshold for the minimum eigenvalue.\n\n    :return: The absolute value of the minimum eigenvalue for the LD matrix. If the minimum\n    eigenvalue is non-negative, we return zero.\n    \"\"\"\n\n    if aggregate is not None:\n        assert aggregate in ('min_block', 'min')\n\n    # Get the attributes of the LD store:\n    store_attrs = self.list_store_attributes()\n\n    def threshold_lambda_min(eigs):\n        return np.abs(np.minimum(eigs['min'] + min_max_ratio*eigs['max'], 0.)) / (1. + min_max_ratio)\n\n    lambda_min = 0.\n\n    if 'Spectral properties' not in store_attrs:\n        if aggregate in ('mean_block', 'median_block', 'min_block'):\n            raise ValueError('Aggregating lambda_min across blocks '\n                             'requires that these blocks are pre-defined.')\n        else:\n            lambda_min = threshold_lambda_min(self.estimate_extremal_eigenvalues())\n\n    else:\n\n        spectral_props = self.get_store_attr('Spectral properties')\n\n        if aggregate == 'min_block':\n            assert 'Eigenvalues per block' in spectral_props, (\n                'Aggregating lambda_min across blocks '\n                'requires that these blocks are pre-defined.')\n\n        if aggregate == 'min' or 'Eigenvalues per block' not in spectral_props:\n\n            if 'Extremal' in spectral_props:\n                lambda_min = threshold_lambda_min(spectral_props['Extremal'])\n            else:\n                lambda_min = threshold_lambda_min(self.estimate_extremal_eigenvalues())\n\n        elif 'Eigenvalues per block' in spectral_props:\n\n            # If we have eigenvalues per block, map the block value to each variant:\n            block_eigs = spectral_props['Eigenvalues per block']\n\n            if aggregate is None:\n\n                # Create a dataframe with the block information:\n                block_df = pd.DataFrame(block_eigs)\n                block_df['add_lam'] = block_df.apply(threshold_lambda_min, axis=1)\n\n                merged_df = pd.merge_asof(pd.DataFrame({'SNP_idx': np.arange(self.stored_n_snps)}),\n                                          block_df,\n                                          right_on='block_start', left_on='SNP_idx', direction='backward')\n                # Filter merged_df to only include variants that were matched properly with a block:\n                merged_df = merged_df.loc[\n                    (merged_df.SNP_idx &gt;= merged_df.block_start) &amp; (merged_df.SNP_idx &lt; merged_df.block_end)\n                ]\n\n                if len(merged_df) &lt; 1:\n                    raise ValueError('No variants were matched to blocks. '\n                                     'This could be due to incorrect block boundaries.')\n\n                lambda_min = np.zeros(self.stored_n_snps)\n                lambda_min[merged_df['SNP_idx'].values] = merged_df['add_lam'].values\n\n                if self.is_mask_set:\n                    lambda_min = lambda_min[self._mask]\n\n            elif aggregate == 'min_block':\n                lambda_min = np.min(block_eigs['min'])\n\n    return lambda_min\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_long_range_ld_variants","title":"<code>get_long_range_ld_variants(return_value='snps')</code>","text":"<p>A utility method to exclude variants that are in long-range LD regions. The boundaries of those regions are derived from here:</p> <p>https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)</p> <p>Which is based on the work of</p> <p>Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\" Nature protocols 5.9 (2010): 1564-1573.</p> <p>Parameters:</p> Name Type Description Default <code>return_value</code> <p>The value to return. Options are 'mask', 'index', 'snps'. If <code>mask</code>, then it returns a boolean array of which variants are in long-range LD regions. If <code>index</code>, then it returns the index of those variants. If <code>snps</code>, then it returns the rsIDs of those variants.</p> <code>'snps'</code> <p>Returns:</p> Type Description <p>An array of the variants that are in long-range LD regions.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_long_range_ld_variants(self, return_value='snps'):\n    \"\"\"\n    A utility method to exclude variants that are in long-range LD regions. The\n    boundaries of those regions are derived from here:\n\n    https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n\n    Which is based on the work of\n\n    &gt; Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\"\n    Nature protocols 5.9 (2010): 1564-1573.\n\n    :param return_value: The value to return. Options are 'mask', 'index', 'snps'. If `mask`, then\n    it returns a boolean array of which variants are in long-range LD regions. If `index`, then it returns\n    the index of those variants. If `snps`, then it returns the rsIDs of those variants.\n\n    :return: An array of the variants that are in long-range LD regions.\n    \"\"\"\n\n    assert return_value in ('mask', 'index', 'snps')\n\n    from .parsers.annotation_parsers import parse_annotation_bed_file\n    from .utils.data_utils import lrld_path\n\n    bed_df = parse_annotation_bed_file(lrld_path())\n\n    # Filter to only regions specific to the chromosome of this matrix:\n    bed_df = bed_df.loc[bed_df['CHR'] == self.chromosome]\n\n    bp_pos = self.bp_position\n    snp_mask = np.zeros(len(bp_pos), dtype=bool)\n\n    # Loop over the LRLD region on this chromosome and include the SNPs in these regions:\n    for _, row in bed_df.iterrows():\n        start, end = row['Start'], row['End']\n        snp_mask |= ((bp_pos &gt;= start) &amp; (bp_pos &lt;= end))\n\n    if return_value == 'mask':\n        return snp_mask\n    elif return_value == 'index':\n        return np.where(snp_mask)[0]\n    else:\n        return self.snps[snp_mask]\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_mask","title":"<code>get_mask()</code>","text":"<p>Returns:</p> Type Description <p>The mask (a boolean flag array) used to hide/remove some SNPs from the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_mask(self):\n    \"\"\"\n    :return: The mask (a boolean flag array) used to hide/remove some SNPs from the LD matrix.\n    \"\"\"\n    return self._mask\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_metadata","title":"<code>get_metadata(key, apply_mask=True)</code>","text":"<p>Get the metadata associated with each variant in the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The key for the metadata item.</p> required <code>apply_mask</code> <p>If True, apply the mask (e.g. filter) to the metadata.</p> <code>True</code> <p>Returns:</p> Type Description <p>The metadata item for each variant in the LD matrix.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>if the metadata item is not set.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_metadata(self, key, apply_mask=True):\n    \"\"\"\n    Get the metadata associated with each variant in the LD matrix.\n\n    :param key: The key for the metadata item.\n    :param apply_mask: If True, apply the mask (e.g. filter) to the metadata.\n\n    :return: The metadata item for each variant in the LD matrix.\n    :raises KeyError: if the metadata item is not set.\n    \"\"\"\n    try:\n        metadata = self._zg[f'metadata/{key}'][:]\n    except KeyError:\n        raise KeyError(f\"LD matrix metadata item {key} is not set!\")\n\n    if self.is_mask_set and apply_mask:\n        metadata = metadata[self._mask]\n\n    return metadata\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_store_attr","title":"<code>get_store_attr(attr)</code>","text":"<p>Get the attribute or metadata <code>attr</code> associated with the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>attr</code> <p>The attribute name.</p> required <p>Returns:</p> Type Description <p>The value for the attribute.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>if the attribute is not set.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_store_attr(self, attr):\n    \"\"\"\n    Get the attribute or metadata `attr` associated with the LD matrix.\n    :param attr: The attribute name.\n\n    :return: The value for the attribute.\n    :raises KeyError: if the attribute is not set.\n    \"\"\"\n    return self._zg.attrs[attr]\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.get_total_storage_size","title":"<code>get_total_storage_size()</code>","text":"<p>Estimate the storage size for all elements of the <code>LDMatrix</code> hierarchy, including the LD data arrays, metadata arrays, and attributes.</p> <p>Returns:</p> Type Description <p>The estimated size of the stored and compressed LDMatrix object in MB.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def get_total_storage_size(self):\n    \"\"\"\n    Estimate the storage size for all elements of the `LDMatrix` hierarchy,\n    including the LD data arrays, metadata arrays, and attributes.\n\n    :return: The estimated size of the stored and compressed LDMatrix object in MB.\n    \"\"\"\n\n    total_bytes = 0\n\n    # Estimate contribution of matrix arrays\n    for arr_name, array in self.zarr_group.matrix.arrays():\n        total_bytes += array.nbytes_stored\n\n    # Estimate contribution of metadata arrays\n    for arr_name, array in self.zarr_group.metadata.arrays():\n        total_bytes += array.nbytes_stored\n\n    # Estimate the contribution of the attributes:\n    if hasattr(self.zarr_group, 'attrs'):\n        total_bytes += len(str(dict(self.zarr_group.attrs)).encode('utf-8'))\n\n    return total_bytes / 1024**2\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.getrow","title":"<code>getrow(index, symmetric=False, return_indices=False)</code>","text":"<p>Extract a single row from the LD matrix.</p>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.getrow--todo-support-extracting-rows-from-the-symmetric-ld-matrix","title":"TODO: Support extracting rows from the symmetric LD matrix.","text":""},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.getrow--todo-verify-that-this-is-correct","title":"TODO: Verify that this is correct.","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <p>The index of the row to extract.</p> required <code>symmetric</code> <p>If True, return a symmetric representation of the row (i.e. LD with variants before and after the index variant).</p> <code>False</code> <code>return_indices</code> <p>If True, return the indices of the non-zero elements of that row.</p> <code>False</code> <p>Returns:</p> Type Description <p>The requested row of the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def getrow(self, index, symmetric=False, return_indices=False):\n    \"\"\"\n    Extract a single row from the LD matrix.\n\n    # TODO: Support extracting rows from the symmetric LD matrix.\n    # TODO: Verify that this is correct.\n\n    :param index: The index of the row to extract.\n    :param symmetric: If True, return a symmetric representation of the row (i.e. LD with\n    variants before and after the index variant).\n    :param return_indices: If True, return the indices of the non-zero elements of that row.\n\n    :return: The requested row of the LD matrix.\n    \"\"\"\n\n    if symmetric:\n        raise NotImplementedError(\"Symmetric row extraction is not yet supported.\")\n\n    if self.in_memory:\n        return self._cached_lop.getrow(index, symmetric=symmetric, return_indices=return_indices)\n    else:\n        start_idx, end_idx = self.indptr[index:index + 2]\n        data = self.data[start_idx:end_idx]\n\n        if return_indices:\n            return data, np.arange(\n                index + 1,\n                index + len(data)\n            )\n        else:\n            return data\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.iter_blocks","title":"<code>iter_blocks(block_size=None, block_size_cm=None, block_size_kb=None, blocks=None, min_block_size=2, max_block_size=None, return_type='csr', return_block_boundaries=False, dry_run=False, **return_type_kwargs)</code>","text":"<p>Iterator over blocks of the LD matrix.</p> <p>This function allows for iterating over blocks of the LD matrix, either based on the number of variants per block, or based on the physical distance between variants. The function yields the requested data in the form of a sparse CSR matrix, a <code>LinearOperator</code>, or a dense numpy array.</p> <p>.. note::     For now, all block-related information is with reference to the original, unfiltered LD matrix.     In future releases, we may consider supporting block iterators based on subsets of the     matrix.</p> <p>Parameters:</p> Name Type Description Default <code>block_size</code> <p>An integer specifying the block size in terms of the number of variants.</p> <code>None</code> <code>block_size_cm</code> <p>The block size in centi-Morgans (cM).</p> <code>None</code> <code>block_size_kb</code> <p>The block size in kilo-base pairs (kb).</p> <code>None</code> <code>blocks</code> <p>An array or list specifying the block boundaries to iterate over.</p> <code>None</code> <code>min_block_size</code> <p>The minimum block size.</p> <code>2</code> <code>return_type</code> <p>The type of data to return. Options are 'csr', 'linop', or 'numpy'.</p> <code>'csr'</code> <code>return_block_boundaries</code> <p>If True, return the boundaries of the generated blocks along with the LD data itself.</p> <code>False</code> <code>dry_run</code> <p>If True, do not load the data, just return the block boundaries. Useful for debugging.</p> <code>False</code> <code>return_type_kwargs</code> <p>Additional keyword arguments to pass to the return type constructor.</p> <code>{}</code> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def iter_blocks(self,\n                block_size=None,\n                block_size_cm=None,\n                block_size_kb=None,\n                blocks=None,\n                min_block_size=2,\n                max_block_size=None,\n                return_type='csr',\n                return_block_boundaries=False,\n                dry_run=False,\n                **return_type_kwargs\n                ):\n    \"\"\"\n    Iterator over blocks of the LD matrix.\n\n    This function allows for iterating over blocks of the LD matrix, either based on the number of variants\n    per block, or based on the physical distance between variants. The function yields the requested data\n    in the form of a sparse CSR matrix, a `LinearOperator`, or a dense numpy array.\n\n    .. note::\n        For now, all block-related information is with reference to the original, unfiltered LD matrix.\n        In future releases, we may consider supporting block iterators based on subsets of the\n        matrix.\n\n    :param block_size: An integer specifying the block size in terms of the number of variants.\n    :param block_size_cm: The block size in centi-Morgans (cM).\n    :param block_size_kb: The block size in kilo-base pairs (kb).\n    :param blocks: An array or list specifying the block boundaries to iterate over.\n    :param min_block_size: The minimum block size.\n    :param return_type: The type of data to return. Options are 'csr', 'linop', or 'numpy'.\n    :param return_block_boundaries: If True, return the boundaries of the generated blocks along with the\n    LD data itself.\n    :param dry_run: If True, do not load the data, just return the block boundaries. Useful for debugging.\n    :param return_type_kwargs: Additional keyword arguments to pass to the return type constructor.\n\n    \"\"\"\n\n    # Sanity checks:\n    assert return_type in ('csr', 'linop', 'numpy')\n    assert min_block_size &gt;= 1\n\n    # Determine the block boundaries based on the input parameters:\n    if self.in_memory:\n        n_snps = self.n_snps\n    else:\n        n_snps = self.stored_n_snps\n\n    from .utils.compute_utils import generate_overlapping_windows\n\n    if blocks is not None:\n        block_iter = blocks\n    elif block_size is not None:\n\n        windows = generate_overlapping_windows(np.arange(n_snps),\n                                               block_size - 1, block_size,\n                                               min_window_size=min_block_size)\n        block_iter = np.insert(windows[:, 1], 0, 0)\n\n    elif block_size_cm is not None or block_size_kb is not None:\n\n        if block_size_cm is not None:\n            dist = self.get_metadata('cm', apply_mask=self.in_memory)\n            block_size = block_size_cm\n        else:\n            dist = self.get_metadata('bp', apply_mask=self.in_memory) / 1000\n            block_size = block_size_kb\n\n        windows = generate_overlapping_windows(dist,\n                                               block_size,\n                                               block_size,\n                                               min_window_size=min_block_size)\n\n        block_iter = np.insert(windows[:, 1], 0, 0)\n    elif self.ld_estimator == 'windowed':\n\n        est_properties = self.estimator_properties\n\n        if 'Window size (cM)' in est_properties:\n            block_size = est_properties['Window size (cM)']\n            dist = self.get_metadata('cm', apply_mask=self.in_memory)\n        elif 'Window size (kb)' in est_properties:\n            block_size = est_properties['Window size (kb)']\n            dist = self.get_metadata('bp', apply_mask=self.in_memory) / 1000\n        else:\n            block_size = est_properties['Window size']\n            dist = np.arange(n_snps)\n\n        windows = generate_overlapping_windows(dist,\n                                               block_size,\n                                               block_size,\n                                               min_window_size=min_block_size)\n\n        block_iter = np.insert(windows[:, 1], 0, 0)\n\n    elif self.ld_estimator == 'block':\n\n        from .utils.model_utils import map_variants_to_genomic_blocks\n\n        variants_to_blocks = map_variants_to_genomic_blocks(\n            pd.DataFrame({\n                'POS': self.get_metadata('bp', apply_mask=self.in_memory)\n            }).reset_index(),\n            pd.DataFrame(np.array(self.estimator_properties['LD blocks']),\n                         columns=['block_start', 'block_end'],\n                         dtype=np.int32),\n            filter_unmatched=True\n        )\n\n        block_iter = [0] + list(variants_to_blocks.groupby('block_end')['index'].max().values + 1)\n    else:\n        block_iter = [0, n_snps]\n\n    # If the maximum block size is specified by the user,\n    # then use the `split_block_boundaries` utility function to split\n    # blocks to conform to this constraint:\n    if max_block_size is not None:\n\n        from .utils.compute_utils import split_block_boundaries\n\n        block_iter = split_block_boundaries(\n            block_iter,\n            max_block_size,\n            mask=[self._mask, None][self.in_memory]\n        )\n\n    mat = None\n\n    # Loop over the blocks and yield the requested data:\n    for bidx in range(len(block_iter) - 1):\n\n        start = block_iter[bidx]\n        end = block_iter[bidx + 1]\n\n        if dry_run:\n            yield start, end\n        else:\n            # If the data is in memory, subset the data for the requested block:\n            if self.in_memory:\n\n                if return_type == 'numpy':\n                    mat = self._cached_lop.to_numpy(start, end)\n                elif return_type in ('csr', 'linop'):\n                    mat = self._cached_lop[start:end, start:end]\n                    if return_type == 'csr':\n                        mat = mat.to_csr()\n\n            else:\n\n                mat = self.load_data(start_row=start,\n                                     end_row=end,\n                                     return_as_csr=return_type in ('csr', 'numpy'),\n                                     **return_type_kwargs)\n                if return_type == 'numpy':\n                    mat = mat.todense()\n\n            if return_block_boundaries:\n                yield mat, (start, end)\n            else:\n                yield mat\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.list_store_attributes","title":"<code>list_store_attributes()</code>","text":"<p>Get all the attributes associated with the LD matrix.</p> <p>Returns:</p> Type Description <p>A list of all the attributes.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def list_store_attributes(self):\n    \"\"\"\n    Get all the attributes associated with the LD matrix.\n    :return: A list of all the attributes.\n    \"\"\"\n    return list(self._zg.attrs.keys())\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.load","title":"<code>load(force_reload=False, return_symmetric=False, dtype=None)</code>","text":"<p>Load the LD matrix from on-disk storage in the form of Zarr arrays to memory, in the form of sparse CSR matrices.</p> <p>Parameters:</p> Name Type Description Default <code>force_reload</code> <p>If True, it will reload the data even if it is already in memory.</p> <code>False</code> <code>return_symmetric</code> <p>If True, return a full symmetric representation of the LD matrix.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix.  !!! seealso \"See Also\" * load_data</p> <code>None</code> <p>Returns:</p> Type Description <code>LDLinearOperator</code> <p>The LD matrix as a <code>scipy</code> sparse CSR matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def load(self,\n         force_reload=False,\n         return_symmetric=False,\n         dtype=None) -&gt; LDLinearOperator:\n\n    \"\"\"\n    Load the LD matrix from on-disk storage in the form of Zarr arrays to memory,\n    in the form of sparse CSR matrices.\n\n    :param force_reload: If True, it will reload the data even if it is already in memory.\n    :param return_symmetric: If True, return a full symmetric representation of the LD matrix.\n    :param dtype: The data type for the entries of the LD matrix.\n\n    !!! seealso \"See Also\"\n        * [load_data][magenpy.LDMatrix.LDMatrix.load_data]\n\n    :return: The LD matrix as a `scipy` sparse CSR matrix.\n    \"\"\"\n\n    if dtype is not None:\n        dtype = np.dtype(dtype)\n    else:\n        dtype = self.dtype\n\n    if not force_reload and self.in_memory and return_symmetric == self._cached_lop.symmetric:\n        # If the LD matrix is already in memory and the requested symmetry is the same,\n        # then we don't need to reload the matrix. Here, we only transform its entries it to\n        # conform to the requested data types of the user:\n\n        # If the requested data type differs from the stored one, we need to cast the data:\n        if dtype is not None and self._cached_lop.ld_data_type != np.dtype(dtype):\n\n            if np.issubdtype(self._cached_lop.ld_data_type, np.floating) and np.issubdtype(dtype, np.floating):\n                # The user requested casting the data to different floating point precision:\n                self._cached_lop.ld_data = self._cached_lop.ld_data.astype(dtype)\n            elif np.issubdtype(self._cached_lop.ld_data_type, np.integer) and np.issubdtype(dtype, np.integer):\n                # The user requested casting the data to different integer format:\n                self._cached_lop.ld_data = quantize(dequantize(self._cached_lop.ld_data), int_dtype=dtype)\n            elif np.issubdtype(self._cached_lop.ld_data_type, np.floating) and np.issubdtype(dtype, np.integer):\n                # The user requested quantizing the data from floats to integers:\n                self._cached_lop.ld_data = quantize(self._cached_lop.ld_data, int_dtype=dtype)\n            else:\n                # The user requested dequantizing the data from integers to floats:\n                self._cached_lop.ld_data = dequantize(self._cached_lop.ld_data, float_dtype=dtype)\n\n    else:\n        # If we are re-loading the matrix, make sure to release the current one:\n        self.release()\n\n        self._cached_lop = self.load_data(return_symmetric=return_symmetric,\n                                          dtype=dtype)\n\n    return self._cached_lop\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.load_data","title":"<code>load_data(start_row=None, end_row=None, dtype=None, return_square=True, keep_original_shape=False, return_symmetric=False, return_as_csr=False)</code>","text":"<p>A utility function to load and process the LD matrix data. This function is particularly useful for filtering, symmetrizing, and dequantizing the LD matrix after it's loaded to memory.</p> <p>.. note ::     Start and end row positions are with reference to the stored on-disk LD matrix. This means     that the mask is not considered when defining the boundaries     based on the start and end row positions.</p> <p>Parameters:</p> Name Type Description Default <code>start_row</code> <p>The start row to load to memory (if loading a subset of the matrix).</p> <code>None</code> <code>end_row</code> <p>The end row (not inclusive) to load to memory (if loading a subset of the matrix).</p> <code>None</code> <code>dtype</code> <p>The data type for the entries of the LD matrix.</p> <code>None</code> <code>return_square</code> <p>If True, return a square representation of the LD matrix. This flag is used in conjunction with the <code>start_row</code> and <code>end_row</code> parameters. In particular, if <code>end_row</code> is less than the number of variants and <code>return_square=False</code>, then we return a rectangular slice of the LD matrix corresponding to the rows requested by the user.</p> <code>True</code> <code>keep_original_shape</code> <p>If True, keep the original shape of the LD matrix. This is useful when returning a subset of the matrix, but keeping the original shape.</p> <code>False</code> <code>return_symmetric</code> <p>If True, return a full symmetric representation of the LD matrix.</p> <code>False</code> <code>return_as_csr</code> <p>If True, return the data in the CSR format.</p> <code>False</code> <p>Returns:</p> Type Description <p>An LDLinearOperator object containing the LD data or a scipy CSR matrix (if <code>return_as_csr</code> is set to <code>True</code>.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def load_data(self,\n              start_row=None,\n              end_row=None,\n              dtype=None,\n              return_square=True,\n              keep_original_shape=False,\n              return_symmetric=False,\n              return_as_csr=False):\n    \"\"\"\n    A utility function to load and process the LD matrix data.\n    This function is particularly useful for filtering, symmetrizing, and dequantizing the LD matrix\n    after it's loaded to memory.\n\n    .. note ::\n        Start and end row positions are with reference to the stored on-disk LD matrix. This means\n        that the mask is not considered when defining the boundaries\n        based on the start and end row positions.\n\n    :param start_row: The start row to load to memory (if loading a subset of the matrix).\n    :param end_row: The end row (not inclusive) to load to memory (if loading a subset of the matrix).\n    :param dtype: The data type for the entries of the LD matrix.\n    :param return_square: If True, return a square representation of the LD matrix. This flag is used in\n    conjunction with the `start_row` and `end_row` parameters. In particular, if `end_row` is less than the\n    number of variants and `return_square=False`, then we return a rectangular slice of the LD matrix\n    corresponding to the rows requested by the user.\n    :param keep_original_shape: If True, keep the original shape of the LD matrix. This is useful when\n    returning a subset of the matrix, but keeping the original shape.\n    :param return_symmetric: If True, return a full symmetric representation of the LD matrix.\n    :param return_as_csr: If True, return the data in the CSR format.\n\n    :return: An LDLinearOperator object containing the LD data or a scipy CSR matrix (if\n    `return_as_csr` is set to `True`.\n    \"\"\"\n\n    # Sanity checking:\n\n    if start_row is not None:\n        assert 0. &lt;= start_row &lt; self.stored_n_snps\n    if end_row is not None:\n        assert 0. &lt; end_row &lt;= self.stored_n_snps\n\n    if keep_original_shape:\n        assert return_as_csr, \"If keeping the original shape, the data must be returned as a CSR matrix.\"\n\n    # -------------- Step 1: Preparing input data type --------------\n    if dtype is None:\n        dtype = self.stored_dtype\n        dequantize_data = False\n    else:\n        dtype = np.dtype(dtype)\n        if np.issubdtype(self.stored_dtype, np.integer) and np.issubdtype(dtype, np.floating):\n            dequantize_data = True\n        else:\n            dequantize_data = False\n\n    # -------------- Step 2: Pre-process data boundaries (if provided) --------------\n    n_snps = self.stored_n_snps\n\n    start_row = start_row or 0\n    end_row = end_row or n_snps\n\n    end_row = min(end_row, n_snps)\n\n    # -------------- Step 2: Fetch the indptr array --------------\n\n    # Get the index pointer array:\n    indptr = self._zg['matrix/indptr'][start_row:end_row + 1]\n\n    # Determine the start and end positions in the data matrix\n    # based on the requested start and end rows:\n    data_start = indptr[0]\n    data_end = indptr[-1]\n\n    # If the user is requesting a subset of the matrix, then we need to adjust\n    # the index pointer accordingly:\n    if start_row &gt; 0:\n        # Zero out all index pointers before `start_row`:\n        indptr = np.clip(indptr - data_start, a_min=0, a_max=None)\n\n    # -------------- Step 3: Loading and filtering data array --------------\n\n    data = self._zg['matrix/data'][data_start:data_end]\n\n    # Filter the data and index pointer arrays based on the mask (if set):\n    if self.is_mask_set or (end_row &lt; n_snps and return_square):\n\n        mask = np.zeros(n_snps, dtype=np.int8)\n\n        # Two cases to consider:\n\n        # 1) If the mask is not set:\n        if not self.is_mask_set:\n\n            # If the returned matrix should be square:\n            if return_square:\n                mask[start_row:end_row] = 1\n            else:\n                mask[start_row:] = 1\n\n            new_nrows = end_row - start_row\n        else:\n            # If the mask is set:\n\n            mask[self._mask] = 1\n\n            # If return square, ensure that elements after end row are set to 0 in the mask:\n            if return_square:\n                mask[end_row:] = 0\n\n            # Compute new size:\n            new_nrows = mask[start_row:end_row].sum()\n\n        from .stats.ld.c_utils import filter_ut_csr_matrix_inplace\n\n        data, indptr = filter_ut_csr_matrix_inplace(indptr, data, mask[start_row:], new_nrows)\n\n    # -------------- Step 4: Symmetrizing input matrix --------------\n\n    if return_symmetric:\n\n        from .stats.ld.c_utils import symmetrize_ut_csr_matrix\n\n        if np.issubdtype(self.stored_dtype, np.integer):\n            fill_val = np.iinfo(self.stored_dtype).max\n        else:\n            fill_val = 1.\n\n        data, indptr, leftmost_idx = symmetrize_ut_csr_matrix(indptr, data, fill_val)\n    else:\n        leftmost_idx = np.arange(1, indptr.shape[0], dtype=np.int32)\n\n    # -------------- Step 5: Dequantizing/type cast requested data --------------\n\n    if dequantize_data:\n        data = dequantize(data, float_dtype=dtype)\n    else:\n        data = data.astype(dtype, copy=False)\n\n    # ---------------------------------------------------------------------------\n    # Determine the shape of the data matrix:\n\n    if keep_original_shape:\n        shape = (n_snps, n_snps)\n    elif return_square or end_row == n_snps:\n        shape = (indptr.shape[0] - 1, indptr.shape[0] - 1)\n    else:\n        shape = (indptr.shape[0] - 1, n_snps)\n\n    # ---------------------------------------------------------------------------\n    # Return the requested data:\n\n    if return_as_csr:\n        # If the user requested the data as CSR matrix:\n\n        from .stats.ld.c_utils import expand_ranges\n        from scipy.sparse import csr_matrix\n\n        indices = expand_ranges(leftmost_idx,\n                                (np.diff(indptr) + leftmost_idx).astype(np.int32),\n                                data.shape[0])\n\n        if keep_original_shape:\n            # TODO: Consider incorporating this in `LDLinearOperator.to_csr`\n            indices += start_row\n            indptr = np.concatenate([np.zeros(start_row, dtype=indptr.dtype),\n                                     indptr,\n                                     np.ones(n_snps - end_row, dtype=indptr.dtype) * indptr[-1]])\n\n        return csr_matrix(\n            (\n                data,\n                indices,\n                indptr\n            ),\n            shape=shape,\n            dtype=dtype\n        )\n\n    else:\n        # Otherwise, return as a linear operator:\n\n        return LDLinearOperator(\n            indptr,\n            data,\n            leftmost_idx,\n            symmetric=return_symmetric,\n            shape=shape\n        )\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.multiply","title":"<code>multiply(vec)</code>","text":"<p>Multiply the LD matrix with an input vector <code>vec</code>.</p> <p>Parameters:</p> Name Type Description Default <code>vec</code> <p>The input vector to multiply with the LD matrix.  !!! seealso \"See Also\" * dot</p> required <p>Returns:</p> Type Description <p>The product of the LD matrix with the input vector.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def multiply(self, vec):\n    \"\"\"\n    Multiply the LD matrix with an input vector `vec`.\n\n    :param vec: The input vector to multiply with the LD matrix.\n\n    !!! seealso \"See Also\"\n        * [dot][magenpy.LDMatrix.LDMatrix.dot]\n\n    :return: The product of the LD matrix with the input vector.\n    \"\"\"\n\n    if self.in_memory:\n        return self._cached_lop.dot(vec)\n    else:\n        ld_opr = self.to_linear_operator()\n        return ld_opr.dot(vec)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.perform_svd","title":"<code>perform_svd(**svds_kwargs)</code>","text":"<p>Perform the Singular Value Decomposition (SVD) on the LD matrix. This method is a wrapper around the <code>scipy.sparse.linalg.svds</code> function and provides utilities to perform SVD with a LinearOperator for large-scale LD matrix, so that the matrices don't need to be fully represented in memory.</p> <p>Parameters:</p> Name Type Description Default <code>svds_kwargs</code> <p>Additional keyword arguments to pass to the <code>scipy.sparse.linalg.svds</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of the SVD decomposition of the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def perform_svd(self, **svds_kwargs):\n    \"\"\"\n    Perform the Singular Value Decomposition (SVD) on the LD matrix.\n    This method is a wrapper around the `scipy.sparse.linalg.svds` function and provides\n    utilities to perform SVD with a LinearOperator for large-scale LD matrix, so that\n    the matrices don't need to be fully represented in memory.\n\n    :param svds_kwargs: Additional keyword arguments to pass to the `scipy.sparse.linalg.svds` function.\n\n    :return: The result of the SVD decomposition of the LD matrix.\n    \"\"\"\n\n    from scipy.sparse.linalg import svds\n\n    if self.in_memory:\n        mat = self._cached_lop\n    else:\n        mat = self.to_linear_operator()\n\n    return svds(mat, **svds_kwargs)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.prune","title":"<code>prune(threshold)</code>","text":"<p>Perform LD pruning to remove variants that are in high LD with other variants. If two variants are in high LD, this function keeps the variant that occurs earlier in the matrix. This behavior will be updated in the future to allow for arbitrary ordering of variants.</p> <p>Note</p> <p>Experimental for now. Needs further testing &amp; improvement.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <p>The absolute value of the Pearson correlation coefficient above which to prune variants. A positive floating point number between 0. and 1.</p> required <p>Returns:</p> Type Description <p>A boolean array indicating whether a variant is kept after pruning.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def prune(self, threshold):\n    \"\"\"\n    Perform LD pruning to remove variants that are in high LD with other variants.\n    If two variants are in high LD, this function keeps the variant that occurs\n    earlier in the matrix. This behavior will be updated in the future to allow\n    for arbitrary ordering of variants.\n\n    !!! note\n        Experimental for now. Needs further testing &amp; improvement.\n\n    :param threshold: The absolute value of the Pearson correlation coefficient above which to prune variants. A\n    positive floating point number between 0. and 1.\n    :return: A boolean array indicating whether a variant is kept after pruning.\n    \"\"\"\n\n    from .stats.ld.c_utils import prune_ld_ut\n\n    assert 0. &lt; threshold &lt;= 1.\n\n    if np.issubdtype(self.dtype, np.integer):\n        threshold = quantize(np.array([threshold]), int_dtype=self.dtype)[0]\n\n    return prune_ld_ut(self.indptr[:], self.data[:], threshold)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.release","title":"<code>release()</code>","text":"<p>Release the LD data and associated arrays from memory.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def release(self):\n    \"\"\"\n    Release the LD data and associated arrays from memory.\n    \"\"\"\n    self._cached_lop = None\n    self.index = 0\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.reset_mask","title":"<code>reset_mask()</code>","text":"<p>Reset the mask to its default value (None).</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def reset_mask(self):\n    \"\"\"\n    Reset the mask to its default value (None).\n    \"\"\"\n\n    self._mask = None\n    self._n_masked = 0\n\n    if self.in_memory:\n        self.load(force_reload=True,\n                  return_symmetric=self.is_symmetric,\n                  dtype=self.dtype)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.set_mask","title":"<code>set_mask(mask)</code>","text":"<p>Set the mask (a boolean array) to hide/remove some SNPs from the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>An array of indices or boolean mask for SNPs to retain.</p> required Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def set_mask(self, mask):\n    \"\"\"\n    Set the mask (a boolean array) to hide/remove some SNPs from the LD matrix.\n    :param mask: An array of indices or boolean mask for SNPs to retain.\n    \"\"\"\n\n    # Check that mask is a numpy array:\n    if not isinstance(mask, np.ndarray):\n        raise ValueError(\"Mask must be a numpy array.\")\n\n    # Check that mask is either a boolean array or an array of indices:\n    if mask.dtype != bool and not np.issubdtype(mask.dtype, np.integer):\n        raise ValueError(\"Mask must be a boolean array or an array of indices.\")\n\n    # If mask is a boolean array, ensure that it matches the number of stored SNPs:\n    if mask.dtype == bool and len(mask) != self.stored_n_snps:\n        raise ValueError(\"Boolean mask must have the same length as the number of stored SNPs.\")\n\n    # If the mask is equivalent to the current mask, return:\n    if np.array_equal(mask, self._mask):\n        return\n\n    # If the mask consists of indices, convert to boolean mask:\n    if mask.dtype != bool:\n        self._mask = np.zeros(self.stored_n_snps, dtype=bool)\n        self._mask[mask] = True\n    else:\n        self._mask = mask\n\n    self._n_masked = np.sum(~self._mask)\n\n    # If the data has already been loaded to memory, reload:\n    if self.in_memory:\n        self.load(force_reload=True,\n                  return_symmetric=self.is_symmetric,\n                  dtype=self.dtype)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.set_metadata","title":"<code>set_metadata(key, value, overwrite=False)</code>","text":"<p>Set the metadata field associated with variants in the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The key for the metadata item.</p> required <code>value</code> <p>The value for the metadata item (an array with the same length as the number of variants).</p> required <code>overwrite</code> <p>If True, overwrite the metadata item if it already exists.</p> <code>False</code> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def set_metadata(self, key, value, overwrite=False):\n    \"\"\"\n    Set the metadata field associated with variants in the LD matrix.\n    :param key: The key for the metadata item.\n    :param value: The value for the metadata item (an array with the same length as the number of variants).\n    :param overwrite: If True, overwrite the metadata item if it already exists.\n    \"\"\"\n\n    if 'metadata' not in list(self._zg.group_keys()):\n        meta = self._zg.create_group('metadata')\n    else:\n        meta = self._zg['metadata']\n\n    value = np.array(value)\n\n    if np.issubdtype(value.dtype, np.floating):\n        dtype = np.float32\n    elif np.issubdtype(value.dtype, np.integer):\n        dtype = np.int32\n    else:\n        dtype = str\n\n    meta.array(key,\n               value,\n               overwrite=overwrite,\n               dtype=dtype,\n               compressor=self.compressor)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.set_store_attr","title":"<code>set_store_attr(attr, value)</code>","text":"<p>Set the attribute <code>attr</code> associated with the LD matrix. This is used to set high-level information, such as information about the sample from which the matrix was computed, the LD estimator used, its properties, etc.</p> <p>Parameters:</p> Name Type Description Default <code>attr</code> <p>The attribute name.</p> required <code>value</code> <p>The value for the attribute.</p> required Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def set_store_attr(self, attr, value):\n    \"\"\"\n    Set the attribute `attr` associated with the LD matrix. This is used\n    to set high-level information, such as information about the sample from which\n    the matrix was computed, the LD estimator used, its properties, etc.\n\n    :param attr: The attribute name.\n    :param value: The value for the attribute.\n    \"\"\"\n\n    self._zg.attrs[attr] = value\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.summary","title":"<code>summary()</code>","text":"<p>Returns:</p> Type Description <p>A <code>pandas</code> dataframe with summary of the main attributes of the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def summary(self):\n    \"\"\"\n    :return: A `pandas` dataframe with summary of the main attributes of the LD matrix.\n    \"\"\"\n\n    return pd.DataFrame([\n        {'LD Matrix property': 'Chromosome', 'Value': self.chromosome},\n        {'LD Matrix property': 'Stored shape', 'Value': self.stored_shape},\n        {'LD Matrix property': 'Stored data type', 'Value': self.stored_dtype},\n        {'LD Matrix property': 'Stored entries', 'Value': self._zg['matrix/data'].shape[0]},\n        {'LD Matrix property': 'Path', 'Value': self.store.path},\n        {'LD Matrix property': 'In memory?', 'Value': self.in_memory},\n        {'LD Matrix property': 'Mask set?', 'Value': self.is_mask_set},\n        {'LD Matrix property': 'On-disk storage', 'Value': f'{self.get_total_storage_size():.3} MB'},\n        {'LD Matrix property': 'Estimated uncompressed size', 'Value': f'{self.estimate_uncompressed_size():.3} MB'}\n    ]).set_index('LD Matrix property')\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.to_csr","title":"<code>to_csr(**load_kwargs)</code>","text":"<p>Get the LD data as a <code>scipy.csr_matrix</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>load_kwargs</code> <p>Additional keyword arguments to pass to the <code>load_data</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A <code>scipy.csr_matrix</code> object representing the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def to_csr(self, **load_kwargs):\n    \"\"\"\n    Get the LD data as a `scipy.csr_matrix` object.\n\n    :param load_kwargs: Additional keyword arguments to pass to the `load_data` method.\n    :return: A `scipy.csr_matrix` object representing the LD matrix.\n    \"\"\"\n\n    if self.in_memory:\n        return self._cached_lop.to_csr()\n    else:\n        return self.load_data(return_as_csr=True, **load_kwargs)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.to_linear_operator","title":"<code>to_linear_operator(**load_kwargs)</code>","text":"<p>Get the LD data as a <code>LDLinearOperator</code> object. This is useful for performing linear algebra operations on the LD matrix efficiently.</p> <p>See Also</p> <ul> <li>[LDLinearOperator][magenpy.LDMatrix.LDLinearOperator]</li> </ul> <p>Parameters:</p> Name Type Description Default <code>load_kwargs</code> <p>Additional keyword arguments to pass to the <code>load_data</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>An <code>LDLinearOperator</code> object containing the LD data.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def to_linear_operator(self, **load_kwargs):\n    \"\"\"\n    Get the LD data as a `LDLinearOperator` object. This is useful for performing\n    linear algebra operations on the LD matrix efficiently.\n\n    !!! seealso \"See Also\"\n    * [LDLinearOperator][magenpy.LDMatrix.LDLinearOperator]\n\n    :param load_kwargs: Additional keyword arguments to pass to the `load_data` method.\n\n    :return: An `LDLinearOperator` object containing the LD data.\n    \"\"\"\n\n    if self.in_memory:\n        return self._cached_lop\n    else:\n        return self.load_data(**load_kwargs)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.to_snp_table","title":"<code>to_snp_table(col_subset=None, use_original_index=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>The subset of columns to add to the table. If None, it returns all available columns.</p> <code>None</code> <code>use_original_index</code> <p>If True, it uses the original index of the SNPs in the LD matrix ( before applying any filters).</p> <code>False</code> <p>Returns:</p> Type Description <p>A <code>pandas</code> dataframe of the SNP attributes and metadata for variants included in the LD matrix.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def to_snp_table(self, col_subset=None, use_original_index=False):\n    \"\"\"\n    :param col_subset: The subset of columns to add to the table. If None, it returns\n    all available columns.\n    :param use_original_index: If True, it uses the original index of the SNPs in the LD matrix (\n    before applying any filters).\n\n    :return: A `pandas` dataframe of the SNP attributes and metadata for variants\n    included in the LD matrix.\n    \"\"\"\n\n    col_subset = col_subset or ['CHR', 'SNP', 'POS', 'A1', 'A2', 'MAF', 'LDScore']\n\n    # Create the index according to the original SNP order:\n    if use_original_index:\n        original_index = np.arange(self.stored_n_snps)\n        if self._mask is not None:\n            original_index = original_index[self._mask]\n    else:\n        original_index = None\n\n    table = pd.DataFrame({'SNP': self.snps}, index=original_index)\n\n    for col in col_subset:\n        if col == 'CHR':\n            table['CHR'] = self.chromosome\n        if col == 'POS':\n            table['POS'] = self.bp_position\n        if col == 'cM':\n            table['cM'] = self.cm_position\n        if col == 'A1':\n            table['A1'] = self.a1\n        if col == 'A2':\n            table['A2'] = self.a2\n        if col == 'MAF':\n            table['MAF'] = self.maf\n        if col == 'LDScore':\n            table['LDScore'] = self.ld_score\n        if col == 'WindowSize':\n            table['WindowSize'] = self.window_size\n\n    return table[list(col_subset)]\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.update_rows_inplace","title":"<code>update_rows_inplace(new_csr, start_row=None, end_row=None)</code>","text":"<p>A utility function to perform partial updates to a subset of rows in the LD matrix. The function takes a new CSR matrix and, optionally, a start and end row delimiting the chunk of the LD matrix to update with the <code>new_csr</code>.</p> <p>Note</p> <p>Current implementation assumes that the update does not change the sparsity structure of the original matrix. Updating the matrix with new sparsity structure is a harder problem that we will try to tackle later on.</p> <p>Note</p> <p>Current implementation assumes <code>new_csr</code> is upper triangular.</p> <p>Parameters:</p> Name Type Description Default <code>new_csr</code> <p>A sparse CSR matrix (<code>scipy.sparse.csr_matrix</code>) where the column dimension matches the column dimension of the LD matrix.</p> required <code>start_row</code> <p>The start row for the chunk to update.</p> <code>None</code> <code>end_row</code> <p>The end row for the chunk to update.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the column dimension of <code>new_csr</code> does not match the column dimension</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def update_rows_inplace(self, new_csr, start_row=None, end_row=None):\n    \"\"\"\n    A utility function to perform partial updates to a subset of rows in the\n    LD matrix. The function takes a new CSR matrix and, optionally, a start\n    and end row delimiting the chunk of the LD matrix to update with the `new_csr`.\n\n    !!! note\n        Current implementation assumes that the update does not change the sparsity\n        structure of the original matrix. Updating the matrix with new sparsity structure\n        is a harder problem that we will try to tackle later on.\n\n    !!! note\n        Current implementation assumes `new_csr` is upper triangular.\n\n    :param new_csr: A sparse CSR matrix (`scipy.sparse.csr_matrix`) where the column dimension\n    matches the column dimension of the LD matrix.\n    :param start_row: The start row for the chunk to update.\n    :param end_row: The end row for the chunk to update.\n\n    :raises AssertionError: if the column dimension of `new_csr` does not match the column dimension\n    \"\"\"\n\n    assert new_csr.shape[1] == self.stored_n_snps\n\n    start_row = start_row or 0\n    end_row = end_row or self.stored_n_snps\n\n    # Sanity checking:\n    assert start_row &gt;= 0\n    assert end_row &lt;= self.stored_n_snps\n\n    indptr = self._zg['matrix/indptr'][:]\n\n    data_start = indptr[start_row]\n    data_end = indptr[end_row]\n\n    # TODO: Check that this covers most cases and would not result in unexpected behavior\n    if np.issubdtype(self.stored_dtype, np.integer) and np.issubdtype(new_csr.dtype, np.floating):\n        self._zg['matrix/data'][data_start:data_end] = quantize(new_csr.data, int_dtype=self.stored_dtype)\n    else:\n        self._zg['matrix/data'][data_start:data_end] = new_csr.data.astype(self.stored_dtype, copy=False)\n</code></pre>"},{"location":"api/LDMatrix/#magenpy.LDMatrix.LDMatrix.validate_ld_matrix","title":"<code>validate_ld_matrix()</code>","text":"<p>Checks that the <code>LDMatrix</code> object has correct structure and checks its contents for validity.</p> <p>Specifically, we check that: * The dimensions of the matrix and its associated attributes are matching. * The masking is working properly. * Index pointer is valid and its contents make sense.</p> <p>Returns:</p> Type Description <p>True if the matrix has the correct structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the matrix or some of its entries are not valid.</p> Source code in <code>magenpy/LDMatrix.py</code> <pre><code>def validate_ld_matrix(self):\n    \"\"\"\n    Checks that the `LDMatrix` object has correct structure and\n    checks its contents for validity.\n\n    Specifically, we check that:\n    * The dimensions of the matrix and its associated attributes are matching.\n    * The masking is working properly.\n    * Index pointer is valid and its contents make sense.\n\n    :return: True if the matrix has the correct structure.\n    :raises ValueError: If the matrix or some of its entries are not valid.\n    \"\"\"\n\n    class_attrs = ['snps', 'a1', 'a2', 'maf', 'bp_position', 'cm_position', 'ld_score']\n\n    for attr in class_attrs:\n        attribute = getattr(self, attr)\n        if attribute is None:\n            continue\n        if len(attribute) != len(self):\n            raise ValueError(f\"Invalid LD Matrix: Dimensions for attribute {attr} are not aligned!\")\n\n    # -------------------- Index pointer checks --------------------\n    # Check that the entries of the index pointer are all positive or zero:\n    indptr = self.indptr[:]\n\n    if indptr.min() &lt; 0:\n        raise ValueError(\"The index pointer contains negative entries!\")\n\n    # Check that the entries don't decrease:\n    indptr_diff = np.diff(indptr)\n    if indptr_diff.min() &lt; 0:\n        raise ValueError(\"The index pointer entries are not increasing!\")\n\n    # Check that the last entry of the index pointer matches the shape of the data:\n    if indptr[-1] != self.data.shape[0]:\n        raise ValueError(\"The last entry of the index pointer \"\n                         \"does not match the shape of the data!\")\n\n    # TODO: Add other sanity checks here?\n\n    return True\n</code></pre>"},{"location":"api/SampleTable/","title":"SampleTable","text":"<p>               Bases: <code>object</code></p> <p>A class to represent sample (individual) information and attributes in the context of a genotype matrix. The sample table is a wrapper around a <code>pandas.DataFrame</code> object that contains the sample information. The table provides methods to read and write sample information from/to disk, filter samples, perform checks/validation, and extract specific columns from the table.</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>Union[DataFrame, None]</code> <p>The sample table as a pandas <code>DataFrame</code>.</p> <code>_phenotype_likelihood</code> <code>Union[str, None]</code> <p>The likelihood of the phenotype values (if present).</p> <code>_covariate_cols</code> <p>The names or IDs of covariates that are present in the sample table.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>class SampleTable(object):\n    \"\"\"\n    A class to represent sample (individual) information and attributes in\n    the context of a genotype matrix. The sample table is a wrapper around\n    a `pandas.DataFrame` object that contains the sample information. The\n    table provides methods to read and write sample information from/to\n    disk, filter samples, perform checks/validation, and extract specific columns\n    from the table.\n\n    :ivar table: The sample table as a pandas `DataFrame`.\n    :ivar _phenotype_likelihood: The likelihood of the phenotype values (if present).\n    :ivar _covariate_cols: The names or IDs of covariates that are present in the sample table.\n\n    \"\"\"\n\n    def __init__(self,\n                 table: Union[pd.DataFrame, None] = None,\n                 phenotype_likelihood: Union[str, None] = None):\n        \"\"\"\n        Initialize the sample table object.\n        :param table: A pandas DataFrame with the sample information.\n        :param phenotype_likelihood: The likelihood of the phenotype values.\n        \"\"\"\n\n        self.table: Union[pd.DataFrame, None] = table\n\n        if self.table is not None and 'original_index' not in self.table.columns:\n            self.table['original_index'] = np.arange(len(self.table))\n\n        assert phenotype_likelihood in (None, 'binomial', 'gaussian', 'infer')\n\n        self._phenotype_likelihood: Union[str, None] = phenotype_likelihood\n        self._covariate_cols = None\n\n        if self.table is not None:\n            self.post_check_phenotype()\n\n    @property\n    def shape(self):\n        \"\"\"\n        :return: The shape of the sample table (mainly sample size) as a tuple (n,).\n        \"\"\"\n        return (self.n,)\n\n    @property\n    def n(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [sample_size][magenpy.SampleTable.SampleTable.sample_size]\n\n        :return: The sample size (number of individuals) in the sample table.\n        \"\"\"\n        return len(self.table)\n\n    @property\n    def sample_size(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n][magenpy.SampleTable.SampleTable.n]\n\n        :return: he sample size (number of individuals) in the sample table.\n        \"\"\"\n        return self.n\n\n    @property\n    def iid(self):\n        \"\"\"\n        :return: The individual ID of each individual in the sample table.\n        \"\"\"\n        if self.table is not None:\n            return self.table['IID'].values\n\n    @property\n    def fid(self):\n        \"\"\"\n        :return: The family ID of each individual in the sample table.\n        \"\"\"\n        if self.table is not None:\n            return self.table['FID'].values\n\n    @property\n    def phenotype(self):\n        \"\"\"\n        :return: The phenotype column from the sample table.\n        :raises KeyError: If the phenotype is not set.\n        \"\"\"\n        if self.table is not None:\n            try:\n                return self.table['phenotype'].values\n            except KeyError:\n                raise KeyError(\"The phenotype is not set!\")\n\n    @property\n    def original_index(self):\n        \"\"\"\n        :return: The original index of each individual in the sample table (before applying any filters).\n        \"\"\"\n        if self.table is not None:\n            return self.table['original_index'].values\n\n    @property\n    def covariates(self):\n        \"\"\"\n        :return: The column names for the covariates stored in the sample table.\n        \"\"\"\n        return self._covariate_cols\n\n    @property\n    def phenotype_likelihood(self):\n        \"\"\"\n        :return: The phenotype likelihood family.\n        \"\"\"\n        return self._phenotype_likelihood\n\n    @classmethod\n    def from_fam_file(cls, fam_file):\n        \"\"\"\n        Initialize a sample table object from a path to PLINK FAM file.\n        :param fam_file: The path to the FAM file.\n\n        :return: A `SampleTable` object.\n        \"\"\"\n\n        from .parsers.plink_parsers import parse_fam_file\n\n        s_tab = parse_fam_file(fam_file)\n        return cls(table=s_tab)\n\n    @classmethod\n    def from_phenotype_file(cls, phenotype_file, filter_na=True, **read_csv_kwargs):\n        \"\"\"\n        Initialize a sample table from a phenotype file.\n        :param phenotype_file: The path to the phenotype file.\n        :param filter_na: Filter samples with missing phenotype values (Default: True).\n        :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n\n        :return: A `SampleTable` object.\n        \"\"\"\n        s_tab = cls()\n        s_tab.read_phenotype_file(phenotype_file, filter_na, **read_csv_kwargs)\n        return s_tab\n\n    @classmethod\n    def from_covariate_file(cls, covar_file, **read_csv_kwargs):\n        \"\"\"\n        Initialize a sample table from a file of covariates.\n        :param covar_file: The path to the covariates file.\n        :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n\n        :return: A `SampleTable` object.\n        \"\"\"\n        s_tab = cls()\n        s_tab.read_covariates_file(covar_file, **read_csv_kwargs)\n        return s_tab\n\n    def read_phenotype_file(self, phenotype_file, drop_na=True, **read_csv_kwargs):\n        \"\"\"\n        Read the phenotype file from disk. The expected format is Family ID (`FID`),\n        Individual ID (`IID`) and the phenotype column `phenotype`. You may adjust\n        the parsing configurations with keyword arguments that will be passed to `pandas.read_csv`.\n\n        !!! warning \"Warning\"\n            If a phenotype column is already present in the sample table, it will be replaced.\n            The data structure currently does not support multiple phenotype columns.\n\n        :param phenotype_file: The path to the phenotype file.\n        :param drop_na: Drop samples whose phenotype value is missing (Default: True).\n        :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n        \"\"\"\n\n        if 'sep' not in read_csv_kwargs and 'delimiter' not in read_csv_kwargs:\n            read_csv_kwargs['sep'] = r'\\s+'\n\n        if 'na_values' not in read_csv_kwargs:\n            read_csv_kwargs['na_values'] = {'phenotype': [-9.]}\n\n        if 'dtype' not in read_csv_kwargs:\n            read_csv_kwargs['dtype'] = {'phenotype': float}\n\n        from .utils.compute_utils import detect_header_keywords\n\n        if all([col not in read_csv_kwargs for col in ('header', 'names')]):\n\n            if detect_header_keywords(phenotype_file, ['FID', 'IID']):\n                read_csv_kwargs['header'] = 0\n            else:\n                read_csv_kwargs['names'] = ['FID', 'IID', 'phenotype']\n\n        pheno_table = pd.read_csv(phenotype_file, **read_csv_kwargs)\n\n        if self.table is not None:\n            pheno_table['FID'] = pheno_table['FID'].astype(type(self.fid[0]))\n            pheno_table['IID'] = pheno_table['IID'].astype(type(self.iid[0]))\n\n            # Drop the phenotype column if it already exists:\n            if 'phenotype' in self.table.columns:\n                self.table.drop(columns=['phenotype'], inplace=True)\n\n            self.table = self.table.merge(pheno_table, on=['FID', 'IID'])\n        else:\n            self.table = pheno_table\n\n        if self.table['phenotype'].isnull().all():\n            self.table.drop('phenotype', axis=1, inplace=True)\n        elif drop_na:\n            # Maybe using converters in the read_csv above?\n            self.table = self.table.dropna(subset=['phenotype'])\n\n        self.post_check_phenotype()\n\n    def read_covariates_file(self, covar_file, **read_csv_kwargs):\n        \"\"\"\n        Read the covariates file from the provided path. The expected format is Family ID (`FID`),\n        Individual ID (`IID`) and the remaining columns are assumed to be covariates. You may adjust\n        the parsing configurations with keyword arguments that will be passed to `pandas.read_csv`.\n\n        !!! warning \"Warning\"\n            If covariate columns are already present in the sample table, they will be replaced.\n            The data structure currently does not support reading separate covariates files.\n\n        :param covar_file: The path to the covariates file.\n        :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n        \"\"\"\n\n        if 'sep' not in read_csv_kwargs and 'delimiter' not in read_csv_kwargs:\n            read_csv_kwargs['sep'] = r'\\s+'\n\n        from .utils.compute_utils import detect_header_keywords\n\n        if 'header' not in read_csv_kwargs:\n\n            if detect_header_keywords(covar_file, ['FID', 'IID']):\n                read_csv_kwargs['header'] = 0\n            else:\n                read_csv_kwargs['header'] = None\n\n        covar_table = pd.read_csv(covar_file, **read_csv_kwargs)\n\n        if self._covariate_cols is not None:\n            self.table.drop(columns=self._covariate_cols, inplace=True)\n\n        if read_csv_kwargs['header'] is None:\n            self._covariate_cols = np.array([f'covar_{i + 1}' for i in range(covar_table.shape[1] - 2)])\n        else:\n            self._covariate_cols = covar_table.columns[2:]\n\n        covar_table.columns = ['FID', 'IID'] + list(self._covariate_cols)\n\n        if self.table is not None:\n            covar_table['FID'] = covar_table['FID'].astype(type(self.fid[0]))\n            covar_table['IID'] = covar_table['IID'].astype(type(self.iid[0]))\n\n            self.table = self.table.merge(covar_table, on=['FID', 'IID'])\n        else:\n            self.table = covar_table\n\n    def post_check_phenotype(self):\n        \"\"\"\n        Apply some simple heuristics to check the phenotype values\n        provided by the user and infer the phenotype likelihood (if feasible).\n\n        :raises ValueError: If the phenotype values could not be matched with the\n        inferred phenotype likelihood.\n        \"\"\"\n\n        if 'phenotype' in self.table.columns:\n\n            unique_vals = self.table['phenotype'].unique()\n\n            if self.table['phenotype'].isnull().all():\n                self.table.drop('phenotype', axis=1, inplace=True)\n            elif self._phenotype_likelihood != 'gaussian':\n\n                if len(unique_vals) &gt; 2:\n                    self._phenotype_likelihood = 'gaussian'\n                    return\n\n                unique_vals = sorted(unique_vals)\n\n                if unique_vals == [1, 2]:\n                    # Plink coding for case/control\n                    self.table['phenotype'] -= 1\n                    self._phenotype_likelihood = 'binomial'\n                elif unique_vals == [0, 1]:\n                    self._phenotype_likelihood = 'binomial'\n                else:\n                    raise ValueError(f\"Unknown values for binary traits: {unique_vals}. \"\n                                     f\"The software only supports 0/1 or 1/2 coding for cases and controls.\")\n\n    def filter_samples(self, keep_samples=None, keep_file=None):\n        \"\"\"\n        Filter samples from the samples table. User must specify\n        either a list of samples to keep or the path to a file\n        with the list of samples to keep.\n\n        :param keep_samples: A list (or array) of sample IDs to keep.\n        :param keep_file: The path to a file with the list of samples to keep.\n        \"\"\"\n\n        assert keep_samples is not None or keep_file is not None\n\n        if keep_samples is None:\n            from .parsers.misc_parsers import read_sample_filter_file\n            keep_samples = read_sample_filter_file(keep_file)\n\n        self.table = self.table.merge(pd.DataFrame({'IID': keep_samples},\n                                                   dtype=type(self.iid[0])))\n\n    def to_table(self, col_subset=None):\n        \"\"\"\n        Get the sample table as a pandas DataFrame.\n\n        :param col_subset: A subset of the columns to include in the table.\n        :return: A pandas DataFrame with the sample information.\n        \"\"\"\n        if col_subset is not None:\n            return self.table[list(col_subset)]\n        else:\n            return self.table\n\n    def get_individual_table(self):\n        \"\"\"\n        :return: A table of individual IDs (FID, IID) present in the sample table.\n        \"\"\"\n        return self.to_table(col_subset=['FID', 'IID'])\n\n    def get_phenotype_table(self):\n        \"\"\"\n        :return: A table of individual IDs and phenotype values (FID IID phenotype) in the sample table.\n        \"\"\"\n        try:\n            return self.to_table(col_subset=['FID', 'IID', 'phenotype'])\n        except KeyError:\n            raise KeyError(\"The phenotype is not set!\")\n\n    def get_covariates_table(self, covar_subset=None):\n        \"\"\"\n        Get a table of covariates associated with each individual in the\n        sample table. The table will be formatted as (FID, IID, covar1, covar2, ...).\n\n        :param covar_subset: A subset of the covariate names or IDs to include in the table.\n        :return: A pandas DataFrame with the covariate information.\n        \"\"\"\n        assert self._covariate_cols is not None\n\n        if covar_subset is None:\n            covar = list(self._covariate_cols)\n        else:\n            covar = list(set(self._covariate_cols).intersection(set(covar_subset)))\n\n        assert len(covar) &gt;= 1\n\n        return self.to_table(col_subset=['FID', 'IID'] + covar)\n\n    def get_covariates_matrix(self, covar_subset=None):\n        \"\"\"\n        Get the covariates associated with each individual in the sample table as a matrix.\n        :param covar_subset: A subset of the covariate names or IDs to include in the matrix.\n\n        :return: A numpy matrix with the covariates values for each individual.\n        \"\"\"\n        return self.get_covariates_table(covar_subset=covar_subset).drop(['FID', 'IID'], axis=1).values\n\n    def set_phenotype(self, phenotype, phenotype_likelihood=None):\n        \"\"\"\n        Update the phenotype in the sample table using the provided values.\n        :param phenotype: The new phenotype values, represented by a numpy array or Iterable.\n        :param phenotype_likelihood: The likelihood of the phenotype values.\n        \"\"\"\n\n        self.table['phenotype'] = phenotype\n\n        if phenotype_likelihood is not None:\n            self._phenotype_likelihood = phenotype_likelihood\n        else:\n            self.post_check_phenotype()\n\n    def to_file(self, output_file, col_subset=None, **to_csv_kwargs):\n        \"\"\"\n        Write the contents of the sample table to file.\n        :param output_file: The path to the file where to write the sample table.\n        :param col_subset: A subset of the columns to write to file.\n        :param to_csv_kwargs: keyword arguments to pass to the `to_csv` function of `pandas`.\n        \"\"\"\n\n        assert self.table is not None\n\n        if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n            to_csv_kwargs['sep'] = '\\t'\n\n        if 'index' not in to_csv_kwargs:\n            to_csv_kwargs['index'] = False\n\n        if col_subset is not None:\n            table = self.table[col_subset]\n        else:\n            table = self.table\n\n        table.to_csv(output_file, **to_csv_kwargs)\n\n    def __len__(self):\n        return self.n\n\n    def __eq__(self, other):\n        return np.array_equal(self.iid, other.iid)\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.covariates","title":"<code>covariates</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The column names for the covariates stored in the sample table.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.fid","title":"<code>fid</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The family ID of each individual in the sample table.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.iid","title":"<code>iid</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The individual ID of each individual in the sample table.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.n","title":"<code>n</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>sample_size</li> </ul> <p>Returns:</p> Type Description <p>The sample size (number of individuals) in the sample table.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.original_index","title":"<code>original_index</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The original index of each individual in the sample table (before applying any filters).</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.phenotype","title":"<code>phenotype</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The phenotype column from the sample table.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the phenotype is not set.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.phenotype_likelihood","title":"<code>phenotype_likelihood</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The phenotype likelihood family.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.sample_size","title":"<code>sample_size</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n</li> </ul> <p>Returns:</p> Type Description <p>he sample size (number of individuals) in the sample table.</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The shape of the sample table (mainly sample size) as a tuple (n,).</p>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.__init__","title":"<code>__init__(table=None, phenotype_likelihood=None)</code>","text":"<p>Initialize the sample table object.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Union[DataFrame, None]</code> <p>A pandas DataFrame with the sample information.</p> <code>None</code> <code>phenotype_likelihood</code> <code>Union[str, None]</code> <p>The likelihood of the phenotype values.</p> <code>None</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def __init__(self,\n             table: Union[pd.DataFrame, None] = None,\n             phenotype_likelihood: Union[str, None] = None):\n    \"\"\"\n    Initialize the sample table object.\n    :param table: A pandas DataFrame with the sample information.\n    :param phenotype_likelihood: The likelihood of the phenotype values.\n    \"\"\"\n\n    self.table: Union[pd.DataFrame, None] = table\n\n    if self.table is not None and 'original_index' not in self.table.columns:\n        self.table['original_index'] = np.arange(len(self.table))\n\n    assert phenotype_likelihood in (None, 'binomial', 'gaussian', 'infer')\n\n    self._phenotype_likelihood: Union[str, None] = phenotype_likelihood\n    self._covariate_cols = None\n\n    if self.table is not None:\n        self.post_check_phenotype()\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.filter_samples","title":"<code>filter_samples(keep_samples=None, keep_file=None)</code>","text":"<p>Filter samples from the samples table. User must specify either a list of samples to keep or the path to a file with the list of samples to keep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_samples</code> <p>A list (or array) of sample IDs to keep.</p> <code>None</code> <code>keep_file</code> <p>The path to a file with the list of samples to keep.</p> <code>None</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def filter_samples(self, keep_samples=None, keep_file=None):\n    \"\"\"\n    Filter samples from the samples table. User must specify\n    either a list of samples to keep or the path to a file\n    with the list of samples to keep.\n\n    :param keep_samples: A list (or array) of sample IDs to keep.\n    :param keep_file: The path to a file with the list of samples to keep.\n    \"\"\"\n\n    assert keep_samples is not None or keep_file is not None\n\n    if keep_samples is None:\n        from .parsers.misc_parsers import read_sample_filter_file\n        keep_samples = read_sample_filter_file(keep_file)\n\n    self.table = self.table.merge(pd.DataFrame({'IID': keep_samples},\n                                               dtype=type(self.iid[0])))\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.from_covariate_file","title":"<code>from_covariate_file(covar_file, **read_csv_kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize a sample table from a file of covariates.</p> <p>Parameters:</p> Name Type Description Default <code>covar_file</code> <p>The path to the covariates file.</p> required <code>read_csv_kwargs</code> <p>keyword arguments to pass to the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A <code>SampleTable</code> object.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>@classmethod\ndef from_covariate_file(cls, covar_file, **read_csv_kwargs):\n    \"\"\"\n    Initialize a sample table from a file of covariates.\n    :param covar_file: The path to the covariates file.\n    :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n\n    :return: A `SampleTable` object.\n    \"\"\"\n    s_tab = cls()\n    s_tab.read_covariates_file(covar_file, **read_csv_kwargs)\n    return s_tab\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.from_fam_file","title":"<code>from_fam_file(fam_file)</code>  <code>classmethod</code>","text":"<p>Initialize a sample table object from a path to PLINK FAM file.</p> <p>Parameters:</p> Name Type Description Default <code>fam_file</code> <p>The path to the FAM file.</p> required <p>Returns:</p> Type Description <p>A <code>SampleTable</code> object.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>@classmethod\ndef from_fam_file(cls, fam_file):\n    \"\"\"\n    Initialize a sample table object from a path to PLINK FAM file.\n    :param fam_file: The path to the FAM file.\n\n    :return: A `SampleTable` object.\n    \"\"\"\n\n    from .parsers.plink_parsers import parse_fam_file\n\n    s_tab = parse_fam_file(fam_file)\n    return cls(table=s_tab)\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.from_phenotype_file","title":"<code>from_phenotype_file(phenotype_file, filter_na=True, **read_csv_kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize a sample table from a phenotype file.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype_file</code> <p>The path to the phenotype file.</p> required <code>filter_na</code> <p>Filter samples with missing phenotype values (Default: True).</p> <code>True</code> <code>read_csv_kwargs</code> <p>keyword arguments to pass to the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A <code>SampleTable</code> object.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>@classmethod\ndef from_phenotype_file(cls, phenotype_file, filter_na=True, **read_csv_kwargs):\n    \"\"\"\n    Initialize a sample table from a phenotype file.\n    :param phenotype_file: The path to the phenotype file.\n    :param filter_na: Filter samples with missing phenotype values (Default: True).\n    :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n\n    :return: A `SampleTable` object.\n    \"\"\"\n    s_tab = cls()\n    s_tab.read_phenotype_file(phenotype_file, filter_na, **read_csv_kwargs)\n    return s_tab\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.get_covariates_matrix","title":"<code>get_covariates_matrix(covar_subset=None)</code>","text":"<p>Get the covariates associated with each individual in the sample table as a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>covar_subset</code> <p>A subset of the covariate names or IDs to include in the matrix.</p> <code>None</code> <p>Returns:</p> Type Description <p>A numpy matrix with the covariates values for each individual.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def get_covariates_matrix(self, covar_subset=None):\n    \"\"\"\n    Get the covariates associated with each individual in the sample table as a matrix.\n    :param covar_subset: A subset of the covariate names or IDs to include in the matrix.\n\n    :return: A numpy matrix with the covariates values for each individual.\n    \"\"\"\n    return self.get_covariates_table(covar_subset=covar_subset).drop(['FID', 'IID'], axis=1).values\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.get_covariates_table","title":"<code>get_covariates_table(covar_subset=None)</code>","text":"<p>Get a table of covariates associated with each individual in the sample table. The table will be formatted as (FID, IID, covar1, covar2, ...).</p> <p>Parameters:</p> Name Type Description Default <code>covar_subset</code> <p>A subset of the covariate names or IDs to include in the table.</p> <code>None</code> <p>Returns:</p> Type Description <p>A pandas DataFrame with the covariate information.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def get_covariates_table(self, covar_subset=None):\n    \"\"\"\n    Get a table of covariates associated with each individual in the\n    sample table. The table will be formatted as (FID, IID, covar1, covar2, ...).\n\n    :param covar_subset: A subset of the covariate names or IDs to include in the table.\n    :return: A pandas DataFrame with the covariate information.\n    \"\"\"\n    assert self._covariate_cols is not None\n\n    if covar_subset is None:\n        covar = list(self._covariate_cols)\n    else:\n        covar = list(set(self._covariate_cols).intersection(set(covar_subset)))\n\n    assert len(covar) &gt;= 1\n\n    return self.to_table(col_subset=['FID', 'IID'] + covar)\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.get_individual_table","title":"<code>get_individual_table()</code>","text":"<p>Returns:</p> Type Description <p>A table of individual IDs (FID, IID) present in the sample table.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def get_individual_table(self):\n    \"\"\"\n    :return: A table of individual IDs (FID, IID) present in the sample table.\n    \"\"\"\n    return self.to_table(col_subset=['FID', 'IID'])\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.get_phenotype_table","title":"<code>get_phenotype_table()</code>","text":"<p>Returns:</p> Type Description <p>A table of individual IDs and phenotype values (FID IID phenotype) in the sample table.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def get_phenotype_table(self):\n    \"\"\"\n    :return: A table of individual IDs and phenotype values (FID IID phenotype) in the sample table.\n    \"\"\"\n    try:\n        return self.to_table(col_subset=['FID', 'IID', 'phenotype'])\n    except KeyError:\n        raise KeyError(\"The phenotype is not set!\")\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.post_check_phenotype","title":"<code>post_check_phenotype()</code>","text":"<p>Apply some simple heuristics to check the phenotype values provided by the user and infer the phenotype likelihood (if feasible).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the phenotype values could not be matched with the inferred phenotype likelihood.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def post_check_phenotype(self):\n    \"\"\"\n    Apply some simple heuristics to check the phenotype values\n    provided by the user and infer the phenotype likelihood (if feasible).\n\n    :raises ValueError: If the phenotype values could not be matched with the\n    inferred phenotype likelihood.\n    \"\"\"\n\n    if 'phenotype' in self.table.columns:\n\n        unique_vals = self.table['phenotype'].unique()\n\n        if self.table['phenotype'].isnull().all():\n            self.table.drop('phenotype', axis=1, inplace=True)\n        elif self._phenotype_likelihood != 'gaussian':\n\n            if len(unique_vals) &gt; 2:\n                self._phenotype_likelihood = 'gaussian'\n                return\n\n            unique_vals = sorted(unique_vals)\n\n            if unique_vals == [1, 2]:\n                # Plink coding for case/control\n                self.table['phenotype'] -= 1\n                self._phenotype_likelihood = 'binomial'\n            elif unique_vals == [0, 1]:\n                self._phenotype_likelihood = 'binomial'\n            else:\n                raise ValueError(f\"Unknown values for binary traits: {unique_vals}. \"\n                                 f\"The software only supports 0/1 or 1/2 coding for cases and controls.\")\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.read_covariates_file","title":"<code>read_covariates_file(covar_file, **read_csv_kwargs)</code>","text":"<p>Read the covariates file from the provided path. The expected format is Family ID (<code>FID</code>), Individual ID (<code>IID</code>) and the remaining columns are assumed to be covariates. You may adjust the parsing configurations with keyword arguments that will be passed to <code>pandas.read_csv</code>.</p> <p>Warning</p> <p>If covariate columns are already present in the sample table, they will be replaced. The data structure currently does not support reading separate covariates files.</p> <p>Parameters:</p> Name Type Description Default <code>covar_file</code> <p>The path to the covariates file.</p> required <code>read_csv_kwargs</code> <p>keyword arguments to pass to the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def read_covariates_file(self, covar_file, **read_csv_kwargs):\n    \"\"\"\n    Read the covariates file from the provided path. The expected format is Family ID (`FID`),\n    Individual ID (`IID`) and the remaining columns are assumed to be covariates. You may adjust\n    the parsing configurations with keyword arguments that will be passed to `pandas.read_csv`.\n\n    !!! warning \"Warning\"\n        If covariate columns are already present in the sample table, they will be replaced.\n        The data structure currently does not support reading separate covariates files.\n\n    :param covar_file: The path to the covariates file.\n    :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n    \"\"\"\n\n    if 'sep' not in read_csv_kwargs and 'delimiter' not in read_csv_kwargs:\n        read_csv_kwargs['sep'] = r'\\s+'\n\n    from .utils.compute_utils import detect_header_keywords\n\n    if 'header' not in read_csv_kwargs:\n\n        if detect_header_keywords(covar_file, ['FID', 'IID']):\n            read_csv_kwargs['header'] = 0\n        else:\n            read_csv_kwargs['header'] = None\n\n    covar_table = pd.read_csv(covar_file, **read_csv_kwargs)\n\n    if self._covariate_cols is not None:\n        self.table.drop(columns=self._covariate_cols, inplace=True)\n\n    if read_csv_kwargs['header'] is None:\n        self._covariate_cols = np.array([f'covar_{i + 1}' for i in range(covar_table.shape[1] - 2)])\n    else:\n        self._covariate_cols = covar_table.columns[2:]\n\n    covar_table.columns = ['FID', 'IID'] + list(self._covariate_cols)\n\n    if self.table is not None:\n        covar_table['FID'] = covar_table['FID'].astype(type(self.fid[0]))\n        covar_table['IID'] = covar_table['IID'].astype(type(self.iid[0]))\n\n        self.table = self.table.merge(covar_table, on=['FID', 'IID'])\n    else:\n        self.table = covar_table\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.read_phenotype_file","title":"<code>read_phenotype_file(phenotype_file, drop_na=True, **read_csv_kwargs)</code>","text":"<p>Read the phenotype file from disk. The expected format is Family ID (<code>FID</code>), Individual ID (<code>IID</code>) and the phenotype column <code>phenotype</code>. You may adjust the parsing configurations with keyword arguments that will be passed to <code>pandas.read_csv</code>.</p> <p>Warning</p> <p>If a phenotype column is already present in the sample table, it will be replaced. The data structure currently does not support multiple phenotype columns.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype_file</code> <p>The path to the phenotype file.</p> required <code>drop_na</code> <p>Drop samples whose phenotype value is missing (Default: True).</p> <code>True</code> <code>read_csv_kwargs</code> <p>keyword arguments to pass to the <code>read_csv</code> function of <code>pandas</code>.</p> <code>{}</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def read_phenotype_file(self, phenotype_file, drop_na=True, **read_csv_kwargs):\n    \"\"\"\n    Read the phenotype file from disk. The expected format is Family ID (`FID`),\n    Individual ID (`IID`) and the phenotype column `phenotype`. You may adjust\n    the parsing configurations with keyword arguments that will be passed to `pandas.read_csv`.\n\n    !!! warning \"Warning\"\n        If a phenotype column is already present in the sample table, it will be replaced.\n        The data structure currently does not support multiple phenotype columns.\n\n    :param phenotype_file: The path to the phenotype file.\n    :param drop_na: Drop samples whose phenotype value is missing (Default: True).\n    :param read_csv_kwargs: keyword arguments to pass to the `read_csv` function of `pandas`.\n    \"\"\"\n\n    if 'sep' not in read_csv_kwargs and 'delimiter' not in read_csv_kwargs:\n        read_csv_kwargs['sep'] = r'\\s+'\n\n    if 'na_values' not in read_csv_kwargs:\n        read_csv_kwargs['na_values'] = {'phenotype': [-9.]}\n\n    if 'dtype' not in read_csv_kwargs:\n        read_csv_kwargs['dtype'] = {'phenotype': float}\n\n    from .utils.compute_utils import detect_header_keywords\n\n    if all([col not in read_csv_kwargs for col in ('header', 'names')]):\n\n        if detect_header_keywords(phenotype_file, ['FID', 'IID']):\n            read_csv_kwargs['header'] = 0\n        else:\n            read_csv_kwargs['names'] = ['FID', 'IID', 'phenotype']\n\n    pheno_table = pd.read_csv(phenotype_file, **read_csv_kwargs)\n\n    if self.table is not None:\n        pheno_table['FID'] = pheno_table['FID'].astype(type(self.fid[0]))\n        pheno_table['IID'] = pheno_table['IID'].astype(type(self.iid[0]))\n\n        # Drop the phenotype column if it already exists:\n        if 'phenotype' in self.table.columns:\n            self.table.drop(columns=['phenotype'], inplace=True)\n\n        self.table = self.table.merge(pheno_table, on=['FID', 'IID'])\n    else:\n        self.table = pheno_table\n\n    if self.table['phenotype'].isnull().all():\n        self.table.drop('phenotype', axis=1, inplace=True)\n    elif drop_na:\n        # Maybe using converters in the read_csv above?\n        self.table = self.table.dropna(subset=['phenotype'])\n\n    self.post_check_phenotype()\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.set_phenotype","title":"<code>set_phenotype(phenotype, phenotype_likelihood=None)</code>","text":"<p>Update the phenotype in the sample table using the provided values.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype</code> <p>The new phenotype values, represented by a numpy array or Iterable.</p> required <code>phenotype_likelihood</code> <p>The likelihood of the phenotype values.</p> <code>None</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def set_phenotype(self, phenotype, phenotype_likelihood=None):\n    \"\"\"\n    Update the phenotype in the sample table using the provided values.\n    :param phenotype: The new phenotype values, represented by a numpy array or Iterable.\n    :param phenotype_likelihood: The likelihood of the phenotype values.\n    \"\"\"\n\n    self.table['phenotype'] = phenotype\n\n    if phenotype_likelihood is not None:\n        self._phenotype_likelihood = phenotype_likelihood\n    else:\n        self.post_check_phenotype()\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.to_file","title":"<code>to_file(output_file, col_subset=None, **to_csv_kwargs)</code>","text":"<p>Write the contents of the sample table to file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <p>The path to the file where to write the sample table.</p> required <code>col_subset</code> <p>A subset of the columns to write to file.</p> <code>None</code> <code>to_csv_kwargs</code> <p>keyword arguments to pass to the <code>to_csv</code> function of <code>pandas</code>.</p> <code>{}</code> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def to_file(self, output_file, col_subset=None, **to_csv_kwargs):\n    \"\"\"\n    Write the contents of the sample table to file.\n    :param output_file: The path to the file where to write the sample table.\n    :param col_subset: A subset of the columns to write to file.\n    :param to_csv_kwargs: keyword arguments to pass to the `to_csv` function of `pandas`.\n    \"\"\"\n\n    assert self.table is not None\n\n    if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n        to_csv_kwargs['sep'] = '\\t'\n\n    if 'index' not in to_csv_kwargs:\n        to_csv_kwargs['index'] = False\n\n    if col_subset is not None:\n        table = self.table[col_subset]\n    else:\n        table = self.table\n\n    table.to_csv(output_file, **to_csv_kwargs)\n</code></pre>"},{"location":"api/SampleTable/#magenpy.SampleTable.SampleTable.to_table","title":"<code>to_table(col_subset=None)</code>","text":"<p>Get the sample table as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>A subset of the columns to include in the table.</p> <code>None</code> <p>Returns:</p> Type Description <p>A pandas DataFrame with the sample information.</p> Source code in <code>magenpy/SampleTable.py</code> <pre><code>def to_table(self, col_subset=None):\n    \"\"\"\n    Get the sample table as a pandas DataFrame.\n\n    :param col_subset: A subset of the columns to include in the table.\n    :return: A pandas DataFrame with the sample information.\n    \"\"\"\n    if col_subset is not None:\n        return self.table[list(col_subset)]\n    else:\n        return self.table\n</code></pre>"},{"location":"api/SumstatsTable/","title":"SumstatsTable","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for representing the summary statistics obtained from Genome-wide Association Studies (GWAS). GWAS software tools publish their results in the form of summary statistics, which include the SNP rsIDs, the effect/reference alleles tested, the marginal effect sizes (BETA), the standard errors (SE), the Z-scores, the p-values, etc.</p> <p>This class provides a convenient way to access/manipulate/harmonize these summary statistics across various formats. Particularly, given the heterogeneity in summary statistics formats, this class provides a common interface to access these statistics in a consistent manner. The class also supports computing some derived statistics from the summary statistics, such as the pseudo-correlation between the SNP and the phenotype, the Chi-squared statistics, etc.</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>DataFrame</code> <p>A pandas DataFrame containing the summary statistics.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>class SumstatsTable(object):\n    \"\"\"\n    A wrapper class for representing the summary statistics obtained from\n    Genome-wide Association Studies (GWAS). GWAS software tools publish their\n    results in the form of summary statistics, which include the SNP rsIDs,\n    the effect/reference alleles tested, the marginal effect sizes (BETA),\n    the standard errors (SE), the Z-scores, the p-values, etc.\n\n    This class provides a convenient way to access/manipulate/harmonize these summary statistics\n    across various formats. Particularly, given the heterogeneity in summary statistics\n    formats, this class provides a common interface to access these statistics\n    in a consistent manner. The class also supports computing some derived statistics\n    from the summary statistics, such as the pseudo-correlation between the SNP and the\n    phenotype, the Chi-squared statistics, etc.\n\n    :ivar table: A pandas DataFrame containing the summary statistics.\n    \"\"\"\n\n    def __init__(self, ss_table: pd.DataFrame):\n        \"\"\"\n        Initialize the summary statistics table.\n\n        :param ss_table: A pandas DataFrame containing the summary statistics.\n\n        !!! seealso \"See Also\"\n            * [from_file][magenpy.SumstatsTable.SumstatsTable.from_file]\n        \"\"\"\n        self.table: pd.DataFrame = ss_table\n\n        # Check that the table contains some of the required columns (non-exhaustive):\n\n        # Either has SNP or CHR+POS:\n        assert 'SNP' in self.table.columns or all([col in self.table.columns for col in ('CHR', 'POS')])\n        # Assert that the table has at least one of the alleles:\n        assert any([col in self.table.columns for col in ('A1', 'A2')])\n        # TODO: Add other assertions?\n\n    @property\n    def shape(self):\n        \"\"\"\n        :return: The shape of the summary statistics table.\n        \"\"\"\n        return self.table.shape\n\n    def __len__(self):\n        return len(self.table)\n\n    @property\n    def chromosome(self):\n        \"\"\"\n        A convenience method to return the chromosome number if there is only\n        one chromosome in the summary statistics.\n        If multiple chromosomes are present, it returns None.\n\n        :return: The chromosome number if there is only one chromosome in the summary statistics.\n        \"\"\"\n        chrom = self.chromosomes\n        if chrom is not None and len(chrom) == 1:\n            return chrom[0]\n\n    @property\n    def chromosomes(self):\n        \"\"\"\n        :return: The unique chromosomes in the summary statistics table.\n        \"\"\"\n        if 'CHR' in self.table.columns:\n            return sorted(self.table['CHR'].unique())\n\n    @property\n    def m(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n_snps][magenpy.SumstatsTable.SumstatsTable.n_snps]\n\n        :return: The number of variants in the summary statistics table.\n        \"\"\"\n        return self.n_snps\n\n    @property\n    def identifier_cols(self):\n        if 'SNP' in self.table.columns:\n            return ['SNP']\n        else:\n            return ['CHR', 'POS']\n\n    @property\n    def n_snps(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [m][magenpy.SumstatsTable.SumstatsTable.m]\n\n        :return: The number of variants in the summary statistics table.\n        \"\"\"\n        return len(self.table)\n\n    @property\n    def snps(self):\n        \"\"\"\n        :return: The rsIDs associated with each variant in the summary statistics table.\n        \"\"\"\n        return self.table['SNP'].values\n\n    @property\n    def a1(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [effect_allele][magenpy.SumstatsTable.SumstatsTable.effect_allele]\n            * [alt_allele][magenpy.SumstatsTable.SumstatsTable.alt_allele]\n\n        :return: The alternative or effect allele for each variant in the summary statistics table.\n\n        \"\"\"\n        return self.table['A1'].values\n\n    @property\n    def a2(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [ref_allele][magenpy.SumstatsTable.SumstatsTable.ref_allele]\n\n        :return: The reference allele for each variant in the summary statistics table.\n        \"\"\"\n        return self.get_col('A2')\n\n    @property\n    def ref_allele(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [a2][magenpy.SumstatsTable.SumstatsTable.a2]\n\n        :return: The reference allele for each variant in the summary statistics table.\n        \"\"\"\n        return self.a2\n\n    @property\n    def alt_allele(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [effect_allele][magenpy.SumstatsTable.SumstatsTable.effect_allele]\n            * [a1][magenpy.SumstatsTable.SumstatsTable.a1]\n\n        :return: The alternative or effect allele for each variant in the summary statistics table.\n        \"\"\"\n        return self.a1\n\n    @property\n    def effect_allele(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [alt_allele][magenpy.SumstatsTable.SumstatsTable.alt_allele]\n            * [a1][magenpy.SumstatsTable.SumstatsTable.a1]\n\n        :return: The alternative or effect allele for each variant in the summary statistics table.\n        \"\"\"\n        return self.a1\n\n    @property\n    def bp_pos(self):\n        \"\"\"\n        :return: The base pair position for each variant in the summary statistics table.\n        \"\"\"\n        return self.get_col('POS')\n\n    @property\n    def maf(self):\n        \"\"\"\n        :return: The minor allele frequency for each variant in the summary statistics table.\n        \"\"\"\n        return self.get_col('MAF')\n\n    @property\n    def maf_var(self):\n        \"\"\"\n        :return: The variance of the minor allele frequency for each variant in the summary statistics table.\n        \"\"\"\n        return 2.*self.maf*(1. - self.maf)\n\n    @property\n    def n(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [n_per_snp][magenpy.SumstatsTable.SumstatsTable.n_per_snp]\n\n        :return: The sample size for the association test of each variant in the summary statistics table.\n        \"\"\"\n        return self.get_col('N')\n\n    @property\n    def n_per_snp(self):\n        \"\"\"\n        # TODO: Add a way to infer N from other sumstats if missing.\n\n        !!! seealso \"See Also\"\n            * [n][magenpy.SumstatsTable.SumstatsTable.n]\n\n        :return: The sample size for the association test of each variant in the summary statistics table.\n        \"\"\"\n        return self.get_col('N')\n\n    @property\n    def beta_hat(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [marginal_beta][magenpy.SumstatsTable.SumstatsTable.marginal_beta]\n\n        :return: The marginal beta from the association test of each variant on the phenotype.\n        \"\"\"\n\n        beta = self.get_col('BETA')\n\n        if beta is None:\n            odds_ratio = self.odds_ratio\n            if odds_ratio is not None:\n                self.table['BETA'] = np.log(odds_ratio)\n                return self.table['BETA'].values\n        else:\n            return beta\n\n    @property\n    def marginal_beta(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [beta_hat][magenpy.SumstatsTable.SumstatsTable.beta_hat]\n\n        :return: The marginal beta from the association test of each variant on the phenotype.\n        \"\"\"\n        return self.beta_hat\n\n    @property\n    def odds_ratio(self):\n        \"\"\"\n        :return: The odds ratio from the association test of each variant on case-control phenotypes.\n        \"\"\"\n        return self.get_col('OR')\n\n    @property\n    def standardized_marginal_beta(self):\n        \"\"\"\n        Get the marginal BETAs assuming that both the genotype matrix\n        and the phenotype vector are standardized column-wise to have mean zero and variance 1.\n        In some contexts, this is also known as the per-SNP correlation or\n        pseudo-correlation with the phenotype.\n\n        !!! seealso \"See Also\"\n            * [get_snp_pseudo_corr][magenpy.SumstatsTable.SumstatsTable.get_snp_pseudo_corr]\n\n        :return: The standardized marginal beta from the association test of each variant on the phenotype.\n        \"\"\"\n        return self.get_snp_pseudo_corr()\n\n    @property\n    def z_score(self):\n        \"\"\"\n        :return: The Z-score from the association test of each SNP on the phenotype.\n        :raises KeyError: If the Z-score statistic is not available and could not be inferred from available data.\n        \"\"\"\n\n        z = self.get_col('Z')\n        if z is not None:\n            return z\n        else:\n            beta = self.beta_hat\n            se = self.se\n\n            if beta is not None and se is not None:\n                self.table['Z'] = beta / se\n                return self.table['Z'].values\n\n        raise KeyError(\"Z-score statistic is not available and could not be inferred from available data!\")\n\n    @property\n    def standard_error(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [se][magenpy.SumstatsTable.SumstatsTable.se]\n\n        :return: The standard error from the association test of each variant on the phenotype.\n\n        \"\"\"\n        return self.get_col('SE')\n\n    @property\n    def se(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [standard_error][magenpy.SumstatsTable.SumstatsTable.standard_error]\n\n        :return: The standard error from the association test of each variant on the phenotype.\n        \"\"\"\n        return self.standard_error\n\n    @property\n    def pval(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [p_value][magenpy.SumstatsTable.SumstatsTable.p_value]\n\n        :return: The p-value from the association test of each variant on the phenotype.\n        \"\"\"\n        p = self.get_col('PVAL')\n\n        if p is not None:\n            return p\n        else:\n            from scipy import stats\n            self.table['PVAL'] = 2.*stats.norm.sf(np.abs(self.z_score))\n            return self.table['PVAL'].values\n\n    @property\n    def p_value(self):\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [pval][magenpy.SumstatsTable.SumstatsTable.pval]\n\n        :return: The p-value from the association test of each variant on the phenotype.\n        \"\"\"\n        return self.pval\n\n    @property\n    def negative_log10_p_value(self):\n        \"\"\"\n        :return: The negative log10 of the p-value (-log10(p_value)) of association\n        test of each variant on the phenotype.\n        \"\"\"\n        return -np.log10(self.pval)\n\n    @property\n    def effect_sign(self):\n        \"\"\"\n        :return: The sign for the effect size (1 for positive effect, -1 for negative effect)\n        of each genetic variant ib the phenotype.\n\n        :raises KeyError: If the sign could not be inferred from available data.\n        \"\"\"\n\n        signed_statistics = ['BETA', 'Z', 'OR']\n\n        for ss in signed_statistics:\n            ss_value = self.get_col(ss)\n            if ss_value is not None:\n                if ss == 'OR':\n                    return np.sign(np.log(ss_value))\n                else:\n                    return np.sign(ss_value)\n\n        raise KeyError(\"No signed statistic to extract the sign from!\")\n\n    def infer_a2(self, reference_table, allow_na=False):\n        \"\"\"\n        Infer the reference allele A2 (if not present in the SumstatsTable)\n        from a reference table. Make sure that the reference table contains the identifier information\n        for each SNP, in addition to the reference allele A2 and the alternative (i.e. effect) allele A1.\n        It is the user's responsibility to make sure that the reference table matches the summary\n        statistics in terms of the specification of reference vs. alternative. They have to be consistent\n        across the two tables.\n\n        :param reference_table: A pandas table containing the following columns at least:\n        SNP identifiers (`SNP` or `CHR` &amp; `POS`) and allele information (`A1` &amp; `A2`).\n        :param allow_na: If True, allow the reference allele to be missing from the final result.\n        \"\"\"\n\n        # Get the identifier columns for this table:\n        id_cols = self.identifier_cols\n\n        # Sanity checks:\n        assert all([col in reference_table.columns for col in id_cols + ['A1', 'A2']])\n\n        # Merge the summary statistics table with the reference table on unique ID:\n        merged_table = self.table[id_cols + ['A1']].merge(\n            reference_table[id_cols + ['A1', 'A2']],\n            how='left',\n            on=id_cols\n        )\n        # If `A1_x` agrees with `A1_y`, then `A2` is indeed the reference allele.\n        # Otherwise, they are flipped and `A1_y` should be the reference allele:\n        merged_table['A2'] = np.where(merged_table['A1_x'] == merged_table['A1_y'],\n                                      merged_table['A2'],\n                                      merged_table['A1_y'])\n\n        # Check that the reference allele could be inferred for all SNPs:\n        if not allow_na and merged_table['A2'].isna().any():\n            raise ValueError(\"The reference allele could not be inferred for some SNPs!\")\n        else:\n            self.table['A2'] = merged_table['A2']\n\n    def infer_snp_id(self, reference_table, allow_na=False):\n        \"\"\"\n        Infer the SNP ID (if not present in the SumstatsTable) from a reference table.\n        Make sure that the reference table contains the SNP ID, chromosome ID, and position.\n\n        :param reference_table: A pandas table containing the following columns at least:\n        `SNP`, `CHR`, `POS`.\n        :param allow_na: If True, allow the SNP ID to be missing from the final result.\n        \"\"\"\n\n        # Merge the summary statistics table with the reference table:\n        merged_table = self.table[['CHR', 'POS']].merge(reference_table[['SNP', 'CHR', 'POS']], how='left')\n\n        # Check that the SNP ID could be inferred for all SNPs:\n        if not allow_na and merged_table['SNP'].isna().any():\n            raise ValueError(\"The SNP ID could not be inferred for some SNPs!\")\n        else:\n            self.table['SNP'] = merged_table['SNP'].values\n\n    def set_sample_size(self, n):\n        \"\"\"\n        Set the sample size for each variant in the summary table.\n        This can be useful when the overall sample size from the GWAS analysis is available,\n        but not on a per-SNP basis.\n\n        :param n: A scalar or array of sample sizes for each variant.\n        \"\"\"\n        self.table['N'] = n\n\n    def run_quality_control(self, reference_table=None):\n        \"\"\"\n        Run quality control checks on the summary statistics table.\n        TODO: Implement quality control checks following recommendations given by Prive et al.:\n        https://doi.org/10.1016/j.xhgg.2022.100136\n        Given user fine-control over which checks to run and which to skip.\n        Maybe move parts of this implementation to a module in `stats` (TBD)\n        \"\"\"\n        pass\n\n    def match(self, reference_table, correct_flips=True):\n        \"\"\"\n        Match the summary statistics table with a reference table,\n        correcting for potential flips in the effect alleles.\n\n        :param reference_table: The SNP table to use as a reference. Must be a pandas\n        table with the following columns: SNP identifier (either `SNP` or `CHR` &amp; `POS`) and allele information\n        (`A1` &amp; `A2`).\n        :param correct_flips: If True, correct the direction of effect size\n         estimates if the effect allele is reversed.\n        \"\"\"\n\n        from .utils.model_utils import merge_snp_tables\n\n        self.table = merge_snp_tables(ref_table=reference_table[self.identifier_cols + ['A1', 'A2']],\n                                      alt_table=self.table,\n                                      how='inner',\n                                      correct_flips=correct_flips)\n\n    def filter_by_allele_frequency(self, min_maf=None, min_mac=None):\n        \"\"\"\n        Filter variants in the summary statistics table by minimum minor allele frequency or allele count\n        :param min_maf: Minimum minor allele frequency\n        :param min_mac: Minimum minor allele count\n        \"\"\"\n\n        if min_mac or min_maf:\n            maf = self.maf\n            n = self.n_per_snp\n        else:\n            return\n\n        keep_flag = None\n\n        if min_mac and n and maf:\n            mac = (2*maf*n).astype(np.int64)\n            keep_flag = (mac &gt;= min_mac) &amp; ((2*n - mac) &gt;= min_mac)\n\n        if min_maf and maf:\n\n            maf_cond = (maf &gt;= min_maf) &amp; (1. - maf &gt;= min_maf)\n            if keep_flag is not None:\n                keep_flag = keep_flag &amp; maf_cond\n            else:\n                keep_flag = maf_cond\n\n        if keep_flag is not None:\n            self.filter_snps(extract_index=np.where(keep_flag)[0])\n\n    def filter_snps(self, extract_snps=None, extract_file=None, extract_index=None):\n        \"\"\"\n        Filter the summary statistics table to keep a subset of SNPs.\n        :param extract_snps: A list or array of SNP IDs to keep.\n        :param extract_file: A plink-style file containing the SNP IDs to keep.\n        :param extract_index: A list or array of the indices of SNPs to retain.\n        \"\"\"\n\n        assert extract_snps is not None or extract_file is not None or extract_index is not None\n\n        if extract_file:\n            from .parsers.misc_parsers import read_snp_filter_file\n            extract_snps = read_snp_filter_file(extract_file)\n\n        if extract_snps is not None:\n            extract_index = intersect_arrays(self.snps, extract_snps, return_index=True)\n\n        if extract_index is not None:\n            self.table = self.table.iloc[extract_index, ].reset_index(drop=True)\n        else:\n            raise Exception(\"To filter a summary statistics table, you must provide \"\n                            \"the list of SNPs, a file containing the list of SNPs, \"\n                            \"or a list of indices to retain.\")\n\n    def drop_duplicates(self):\n        \"\"\"\n        Drop variants with duplicated rsIDs from the summary statistics table.\n        \"\"\"\n\n        self.table = self.table.drop_duplicates(subset=self.identifier_cols, keep=False)\n\n    def get_col(self, col_name):\n        \"\"\"\n        :param col_name: The name of the column to extract.\n\n        :return: The column associated with `col_name` from summary statistics table.\n        \"\"\"\n        if col_name in self.table.columns:\n            return self.table[col_name].values\n\n    def get_chisq_statistic(self):\n        \"\"\"\n        :return: The Chi-Squared statistic from the association test of each variant on the phenotype.\n        :raises KeyError: If the Chi-Squared statistic is not available and could not be inferred from available data.\n        \"\"\"\n        chisq = self.get_col('CHISQ')\n\n        if chisq is not None:\n            return chisq\n        else:\n            z = self.z_score\n            if z is not None:\n                self.table['CHISQ'] = z**2\n            else:\n                p_val = self.p_value\n                if p_val is not None:\n                    from scipy.stats import chi2\n\n                    self.table['CHISQ'] = chi2.ppf(1. - p_val, 1)\n                else:\n                    raise KeyError(\"Chi-Squared statistic is not available!\")\n\n        return self.table['CHISQ'].values\n\n    def get_snp_pseudo_corr(self):\n        \"\"\"\n\n        Computes the pseudo-correlation coefficient (standardized beta) between the SNP and\n        the phenotype (X_jTy / N) from GWAS summary statistics.\n\n        This method uses Equation 15 in Mak et al. 2017\n\n            $$\n            beta =  z_j / sqrt(n - 1 + z_j^2)\n            $$\n\n        Where `z_j` is the marginal GWAS Z-score.\n\n        !!! seealso \"See Also\"\n            * [standardized_marginal_beta][magenpy.SumstatsTable.SumstatsTable.standardized_marginal_beta]\n\n        :return: The pseudo-correlation coefficient between the SNP and the phenotype.\n        :raises KeyError: If the Z-scores are not available or the sample size is not available.\n\n        \"\"\"\n\n        zsc = self.z_score\n        n = self.n\n\n        if zsc is not None:\n            if n is not None:\n                return zsc / (np.sqrt(n - 1 + zsc**2))\n            else:\n                raise KeyError(\"Sample size is not available!\")\n        else:\n            raise KeyError(\"Z-scores are not available!\")\n\n    def get_yy_per_snp(self):\n        \"\"\"\n        Computes the quantity (y'y)_j/n_j following SBayesR (Lloyd-Jones 2019) and Yang et al. (2012).\n\n        (y'y)_j/n_j is defined as the empirical variance for continuous phenotypes and may be estimated\n        from GWAS summary statistics by re-arranging the equation for the\n        squared standard error:\n\n            $$\n            SE(b_j)^2 = (Var(y) - Var(x_j)*b_j^2) / (Var(x)*n)\n            $$\n\n        Which gives the following estimate:\n\n            $$\n            (y'y)_j / n_j = (n_j - 2)*SE(b_j)^2 + b_j^2\n            $$\n\n        :return: The quantity (y'y)_j/n_j for each SNP in the summary statistics table.\n        :raises KeyError: If the marginal betas, standard errors or sample sizes are not available.\n\n        \"\"\"\n\n        b = self.beta_hat\n        se = self.standard_error\n        n = self.n\n\n        if n is not None:\n            if b is not None:\n                if se is not None:\n                    return (n - 2)*se**2 + b**2\n                else:\n                    raise KeyError(\"Standard errors are not available!\")\n            else:\n                raise KeyError(\"Marginal betas are not available!\")\n        else:\n            raise KeyError(\"Sample size per SNP is not available!\")\n\n    def split_by_chromosome(self, snps_per_chrom=None):\n        \"\"\"\n        Split the summary statistics table by chromosome, so that we would\n        have a separate `SumstatsTable` object for each chromosome.\n        :param snps_per_chrom: A dictionary where the keys are the chromosome number \n        and the value is an array or list of SNPs on that chromosome.\n\n        :return: A dictionary where the keys are the chromosome number and the value is a `SumstatsTable` object.\n        \"\"\"\n\n        if 'CHR' in self.table.columns:\n            chrom_tables = self.table.groupby('CHR')\n            return {\n                c: SumstatsTable(chrom_tables.get_group(c).copy())\n                for c in chrom_tables.groups\n            }\n        elif snps_per_chrom is not None:\n            chrom_dict = {\n                c: SumstatsTable(pd.DataFrame({'SNP': snps}).merge(self.table))\n                for c, snps in snps_per_chrom.items()\n            }\n\n            for c, ss_tab in chrom_dict.items():\n                ss_tab.table['CHR'] = c\n\n            return chrom_dict\n        else:\n            raise Exception(\"To split the summary statistics table by chromosome, \"\n                            \"you must provide the a dictionary mapping chromosome number \"\n                            \"to an array of SNPs `snps_per_chrom`.\")\n\n    def to_table(self, col_subset=None):\n        \"\"\"\n        A convenience method to extract the summary statistics table or subsets of it.\n\n        :param col_subset: A list corresponding to a subset of columns to return.\n\n        :return: A pandas DataFrame containing the summary statistics with the requested column subset.\n        \"\"\"\n\n        col_subset = col_subset or ['CHR', 'SNP', 'POS', 'A1', 'A2', 'MAF',\n                                    'N', 'BETA', 'Z', 'SE', 'PVAL']\n\n        # Because some of the quantities that the user needs may be need to be\n        # computed, we separate the column subset into those that are already\n        # present in the table and those that are not (but can still be computed\n        # from other summary statistics):\n\n        present_cols = list(set(col_subset).intersection(set(self.table.columns)))\n        non_present_cols = list(set(col_subset) - set(present_cols))\n\n        if len(present_cols) &gt; 0:\n            table = self.table[present_cols].copy()\n        else:\n            table = pd.DataFrame({c: [] for c in non_present_cols})\n\n        for col in non_present_cols:\n\n            if col == 'Z':\n                table['Z'] = self.z_score\n            elif col == 'PVAL':\n                table['PVAL'] = self.p_value\n            elif col == 'LOG10_PVAL':\n                table['NLOG10_PVAL'] = self.negative_log10_p_value\n            elif col == 'CHISQ':\n                table['CHISQ'] = self.get_chisq_statistic()\n            elif col == 'MAF_VAR':\n                table['MAF_VAR'] = self.maf_var\n            elif col == 'STD_BETA':\n                table['STD_BETA'] = self.get_snp_pseudo_corr()\n            else:\n                warnings.warn(f\"Column '{col}' is not available in the summary statistics table!\")\n\n        return table[list(col_subset)]\n\n    def to_file(self, output_file, col_subset=None, **to_csv_kwargs):\n        \"\"\"\n        A convenience method to write the summary statistics table to file.\n\n        TODO: Add a format argument to this method and allow the user to output summary statistics\n        according to supported formats (e.g. COJO, plink, fastGWA, etc.).\n\n        :param output_file: The path to the file where to write the summary statistics.\n        :param col_subset: A subset of the columns to write to file.\n        :param to_csv_kwargs: Keyword arguments to pass to pandas' `to_csv` method.\n\n        \"\"\"\n\n        if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n            to_csv_kwargs['sep'] = '\\t'\n\n        if 'index' not in to_csv_kwargs:\n            to_csv_kwargs['index'] = False\n\n        table = self.to_table(col_subset)\n        table.to_csv(output_file, **to_csv_kwargs)\n\n    @classmethod\n    def from_file(cls, sumstats_file, sumstats_format=None, parser=None, **parse_kwargs):\n        \"\"\"\n        Initialize a summary statistics table from file. The user must provide either\n        the format for the summary statistics file or the parser object\n        (see `parsers.sumstats_parsers`).\n\n        :param sumstats_file: The path to the summary statistics file.\n        :param sumstats_format: The format for the summary statistics file. Currently,\n        we support the following summary statistics formats: `magenpy`, `plink1.9`, `plink` or `plink2`,\n        `COJO`, `fastGWA`, `SAIGE`, `GWASCatalog` (also denoted as `GWAS-SSF` and `SSF`).\n        :param parser: An instance of SumstatsParser parser, implements basic parsing/conversion\n        functionalities.\n        :param parse_kwargs: arguments for the pandas `read_csv` function, such as the delimiter.\n\n        :return: A `SumstatsTable` object initialized from the summary statistics file.\n        \"\"\"\n        assert sumstats_format is not None or parser is not None\n\n        from .parsers.sumstats_parsers import (\n            SumstatsParser, Plink1SSParser, Plink2SSParser, COJOSSParser,\n            FastGWASSParser, SSFParser, SaigeSSParser\n        )\n\n        sumstats_format_l = sumstats_format.lower()\n\n        if parser is None:\n            if sumstats_format_l == 'magenpy':\n                parser = SumstatsParser(None, **parse_kwargs)\n            elif sumstats_format_l in ('plink', 'plink2'):\n                parser = Plink2SSParser(None, **parse_kwargs)\n            elif sumstats_format_l == 'plink1.9':\n                parser = Plink1SSParser(None, **parse_kwargs)\n            elif sumstats_format_l == 'cojo':\n                parser = COJOSSParser(None, **parse_kwargs)\n            elif sumstats_format_l == 'fastgwa':\n                parser = FastGWASSParser(None, **parse_kwargs)\n            elif sumstats_format_l in ('ssf', 'gwas-ssf', 'gwascatalog'):\n                parser = SSFParser(None, **parse_kwargs)\n            elif sumstats_format_l == 'saige':\n                parser = SaigeSSParser(None, **parse_kwargs)\n            else:\n                raise KeyError(f\"Parsers for summary statistics format {sumstats_format} are not implemented!\")\n\n        sumstats_table = parser.parse(sumstats_file)\n        return cls(sumstats_table)\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.a1","title":"<code>a1</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>effect_allele</li> <li>alt_allele</li> </ul> <p>Returns:</p> Type Description <p>The alternative or effect allele for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.a2","title":"<code>a2</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>ref_allele</li> </ul> <p>Returns:</p> Type Description <p>The reference allele for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.alt_allele","title":"<code>alt_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>effect_allele</li> <li>a1</li> </ul> <p>Returns:</p> Type Description <p>The alternative or effect allele for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.beta_hat","title":"<code>beta_hat</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>marginal_beta</li> </ul> <p>Returns:</p> Type Description <p>The marginal beta from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.bp_pos","title":"<code>bp_pos</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The base pair position for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.chromosome","title":"<code>chromosome</code>  <code>property</code>","text":"<p>A convenience method to return the chromosome number if there is only one chromosome in the summary statistics. If multiple chromosomes are present, it returns None.</p> <p>Returns:</p> Type Description <p>The chromosome number if there is only one chromosome in the summary statistics.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.chromosomes","title":"<code>chromosomes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The unique chromosomes in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.effect_allele","title":"<code>effect_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>alt_allele</li> <li>a1</li> </ul> <p>Returns:</p> Type Description <p>The alternative or effect allele for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.effect_sign","title":"<code>effect_sign</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The sign for the effect size (1 for positive effect, -1 for negative effect) of each genetic variant ib the phenotype.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the sign could not be inferred from available data.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.m","title":"<code>m</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_snps</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.maf","title":"<code>maf</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The minor allele frequency for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.maf_var","title":"<code>maf_var</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The variance of the minor allele frequency for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.marginal_beta","title":"<code>marginal_beta</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>beta_hat</li> </ul> <p>Returns:</p> Type Description <p>The marginal beta from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.n","title":"<code>n</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_per_snp</li> </ul> <p>Returns:</p> Type Description <p>The sample size for the association test of each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.n_per_snp","title":"<code>n_per_snp</code>  <code>property</code>","text":""},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.n_per_snp--todo-add-a-way-to-infer-n-from-other-sumstats-if-missing","title":"TODO: Add a way to infer N from other sumstats if missing.","text":"<p>See Also</p> <ul> <li>n</li> </ul> <p>Returns:</p> Type Description <p>The sample size for the association test of each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>m</li> </ul> <p>Returns:</p> Type Description <p>The number of variants in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.negative_log10_p_value","title":"<code>negative_log10_p_value</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The negative log10 of the p-value (-log10(p_value)) of association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.odds_ratio","title":"<code>odds_ratio</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The odds ratio from the association test of each variant on case-control phenotypes.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.p_value","title":"<code>p_value</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>pval</li> </ul> <p>Returns:</p> Type Description <p>The p-value from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.pval","title":"<code>pval</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>p_value</li> </ul> <p>Returns:</p> Type Description <p>The p-value from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.ref_allele","title":"<code>ref_allele</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>a2</li> </ul> <p>Returns:</p> Type Description <p>The reference allele for each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.se","title":"<code>se</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>standard_error</li> </ul> <p>Returns:</p> Type Description <p>The standard error from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The shape of the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The rsIDs associated with each variant in the summary statistics table.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.standard_error","title":"<code>standard_error</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>se</li> </ul> <p>Returns:</p> Type Description <p>The standard error from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.standardized_marginal_beta","title":"<code>standardized_marginal_beta</code>  <code>property</code>","text":"<p>Get the marginal BETAs assuming that both the genotype matrix and the phenotype vector are standardized column-wise to have mean zero and variance 1. In some contexts, this is also known as the per-SNP correlation or pseudo-correlation with the phenotype.</p> <p>See Also</p> <ul> <li>get_snp_pseudo_corr</li> </ul> <p>Returns:</p> Type Description <p>The standardized marginal beta from the association test of each variant on the phenotype.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.z_score","title":"<code>z_score</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The Z-score from the association test of each SNP on the phenotype.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the Z-score statistic is not available and could not be inferred from available data.</p>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.__init__","title":"<code>__init__(ss_table)</code>","text":"<p>Initialize the summary statistics table.</p> <p>Parameters:</p> Name Type Description Default <code>ss_table</code> <code>DataFrame</code> <p>A pandas DataFrame containing the summary statistics.  !!! seealso \"See Also\" * from_file</p> required Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def __init__(self, ss_table: pd.DataFrame):\n    \"\"\"\n    Initialize the summary statistics table.\n\n    :param ss_table: A pandas DataFrame containing the summary statistics.\n\n    !!! seealso \"See Also\"\n        * [from_file][magenpy.SumstatsTable.SumstatsTable.from_file]\n    \"\"\"\n    self.table: pd.DataFrame = ss_table\n\n    # Check that the table contains some of the required columns (non-exhaustive):\n\n    # Either has SNP or CHR+POS:\n    assert 'SNP' in self.table.columns or all([col in self.table.columns for col in ('CHR', 'POS')])\n    # Assert that the table has at least one of the alleles:\n    assert any([col in self.table.columns for col in ('A1', 'A2')])\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.drop_duplicates","title":"<code>drop_duplicates()</code>","text":"<p>Drop variants with duplicated rsIDs from the summary statistics table.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def drop_duplicates(self):\n    \"\"\"\n    Drop variants with duplicated rsIDs from the summary statistics table.\n    \"\"\"\n\n    self.table = self.table.drop_duplicates(subset=self.identifier_cols, keep=False)\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.filter_by_allele_frequency","title":"<code>filter_by_allele_frequency(min_maf=None, min_mac=None)</code>","text":"<p>Filter variants in the summary statistics table by minimum minor allele frequency or allele count</p> <p>Parameters:</p> Name Type Description Default <code>min_maf</code> <p>Minimum minor allele frequency</p> <code>None</code> <code>min_mac</code> <p>Minimum minor allele count</p> <code>None</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def filter_by_allele_frequency(self, min_maf=None, min_mac=None):\n    \"\"\"\n    Filter variants in the summary statistics table by minimum minor allele frequency or allele count\n    :param min_maf: Minimum minor allele frequency\n    :param min_mac: Minimum minor allele count\n    \"\"\"\n\n    if min_mac or min_maf:\n        maf = self.maf\n        n = self.n_per_snp\n    else:\n        return\n\n    keep_flag = None\n\n    if min_mac and n and maf:\n        mac = (2*maf*n).astype(np.int64)\n        keep_flag = (mac &gt;= min_mac) &amp; ((2*n - mac) &gt;= min_mac)\n\n    if min_maf and maf:\n\n        maf_cond = (maf &gt;= min_maf) &amp; (1. - maf &gt;= min_maf)\n        if keep_flag is not None:\n            keep_flag = keep_flag &amp; maf_cond\n        else:\n            keep_flag = maf_cond\n\n    if keep_flag is not None:\n        self.filter_snps(extract_index=np.where(keep_flag)[0])\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.filter_snps","title":"<code>filter_snps(extract_snps=None, extract_file=None, extract_index=None)</code>","text":"<p>Filter the summary statistics table to keep a subset of SNPs.</p> <p>Parameters:</p> Name Type Description Default <code>extract_snps</code> <p>A list or array of SNP IDs to keep.</p> <code>None</code> <code>extract_file</code> <p>A plink-style file containing the SNP IDs to keep.</p> <code>None</code> <code>extract_index</code> <p>A list or array of the indices of SNPs to retain.</p> <code>None</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def filter_snps(self, extract_snps=None, extract_file=None, extract_index=None):\n    \"\"\"\n    Filter the summary statistics table to keep a subset of SNPs.\n    :param extract_snps: A list or array of SNP IDs to keep.\n    :param extract_file: A plink-style file containing the SNP IDs to keep.\n    :param extract_index: A list or array of the indices of SNPs to retain.\n    \"\"\"\n\n    assert extract_snps is not None or extract_file is not None or extract_index is not None\n\n    if extract_file:\n        from .parsers.misc_parsers import read_snp_filter_file\n        extract_snps = read_snp_filter_file(extract_file)\n\n    if extract_snps is not None:\n        extract_index = intersect_arrays(self.snps, extract_snps, return_index=True)\n\n    if extract_index is not None:\n        self.table = self.table.iloc[extract_index, ].reset_index(drop=True)\n    else:\n        raise Exception(\"To filter a summary statistics table, you must provide \"\n                        \"the list of SNPs, a file containing the list of SNPs, \"\n                        \"or a list of indices to retain.\")\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.from_file","title":"<code>from_file(sumstats_file, sumstats_format=None, parser=None, **parse_kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize a summary statistics table from file. The user must provide either the format for the summary statistics file or the parser object (see <code>parsers.sumstats_parsers</code>).</p> <p>Parameters:</p> Name Type Description Default <code>sumstats_file</code> <p>The path to the summary statistics file.</p> required <code>sumstats_format</code> <p>The format for the summary statistics file. Currently, we support the following summary statistics formats: <code>magenpy</code>, <code>plink1.9</code>, <code>plink</code> or <code>plink2</code>, <code>COJO</code>, <code>fastGWA</code>, <code>SAIGE</code>, <code>GWASCatalog</code> (also denoted as <code>GWAS-SSF</code> and <code>SSF</code>).</p> <code>None</code> <code>parser</code> <p>An instance of SumstatsParser parser, implements basic parsing/conversion functionalities.</p> <code>None</code> <code>parse_kwargs</code> <p>arguments for the pandas <code>read_csv</code> function, such as the delimiter.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A <code>SumstatsTable</code> object initialized from the summary statistics file.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>@classmethod\ndef from_file(cls, sumstats_file, sumstats_format=None, parser=None, **parse_kwargs):\n    \"\"\"\n    Initialize a summary statistics table from file. The user must provide either\n    the format for the summary statistics file or the parser object\n    (see `parsers.sumstats_parsers`).\n\n    :param sumstats_file: The path to the summary statistics file.\n    :param sumstats_format: The format for the summary statistics file. Currently,\n    we support the following summary statistics formats: `magenpy`, `plink1.9`, `plink` or `plink2`,\n    `COJO`, `fastGWA`, `SAIGE`, `GWASCatalog` (also denoted as `GWAS-SSF` and `SSF`).\n    :param parser: An instance of SumstatsParser parser, implements basic parsing/conversion\n    functionalities.\n    :param parse_kwargs: arguments for the pandas `read_csv` function, such as the delimiter.\n\n    :return: A `SumstatsTable` object initialized from the summary statistics file.\n    \"\"\"\n    assert sumstats_format is not None or parser is not None\n\n    from .parsers.sumstats_parsers import (\n        SumstatsParser, Plink1SSParser, Plink2SSParser, COJOSSParser,\n        FastGWASSParser, SSFParser, SaigeSSParser\n    )\n\n    sumstats_format_l = sumstats_format.lower()\n\n    if parser is None:\n        if sumstats_format_l == 'magenpy':\n            parser = SumstatsParser(None, **parse_kwargs)\n        elif sumstats_format_l in ('plink', 'plink2'):\n            parser = Plink2SSParser(None, **parse_kwargs)\n        elif sumstats_format_l == 'plink1.9':\n            parser = Plink1SSParser(None, **parse_kwargs)\n        elif sumstats_format_l == 'cojo':\n            parser = COJOSSParser(None, **parse_kwargs)\n        elif sumstats_format_l == 'fastgwa':\n            parser = FastGWASSParser(None, **parse_kwargs)\n        elif sumstats_format_l in ('ssf', 'gwas-ssf', 'gwascatalog'):\n            parser = SSFParser(None, **parse_kwargs)\n        elif sumstats_format_l == 'saige':\n            parser = SaigeSSParser(None, **parse_kwargs)\n        else:\n            raise KeyError(f\"Parsers for summary statistics format {sumstats_format} are not implemented!\")\n\n    sumstats_table = parser.parse(sumstats_file)\n    return cls(sumstats_table)\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.get_chisq_statistic","title":"<code>get_chisq_statistic()</code>","text":"<p>Returns:</p> Type Description <p>The Chi-Squared statistic from the association test of each variant on the phenotype.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the Chi-Squared statistic is not available and could not be inferred from available data.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def get_chisq_statistic(self):\n    \"\"\"\n    :return: The Chi-Squared statistic from the association test of each variant on the phenotype.\n    :raises KeyError: If the Chi-Squared statistic is not available and could not be inferred from available data.\n    \"\"\"\n    chisq = self.get_col('CHISQ')\n\n    if chisq is not None:\n        return chisq\n    else:\n        z = self.z_score\n        if z is not None:\n            self.table['CHISQ'] = z**2\n        else:\n            p_val = self.p_value\n            if p_val is not None:\n                from scipy.stats import chi2\n\n                self.table['CHISQ'] = chi2.ppf(1. - p_val, 1)\n            else:\n                raise KeyError(\"Chi-Squared statistic is not available!\")\n\n    return self.table['CHISQ'].values\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.get_col","title":"<code>get_col(col_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>col_name</code> <p>The name of the column to extract.</p> required <p>Returns:</p> Type Description <p>The column associated with <code>col_name</code> from summary statistics table.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def get_col(self, col_name):\n    \"\"\"\n    :param col_name: The name of the column to extract.\n\n    :return: The column associated with `col_name` from summary statistics table.\n    \"\"\"\n    if col_name in self.table.columns:\n        return self.table[col_name].values\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.get_snp_pseudo_corr","title":"<code>get_snp_pseudo_corr()</code>","text":"<p>Computes the pseudo-correlation coefficient (standardized beta) between the SNP and the phenotype (X_jTy / N) from GWAS summary statistics.</p> <p>This method uses Equation 15 in Mak et al. 2017</p> <pre><code>$$\nbeta =  z_j / sqrt(n - 1 + z_j^2)\n$$\n</code></pre> <p>Where <code>z_j</code> is the marginal GWAS Z-score.</p> <p>See Also</p> <ul> <li>standardized_marginal_beta</li> </ul> <p>Returns:</p> Type Description <p>The pseudo-correlation coefficient between the SNP and the phenotype.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the Z-scores are not available or the sample size is not available.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def get_snp_pseudo_corr(self):\n    \"\"\"\n\n    Computes the pseudo-correlation coefficient (standardized beta) between the SNP and\n    the phenotype (X_jTy / N) from GWAS summary statistics.\n\n    This method uses Equation 15 in Mak et al. 2017\n\n        $$\n        beta =  z_j / sqrt(n - 1 + z_j^2)\n        $$\n\n    Where `z_j` is the marginal GWAS Z-score.\n\n    !!! seealso \"See Also\"\n        * [standardized_marginal_beta][magenpy.SumstatsTable.SumstatsTable.standardized_marginal_beta]\n\n    :return: The pseudo-correlation coefficient between the SNP and the phenotype.\n    :raises KeyError: If the Z-scores are not available or the sample size is not available.\n\n    \"\"\"\n\n    zsc = self.z_score\n    n = self.n\n\n    if zsc is not None:\n        if n is not None:\n            return zsc / (np.sqrt(n - 1 + zsc**2))\n        else:\n            raise KeyError(\"Sample size is not available!\")\n    else:\n        raise KeyError(\"Z-scores are not available!\")\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.get_yy_per_snp","title":"<code>get_yy_per_snp()</code>","text":"<p>Computes the quantity (y'y)_j/n_j following SBayesR (Lloyd-Jones 2019) and Yang et al. (2012).</p> <p>(y'y)_j/n_j is defined as the empirical variance for continuous phenotypes and may be estimated from GWAS summary statistics by re-arranging the equation for the squared standard error:</p> <pre><code>$$\nSE(b_j)^2 = (Var(y) - Var(x_j)*b_j^2) / (Var(x)*n)\n$$\n</code></pre> <p>Which gives the following estimate:</p> <pre><code>$$\n(y'y)_j / n_j = (n_j - 2)*SE(b_j)^2 + b_j^2\n$$\n</code></pre> <p>Returns:</p> Type Description <p>The quantity (y'y)_j/n_j for each SNP in the summary statistics table.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the marginal betas, standard errors or sample sizes are not available.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def get_yy_per_snp(self):\n    \"\"\"\n    Computes the quantity (y'y)_j/n_j following SBayesR (Lloyd-Jones 2019) and Yang et al. (2012).\n\n    (y'y)_j/n_j is defined as the empirical variance for continuous phenotypes and may be estimated\n    from GWAS summary statistics by re-arranging the equation for the\n    squared standard error:\n\n        $$\n        SE(b_j)^2 = (Var(y) - Var(x_j)*b_j^2) / (Var(x)*n)\n        $$\n\n    Which gives the following estimate:\n\n        $$\n        (y'y)_j / n_j = (n_j - 2)*SE(b_j)^2 + b_j^2\n        $$\n\n    :return: The quantity (y'y)_j/n_j for each SNP in the summary statistics table.\n    :raises KeyError: If the marginal betas, standard errors or sample sizes are not available.\n\n    \"\"\"\n\n    b = self.beta_hat\n    se = self.standard_error\n    n = self.n\n\n    if n is not None:\n        if b is not None:\n            if se is not None:\n                return (n - 2)*se**2 + b**2\n            else:\n                raise KeyError(\"Standard errors are not available!\")\n        else:\n            raise KeyError(\"Marginal betas are not available!\")\n    else:\n        raise KeyError(\"Sample size per SNP is not available!\")\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.infer_a2","title":"<code>infer_a2(reference_table, allow_na=False)</code>","text":"<p>Infer the reference allele A2 (if not present in the SumstatsTable) from a reference table. Make sure that the reference table contains the identifier information for each SNP, in addition to the reference allele A2 and the alternative (i.e. effect) allele A1. It is the user's responsibility to make sure that the reference table matches the summary statistics in terms of the specification of reference vs. alternative. They have to be consistent across the two tables.</p> <p>Parameters:</p> Name Type Description Default <code>reference_table</code> <p>A pandas table containing the following columns at least: SNP identifiers (<code>SNP</code> or <code>CHR</code> &amp; <code>POS</code>) and allele information (<code>A1</code> &amp; <code>A2</code>).</p> required <code>allow_na</code> <p>If True, allow the reference allele to be missing from the final result.</p> <code>False</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def infer_a2(self, reference_table, allow_na=False):\n    \"\"\"\n    Infer the reference allele A2 (if not present in the SumstatsTable)\n    from a reference table. Make sure that the reference table contains the identifier information\n    for each SNP, in addition to the reference allele A2 and the alternative (i.e. effect) allele A1.\n    It is the user's responsibility to make sure that the reference table matches the summary\n    statistics in terms of the specification of reference vs. alternative. They have to be consistent\n    across the two tables.\n\n    :param reference_table: A pandas table containing the following columns at least:\n    SNP identifiers (`SNP` or `CHR` &amp; `POS`) and allele information (`A1` &amp; `A2`).\n    :param allow_na: If True, allow the reference allele to be missing from the final result.\n    \"\"\"\n\n    # Get the identifier columns for this table:\n    id_cols = self.identifier_cols\n\n    # Sanity checks:\n    assert all([col in reference_table.columns for col in id_cols + ['A1', 'A2']])\n\n    # Merge the summary statistics table with the reference table on unique ID:\n    merged_table = self.table[id_cols + ['A1']].merge(\n        reference_table[id_cols + ['A1', 'A2']],\n        how='left',\n        on=id_cols\n    )\n    # If `A1_x` agrees with `A1_y`, then `A2` is indeed the reference allele.\n    # Otherwise, they are flipped and `A1_y` should be the reference allele:\n    merged_table['A2'] = np.where(merged_table['A1_x'] == merged_table['A1_y'],\n                                  merged_table['A2'],\n                                  merged_table['A1_y'])\n\n    # Check that the reference allele could be inferred for all SNPs:\n    if not allow_na and merged_table['A2'].isna().any():\n        raise ValueError(\"The reference allele could not be inferred for some SNPs!\")\n    else:\n        self.table['A2'] = merged_table['A2']\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.infer_snp_id","title":"<code>infer_snp_id(reference_table, allow_na=False)</code>","text":"<p>Infer the SNP ID (if not present in the SumstatsTable) from a reference table. Make sure that the reference table contains the SNP ID, chromosome ID, and position.</p> <p>Parameters:</p> Name Type Description Default <code>reference_table</code> <p>A pandas table containing the following columns at least: <code>SNP</code>, <code>CHR</code>, <code>POS</code>.</p> required <code>allow_na</code> <p>If True, allow the SNP ID to be missing from the final result.</p> <code>False</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def infer_snp_id(self, reference_table, allow_na=False):\n    \"\"\"\n    Infer the SNP ID (if not present in the SumstatsTable) from a reference table.\n    Make sure that the reference table contains the SNP ID, chromosome ID, and position.\n\n    :param reference_table: A pandas table containing the following columns at least:\n    `SNP`, `CHR`, `POS`.\n    :param allow_na: If True, allow the SNP ID to be missing from the final result.\n    \"\"\"\n\n    # Merge the summary statistics table with the reference table:\n    merged_table = self.table[['CHR', 'POS']].merge(reference_table[['SNP', 'CHR', 'POS']], how='left')\n\n    # Check that the SNP ID could be inferred for all SNPs:\n    if not allow_na and merged_table['SNP'].isna().any():\n        raise ValueError(\"The SNP ID could not be inferred for some SNPs!\")\n    else:\n        self.table['SNP'] = merged_table['SNP'].values\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.match","title":"<code>match(reference_table, correct_flips=True)</code>","text":"<p>Match the summary statistics table with a reference table, correcting for potential flips in the effect alleles.</p> <p>Parameters:</p> Name Type Description Default <code>reference_table</code> <p>The SNP table to use as a reference. Must be a pandas table with the following columns: SNP identifier (either <code>SNP</code> or <code>CHR</code> &amp; <code>POS</code>) and allele information (<code>A1</code> &amp; <code>A2</code>).</p> required <code>correct_flips</code> <p>If True, correct the direction of effect size estimates if the effect allele is reversed.</p> <code>True</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def match(self, reference_table, correct_flips=True):\n    \"\"\"\n    Match the summary statistics table with a reference table,\n    correcting for potential flips in the effect alleles.\n\n    :param reference_table: The SNP table to use as a reference. Must be a pandas\n    table with the following columns: SNP identifier (either `SNP` or `CHR` &amp; `POS`) and allele information\n    (`A1` &amp; `A2`).\n    :param correct_flips: If True, correct the direction of effect size\n     estimates if the effect allele is reversed.\n    \"\"\"\n\n    from .utils.model_utils import merge_snp_tables\n\n    self.table = merge_snp_tables(ref_table=reference_table[self.identifier_cols + ['A1', 'A2']],\n                                  alt_table=self.table,\n                                  how='inner',\n                                  correct_flips=correct_flips)\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.run_quality_control","title":"<code>run_quality_control(reference_table=None)</code>","text":"<p>Run quality control checks on the summary statistics table. TODO: Implement quality control checks following recommendations given by Prive et al.: https://doi.org/10.1016/j.xhgg.2022.100136 Given user fine-control over which checks to run and which to skip. Maybe move parts of this implementation to a module in <code>stats</code> (TBD)</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def run_quality_control(self, reference_table=None):\n    \"\"\"\n    Run quality control checks on the summary statistics table.\n    TODO: Implement quality control checks following recommendations given by Prive et al.:\n    https://doi.org/10.1016/j.xhgg.2022.100136\n    Given user fine-control over which checks to run and which to skip.\n    Maybe move parts of this implementation to a module in `stats` (TBD)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.set_sample_size","title":"<code>set_sample_size(n)</code>","text":"<p>Set the sample size for each variant in the summary table. This can be useful when the overall sample size from the GWAS analysis is available, but not on a per-SNP basis.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>A scalar or array of sample sizes for each variant.</p> required Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def set_sample_size(self, n):\n    \"\"\"\n    Set the sample size for each variant in the summary table.\n    This can be useful when the overall sample size from the GWAS analysis is available,\n    but not on a per-SNP basis.\n\n    :param n: A scalar or array of sample sizes for each variant.\n    \"\"\"\n    self.table['N'] = n\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.split_by_chromosome","title":"<code>split_by_chromosome(snps_per_chrom=None)</code>","text":"<p>Split the summary statistics table by chromosome, so that we would have a separate <code>SumstatsTable</code> object for each chromosome.</p> <p>Parameters:</p> Name Type Description Default <code>snps_per_chrom</code> <p>A dictionary where the keys are the chromosome number  and the value is an array or list of SNPs on that chromosome.</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary where the keys are the chromosome number and the value is a <code>SumstatsTable</code> object.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def split_by_chromosome(self, snps_per_chrom=None):\n    \"\"\"\n    Split the summary statistics table by chromosome, so that we would\n    have a separate `SumstatsTable` object for each chromosome.\n    :param snps_per_chrom: A dictionary where the keys are the chromosome number \n    and the value is an array or list of SNPs on that chromosome.\n\n    :return: A dictionary where the keys are the chromosome number and the value is a `SumstatsTable` object.\n    \"\"\"\n\n    if 'CHR' in self.table.columns:\n        chrom_tables = self.table.groupby('CHR')\n        return {\n            c: SumstatsTable(chrom_tables.get_group(c).copy())\n            for c in chrom_tables.groups\n        }\n    elif snps_per_chrom is not None:\n        chrom_dict = {\n            c: SumstatsTable(pd.DataFrame({'SNP': snps}).merge(self.table))\n            for c, snps in snps_per_chrom.items()\n        }\n\n        for c, ss_tab in chrom_dict.items():\n            ss_tab.table['CHR'] = c\n\n        return chrom_dict\n    else:\n        raise Exception(\"To split the summary statistics table by chromosome, \"\n                        \"you must provide the a dictionary mapping chromosome number \"\n                        \"to an array of SNPs `snps_per_chrom`.\")\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.to_file","title":"<code>to_file(output_file, col_subset=None, **to_csv_kwargs)</code>","text":"<p>A convenience method to write the summary statistics table to file.</p> <p>TODO: Add a format argument to this method and allow the user to output summary statistics according to supported formats (e.g. COJO, plink, fastGWA, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <p>The path to the file where to write the summary statistics.</p> required <code>col_subset</code> <p>A subset of the columns to write to file.</p> <code>None</code> <code>to_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>to_csv</code> method.</p> <code>{}</code> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def to_file(self, output_file, col_subset=None, **to_csv_kwargs):\n    \"\"\"\n    A convenience method to write the summary statistics table to file.\n\n    TODO: Add a format argument to this method and allow the user to output summary statistics\n    according to supported formats (e.g. COJO, plink, fastGWA, etc.).\n\n    :param output_file: The path to the file where to write the summary statistics.\n    :param col_subset: A subset of the columns to write to file.\n    :param to_csv_kwargs: Keyword arguments to pass to pandas' `to_csv` method.\n\n    \"\"\"\n\n    if 'sep' not in to_csv_kwargs and 'delimiter' not in to_csv_kwargs:\n        to_csv_kwargs['sep'] = '\\t'\n\n    if 'index' not in to_csv_kwargs:\n        to_csv_kwargs['index'] = False\n\n    table = self.to_table(col_subset)\n    table.to_csv(output_file, **to_csv_kwargs)\n</code></pre>"},{"location":"api/SumstatsTable/#magenpy.SumstatsTable.SumstatsTable.to_table","title":"<code>to_table(col_subset=None)</code>","text":"<p>A convenience method to extract the summary statistics table or subsets of it.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>A list corresponding to a subset of columns to return.</p> <code>None</code> <p>Returns:</p> Type Description <p>A pandas DataFrame containing the summary statistics with the requested column subset.</p> Source code in <code>magenpy/SumstatsTable.py</code> <pre><code>def to_table(self, col_subset=None):\n    \"\"\"\n    A convenience method to extract the summary statistics table or subsets of it.\n\n    :param col_subset: A list corresponding to a subset of columns to return.\n\n    :return: A pandas DataFrame containing the summary statistics with the requested column subset.\n    \"\"\"\n\n    col_subset = col_subset or ['CHR', 'SNP', 'POS', 'A1', 'A2', 'MAF',\n                                'N', 'BETA', 'Z', 'SE', 'PVAL']\n\n    # Because some of the quantities that the user needs may be need to be\n    # computed, we separate the column subset into those that are already\n    # present in the table and those that are not (but can still be computed\n    # from other summary statistics):\n\n    present_cols = list(set(col_subset).intersection(set(self.table.columns)))\n    non_present_cols = list(set(col_subset) - set(present_cols))\n\n    if len(present_cols) &gt; 0:\n        table = self.table[present_cols].copy()\n    else:\n        table = pd.DataFrame({c: [] for c in non_present_cols})\n\n    for col in non_present_cols:\n\n        if col == 'Z':\n            table['Z'] = self.z_score\n        elif col == 'PVAL':\n            table['PVAL'] = self.p_value\n        elif col == 'LOG10_PVAL':\n            table['NLOG10_PVAL'] = self.negative_log10_p_value\n        elif col == 'CHISQ':\n            table['CHISQ'] = self.get_chisq_statistic()\n        elif col == 'MAF_VAR':\n            table['MAF_VAR'] = self.maf_var\n        elif col == 'STD_BETA':\n            table['STD_BETA'] = self.get_snp_pseudo_corr()\n        else:\n            warnings.warn(f\"Column '{col}' is not available in the summary statistics table!\")\n\n    return table[list(col_subset)]\n</code></pre>"},{"location":"api/overview/","title":"API Reference","text":""},{"location":"api/overview/#data-structures","title":"Data Structures","text":"<ul> <li>GWADataLoader: A general class for loading multiple statistical genetics data sources and harmonizing them for downstream analyses.</li> <li>GenotypeMatrix: A class for representing on-disk genotype matrices. It provides  interfaces for querying / manipulating / and performing computations on genotype data.</li> <li>LDMatrix: A class for representing on-disk Linkage-Disequilibrium (LD) matrices. It provides  interfaces for querying / manipulating / and performing computations on LD data.</li> <li>SampleTable: A class for representing data about samples (individuals), including covariates, phenotypes, and other sample-specific metadata.</li> <li>SumstatsTable: A class for representing summary statistics data from a GWAS study. It provides interfaces for querying / manipulating / and performing computations on summary statistics data.</li> <li>AnnotationMatrix: A class for representing variant annotations (e.g. functional annotations,  pathogenicity scores, etc.) for a set of variants. It provides interfaces for querying / manipulating / and performing computations on annotation data.</li> </ul>"},{"location":"api/overview/#simulation","title":"Simulation","text":"<ul> <li>PhenotypeSimulator: A general class for simulating phenotypes based on genetic data.</li> </ul>"},{"location":"api/overview/#parsers","title":"Parsers","text":"<ul> <li>Sumstats Parsers: A collection of parsers for reading GWAS summary statistics files in various formats.</li> <li>Annotation Parsers: A collection of parsers for reading variant annotation files in various formats.</li> <li>Plink Parsers: A collection of parsers for reading PLINK files (BED/BIM/FAM) and other PLINK-related formats.</li> </ul>"},{"location":"api/overview/#statistics","title":"Statistics","text":""},{"location":"api/overview/#plotting","title":"Plotting","text":"<ul> <li>GWAS plots: Functions for plotting various quantities / results from GWAS studies.</li> <li>LD plots: Functions for plotting various quantities from LD matrices.</li> </ul>"},{"location":"api/overview/#utilities","title":"Utilities","text":"<ul> <li>Compute utilities: Utilities for computing various statistics / quantities over python data structures. </li> <li>Data utilities: Utilities for downloading and processing relevant data.</li> <li>Executors: A collection of classes for interfacing with third party software, such as <code>plink</code>.</li> <li>Model utilities: Utilities for merging / aligning / filtering GWAS data sources.</li> <li>System utilities: Utilities for interfacing with the system environment (e.g. file I/O, environment variables, etc.).</li> </ul>"},{"location":"api/overview/#data","title":"Data","text":""},{"location":"api/parsers/annotation_parsers/","title":"Annotation parsers","text":""},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.AnnotationMatrixParser","title":"<code>AnnotationMatrixParser</code>","text":"<p>               Bases: <code>object</code></p> <p>A generic annotation matrix parser class.</p> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>class AnnotationMatrixParser(object):\n    \"\"\"\n    A generic annotation matrix parser class.\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        :param col_name_converter: A dictionary mapping column names\n        in the original table to magenpy's column names for the various\n        SNP features in the annotation matrix.\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n        \"\"\"\n\n        self.col_name_converter = col_name_converter\n        self.read_csv_kwargs = read_csv_kwargs\n\n        # If the delimiter is not specified, assume whitespace by default:\n        if 'sep' not in self.read_csv_kwargs and 'delimiter' not in self.read_csv_kwargs:\n            self.read_csv_kwargs['sep'] = r'\\s+'\n\n    def parse(self, annotation_file, drop_na=True):\n        \"\"\"\n        Parse the annotation matrix file\n        :param annotation_file: The path to the annotation file.\n        :param drop_na: Drop any entries with missing values.\n        \"\"\"\n\n        try:\n            df = pd.read_csv(annotation_file, **self.read_csv_kwargs)\n        except Exception as e:\n            raise e\n\n        if drop_na:\n            df = df.dropna()\n\n        if self.col_name_converter is not None:\n            df.rename(columns=self.col_name_converter, inplace=True)\n\n        df.sort_values(['CHR', 'POS'], inplace=True)\n\n        annotations = [ann for ann in df.columns if ann not in ('CHR', 'SNP', 'POS')]\n\n        return df, annotations\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.AnnotationMatrixParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names for the various SNP features in the annotation matrix.</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> <code>{}</code> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    :param col_name_converter: A dictionary mapping column names\n    in the original table to magenpy's column names for the various\n    SNP features in the annotation matrix.\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n    \"\"\"\n\n    self.col_name_converter = col_name_converter\n    self.read_csv_kwargs = read_csv_kwargs\n\n    # If the delimiter is not specified, assume whitespace by default:\n    if 'sep' not in self.read_csv_kwargs and 'delimiter' not in self.read_csv_kwargs:\n        self.read_csv_kwargs['sep'] = r'\\s+'\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.AnnotationMatrixParser.parse","title":"<code>parse(annotation_file, drop_na=True)</code>","text":"<p>Parse the annotation matrix file</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <p>The path to the annotation file.</p> required <code>drop_na</code> <p>Drop any entries with missing values.</p> <code>True</code> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>def parse(self, annotation_file, drop_na=True):\n    \"\"\"\n    Parse the annotation matrix file\n    :param annotation_file: The path to the annotation file.\n    :param drop_na: Drop any entries with missing values.\n    \"\"\"\n\n    try:\n        df = pd.read_csv(annotation_file, **self.read_csv_kwargs)\n    except Exception as e:\n        raise e\n\n    if drop_na:\n        df = df.dropna()\n\n    if self.col_name_converter is not None:\n        df.rename(columns=self.col_name_converter, inplace=True)\n\n    df.sort_values(['CHR', 'POS'], inplace=True)\n\n    annotations = [ann for ann in df.columns if ann not in ('CHR', 'SNP', 'POS')]\n\n    return df, annotations\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.LDSCAnnotationMatrixParser","title":"<code>LDSCAnnotationMatrixParser</code>","text":"<p>               Bases: <code>AnnotationMatrixParser</code></p> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>class LDSCAnnotationMatrixParser(AnnotationMatrixParser):\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        :param col_name_converter: A dictionary mapping column names\n        in the original table to magenpy's column names for the various\n        SNP features in the annotation matrix.\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n\n        super().__init__(col_name_converter, **read_csv_kwargs)\n        self.col_name_converter = self.col_name_converter or {}\n        self.col_name_converter.update(\n            {\n                'BP': 'POS'\n            }\n        )\n\n    def parse(self, annotation_file, drop_na=True):\n        \"\"\"\n        Parse the annotation matrix file\n        :param annotation_file: The path to the annotation file.\n        :param drop_na: Drop any entries with missing values.\n        \"\"\"\n\n        df, annotations = super().parse(annotation_file, drop_na=drop_na)\n\n        df = df.drop(['CM', 'base'], axis=1)\n        annotations = [ann for ann in annotations if ann not in ('CM', 'base')]\n\n        return df, annotations\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.LDSCAnnotationMatrixParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names for the various SNP features in the annotation matrix.</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    :param col_name_converter: A dictionary mapping column names\n    in the original table to magenpy's column names for the various\n    SNP features in the annotation matrix.\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n\n    super().__init__(col_name_converter, **read_csv_kwargs)\n    self.col_name_converter = self.col_name_converter or {}\n    self.col_name_converter.update(\n        {\n            'BP': 'POS'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.LDSCAnnotationMatrixParser.parse","title":"<code>parse(annotation_file, drop_na=True)</code>","text":"<p>Parse the annotation matrix file</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <p>The path to the annotation file.</p> required <code>drop_na</code> <p>Drop any entries with missing values.</p> <code>True</code> Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>def parse(self, annotation_file, drop_na=True):\n    \"\"\"\n    Parse the annotation matrix file\n    :param annotation_file: The path to the annotation file.\n    :param drop_na: Drop any entries with missing values.\n    \"\"\"\n\n    df, annotations = super().parse(annotation_file, drop_na=drop_na)\n\n    df = df.drop(['CM', 'base'], axis=1)\n    annotations = [ann for ann in annotations if ann not in ('CM', 'base')]\n\n    return df, annotations\n</code></pre>"},{"location":"api/parsers/annotation_parsers/#magenpy.parsers.annotation_parsers.parse_annotation_bed_file","title":"<code>parse_annotation_bed_file(annot_bed_file)</code>","text":"<p>Parse an annotation bed file in the format specified by Ensemble: https://uswest.ensembl.org/info/website/upload/bed.html</p> <p>The file contains 3-12 columns, starting with Chromosome, start_coordinate, end_coordinate, etc. After reading the raw file, we let pandas infer whether the file has a header or not and we standardize the names of the first 3 columns and convert the chromosome column into an integer.</p> <p>Parameters:</p> Name Type Description Default <code>annot_bed_file</code> <code>str</code> <p>The path to the annotation BED file.</p> required Source code in <code>magenpy/parsers/annotation_parsers.py</code> <pre><code>def parse_annotation_bed_file(annot_bed_file):\n    \"\"\"\n    Parse an annotation bed file in the format specified by Ensemble:\n    https://uswest.ensembl.org/info/website/upload/bed.html\n\n    The file contains 3-12 columns, starting with Chromosome, start_coordinate, end_coordinate, etc.\n    After reading the raw file, we let pandas infer whether the file has a header or not and we\n    standardize the names of the first 3 columns and convert the chromosome column into an integer.\n\n    :param annot_bed_file: The path to the annotation BED file.\n    :type annot_bed_file: str\n    \"\"\"\n\n    try:\n        annot_bed = pd.read_csv(annot_bed_file, usecols=[0, 1, 2],\n                                sep=r'\\s+',\n                                names=['CHR', 'Start', 'End'])\n    except Exception as e:\n        raise e\n\n    annot_bed['CHR'] = annot_bed['CHR'].str.replace('chr', '').astype(int)\n\n    return annot_bed\n</code></pre>"},{"location":"api/parsers/misc_parsers/","title":"Misc parsers","text":""},{"location":"api/parsers/misc_parsers/#magenpy.parsers.misc_parsers.parse_cluster_assignment_file","title":"<code>parse_cluster_assignment_file(cluster_assignment_file)</code>","text":"<p>Parses a file that maps each individual in the sample table to a cluster, and returns the pandas dataframe. The expected file should be whitespace delimited and contain three columns: FID, IID, and Cluster</p> <p>Parameters:</p> Name Type Description Default <code>cluster_assignment_file</code> <code>str</code> <p>The path to the cluster assignment file.</p> required <p>Returns:</p> Type Description <p>A pandas dataframe with the cluster assignments.</p> Source code in <code>magenpy/parsers/misc_parsers.py</code> <pre><code>def parse_cluster_assignment_file(cluster_assignment_file):\n    \"\"\"\n    Parses a file that maps each individual in the sample table to a cluster,\n    and returns the pandas dataframe. The expected file should be whitespace delimited\n    and contain three columns: FID, IID, and Cluster\n\n    :param cluster_assignment_file: The path to the cluster assignment file.\n    :type cluster_assignment_file: str\n\n    :return: A pandas dataframe with the cluster assignments.\n    \"\"\"\n    try:\n        clusters = pd.read_csv(cluster_assignment_file, sep=r'\\s+')\n        clusters.columns = ['FID', 'IID', 'Cluster']\n    except Exception as e:\n        raise e\n\n    return clusters\n</code></pre>"},{"location":"api/parsers/misc_parsers/#magenpy.parsers.misc_parsers.parse_ld_block_data","title":"<code>parse_ld_block_data(ldb_file_path)</code>","text":"<p>This function takes a path to a file with the LD blocks and returns a dictionary with the chromosome ID and a list of the start and end positions for the blocks in that chromosome. The parser assumes that the LD block files have the ldetect format: https://bitbucket.org/nygcresearch/ldetect-data/src/master/</p> <p>Parameters:</p> Name Type Description Default <code>ldb_file_path</code> <code>str</code> <p>The path (or URL) to the LD blocks file</p> required <p>Returns:</p> Type Description <p>A dictionary with the chromosome ID and a list of the start and end positions for the blocks in that chromosome.</p> Source code in <code>magenpy/parsers/misc_parsers.py</code> <pre><code>def parse_ld_block_data(ldb_file_path):\n    \"\"\"\n    This function takes a path to a file with the LD blocks\n    and returns a dictionary with the chromosome ID and a list of the\n    start and end positions for the blocks in that chromosome.\n    The parser assumes that the LD block files have the ldetect format:\n    https://bitbucket.org/nygcresearch/ldetect-data/src/master/\n\n    :param ldb_file_path: The path (or URL) to the LD blocks file\n    :type ldb_file_path: str\n\n    :return: A dictionary with the chromosome ID and a list of the start\n    and end positions for the blocks in that chromosome.\n    \"\"\"\n\n    ld_blocks = {}\n\n    df = pd.read_csv(ldb_file_path, sep=r'\\s+')\n\n    # Drop rows with missing values:\n    df.dropna(inplace=True)\n    df = df.loc[(df.start != 'None') &amp; (df.stop != 'None')]\n\n    # Cast the start/stop columns to integers:\n    df = df.astype({'chr': str, 'start': np.int32, 'stop': np.int32})\n\n    # Sort the dataframe:\n    df = df.sort_values('start')\n\n    for chrom in df['chr'].unique():\n        ld_blocks[int(chrom.replace('chr', ''))] = df.loc[df['chr'] == chrom, ['start', 'stop']].values\n\n    return ld_blocks\n</code></pre>"},{"location":"api/parsers/misc_parsers/#magenpy.parsers.misc_parsers.read_sample_filter_file","title":"<code>read_sample_filter_file(filename)</code>","text":"<p>Read plink-style file listing sample IDs. The file should not have a header, be tab-separated, and has two columns corresponding to Family ID (FID) and Individual ID (IID). You may also pass a file with a single-column of Individual IDs instead.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the file containing the sample IDs</p> required <p>Returns:</p> Type Description <p>A numpy array with the sample IDs</p> Source code in <code>magenpy/parsers/misc_parsers.py</code> <pre><code>def read_sample_filter_file(filename):\n    \"\"\"\n    Read plink-style file listing sample IDs.\n    The file should not have a header, be tab-separated, and has two\n    columns corresponding to Family ID (FID) and Individual ID (IID).\n    You may also pass a file with a single-column of Individual IDs instead.\n\n    :param filename: The path to the file containing the sample IDs\n    :type filename: str\n\n    :return: A numpy array with the sample IDs\n    \"\"\"\n\n    keep_list = pd.read_csv(filename, sep=\"\\t\", header=None).values\n\n    if keep_list.shape[1] == 1:\n        return keep_list[:, 0]\n    elif keep_list.shape[1] == 2:\n        return keep_list[:, 1]\n</code></pre>"},{"location":"api/parsers/misc_parsers/#magenpy.parsers.misc_parsers.read_snp_filter_file","title":"<code>read_snp_filter_file(filename, snp_id_col=0)</code>","text":"<p>Read plink-style file listing variant IDs. The file should not have a header and only has a single column.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the file containing the SNP IDs</p> required <code>snp_id_col</code> <code>int</code> <p>The column index containing the SNP IDs</p> <code>0</code> <p>Returns:</p> Type Description <p>A numpy array with the SNP IDs</p> Source code in <code>magenpy/parsers/misc_parsers.py</code> <pre><code>def read_snp_filter_file(filename, snp_id_col=0):\n    \"\"\"\n    Read plink-style file listing variant IDs.\n    The file should not have a header and only has a single column.\n\n    :param filename: The path to the file containing the SNP IDs\n    :type filename: str\n    :param snp_id_col: The column index containing the SNP IDs\n    :type snp_id_col: int\n\n    :return keep_list: A numpy array with the SNP IDs\n    \"\"\"\n\n    try:\n        keep_list = pd.read_csv(filename, sep=\"\\t\", header=None).values[:, snp_id_col]\n    except Exception as e:\n        raise e\n\n    return keep_list\n</code></pre>"},{"location":"api/parsers/plink_parsers/","title":"Plink parsers","text":""},{"location":"api/parsers/plink_parsers/#magenpy.parsers.plink_parsers.parse_bim_file","title":"<code>parse_bim_file(plink_bfile)</code>","text":"<p>From the plink documentation: https://www.cog-genomics.org/plink/1.9/formats#bim</p> <pre><code>A text file with no header line, and one line per variant with the following six fields:\n\n- Chromosome code (either an integer, or 'X'/'Y'/'XY'/'MT'; '0' indicates unknown) or name\n- Variant identifier\n- Position in morgans or centimorgans (safe to use dummy value of '0')\n- Base-pair coordinate (1-based; limited to 231-2)\n- Allele 1 (corresponding to clear bits in .bed; usually minor)\n- Allele 2 (corresponding to set bits in .bed; usually major)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>plink_bfile</code> <code>str</code> <p>The path to the plink bfile (with or without the extension).</p> required Source code in <code>magenpy/parsers/plink_parsers.py</code> <pre><code>def parse_bim_file(plink_bfile):\n    \"\"\"\n    From the plink documentation:\n    https://www.cog-genomics.org/plink/1.9/formats#bim\n\n        A text file with no header line, and one line per variant with the following six fields:\n\n        - Chromosome code (either an integer, or 'X'/'Y'/'XY'/'MT'; '0' indicates unknown) or name\n        - Variant identifier\n        - Position in morgans or centimorgans (safe to use dummy value of '0')\n        - Base-pair coordinate (1-based; limited to 231-2)\n        - Allele 1 (corresponding to clear bits in .bed; usually minor)\n        - Allele 2 (corresponding to set bits in .bed; usually major)\n\n    :param plink_bfile: The path to the plink bfile (with or without the extension).\n    :type plink_bfile: str\n    \"\"\"\n\n    if '.bim' not in plink_bfile:\n        if '.bed' in plink_bfile:\n            plink_bfile = plink_bfile.replace('.bed', '.bim')\n        else:\n            plink_bfile = plink_bfile + '.bim'\n\n    bim_df = pd.read_csv(plink_bfile,\n                         sep=r'\\s+',\n                         names=['CHR', 'SNP', 'cM', 'POS', 'A1', 'A2'],\n                         dtype={\n                             'CHR': int,\n                             'SNP': str,\n                             'cM': np.float32,\n                             'POS': np.int32,\n                             'A1': str,\n                             'A2': str\n                         })\n\n    return bim_df\n</code></pre>"},{"location":"api/parsers/plink_parsers/#magenpy.parsers.plink_parsers.parse_fam_file","title":"<code>parse_fam_file(plink_bfile)</code>","text":"<p>From the plink documentation: https://www.cog-genomics.org/plink/1.9/formats#fam</p> <pre><code>A text file with no header line, and one line per sample with the following six fields:\n\n- Family ID ('FID')\n- Within-family ID ('IID'; cannot be '0')\n- Within-family ID of father ('0' if father isn't in dataset)\n- Within-family ID of mother ('0' if mother isn't in dataset)\n- Sex code ('1' = male, '2' = female, '0' = unknown)\n- Phenotype value ('1' = control, '2' = case, '-9'/'0'/non-numeric = missing data if case/control)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>plink_bfile</code> <code>str</code> <p>The path to the plink bfile (with or without the extension).</p> required Source code in <code>magenpy/parsers/plink_parsers.py</code> <pre><code>def parse_fam_file(plink_bfile):\n    \"\"\"\n    From the plink documentation:\n    https://www.cog-genomics.org/plink/1.9/formats#fam\n\n        A text file with no header line, and one line per sample with the following six fields:\n\n        - Family ID ('FID')\n        - Within-family ID ('IID'; cannot be '0')\n        - Within-family ID of father ('0' if father isn't in dataset)\n        - Within-family ID of mother ('0' if mother isn't in dataset)\n        - Sex code ('1' = male, '2' = female, '0' = unknown)\n        - Phenotype value ('1' = control, '2' = case, '-9'/'0'/non-numeric = missing data if case/control)\n\n    :param plink_bfile: The path to the plink bfile (with or without the extension).\n    :type plink_bfile: str\n    \"\"\"\n\n    if '.fam' not in plink_bfile:\n        if '.bed' in plink_bfile:\n            plink_bfile = plink_bfile.replace('.bed', '.fam')\n        else:\n            plink_bfile = plink_bfile + '.fam'\n\n    fam_df = pd.read_csv(plink_bfile,\n                         sep=r'\\s+',\n                         usecols=list(range(6)),\n                         names=['FID', 'IID', 'fatherID', 'motherID', 'sex', 'phenotype'],\n                         dtype={'FID': str,\n                                'IID': str,\n                                'fatherID': str,\n                                'motherID': str,\n                                'sex': np.float32,\n                                'phenotype': np.float32\n                                },\n                         na_values={\n                             'phenotype': [-9.],\n                             'sex': [0]\n                         })\n\n    # If the phenotype is all null or unknown, drop the column:\n    if fam_df['phenotype'].isnull().all():\n        fam_df.drop('phenotype', axis=1, inplace=True)\n\n    # If the sex column is all null or unknown, drop the column:\n    if fam_df['sex'].isnull().all():\n        fam_df.drop('sex', axis=1, inplace=True)\n\n    return fam_df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/","title":"Sumstats parsers","text":""},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.COJOSSParser","title":"<code>COJOSSParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics files generated by the <code>COJO</code> software.</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>Plink1SSParser</li> <li>FastGWASSParser</li> <li>SSFParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class COJOSSParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics files generated by the `COJO` software.\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n\n        Initialize the COJO summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        self.col_name_converter.update(\n            {\n                'freq': 'MAF',\n                'b': 'BETA',\n                'se': 'SE',\n                'p': 'PVAL'\n            }\n        )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.COJOSSParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the COJO summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n\n    Initialize the COJO summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    self.col_name_converter.update(\n        {\n            'freq': 'MAF',\n            'b': 'BETA',\n            'se': 'SE',\n            'p': 'PVAL'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.FastGWASSParser","title":"<code>FastGWASSParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics files generated by the <code>FastGWA</code> software.</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>Plink1SSParser</li> <li>COJOSSParser</li> <li>SSFParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class FastGWASSParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics files generated by the `FastGWA` software.\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        self.col_name_converter.update(\n            {\n                'AF1': 'MAF',\n                'P': 'PVAL'\n            }\n        )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.FastGWASSParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    self.col_name_converter.update(\n        {\n            'AF1': 'MAF',\n            'P': 'PVAL'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.Plink1SSParser","title":"<code>Plink1SSParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics files generated by <code>plink1.9</code>.</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>COJOSSParser</li> <li>FastGWASSParser</li> <li>SSFParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class Plink1SSParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics files generated by `plink1.9`.\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        Initialize the `plink1.9` summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        self.col_name_converter.update(\n            {\n                'P': 'PVAL',\n                'NMISS': 'N',\n                'STAT': 'Z',\n                'BP': 'POS'\n            }\n        )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.Plink1SSParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the <code>plink1.9</code> summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    Initialize the `plink1.9` summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    self.col_name_converter.update(\n        {\n            'P': 'PVAL',\n            'NMISS': 'N',\n            'STAT': 'Z',\n            'BP': 'POS'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.Plink2SSParser","title":"<code>Plink2SSParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics files generated by <code>plink2</code>.</p> <p>See Also</p> <ul> <li>Plink1SSParser</li> <li>COJOSSParser</li> <li>FastGWASSParser</li> <li>SSFParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class Plink2SSParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics files generated by `plink2`.\n\n    !!! seealso \"See Also\"\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n\n        Initialize the `plink2` summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        self.col_name_converter.update(\n            {\n                '#CHROM': 'CHR',\n                'ID': 'SNP',\n                'P': 'PVAL',\n                'OBS_CT': 'N',\n                'A1_FREQ': 'MAF',\n                'T_STAT': 'Z',\n                'Z_STAT': 'Z'\n            }\n        )\n\n    def parse(self, file_name, drop_na=True):\n        \"\"\"\n        Parse a summary statistics file.\n        :param file_name: The path to the summary statistics file.\n        :param drop_na: Drop any entries with missing values.\n\n        :return: A pandas DataFrame containing the parsed summary statistics.\n        \"\"\"\n\n        df = super().parse(file_name, drop_na=drop_na)\n\n        if 'A2' not in df.columns:\n            try:\n                if 'ALT1' in df.columns:\n                    df['A2'] = np.where(df['A1'] == df['ALT1'], df['REF'], df['ALT1'])\n                elif 'ALT' in df.columns:\n                    df['A2'] = np.where(df['A1'] == df['ALT'], df['REF'], df['ALT'])\n                else:\n                    warnings.warn(\"The reference allele A2 could not be inferred \"\n                                  \"from the summary statistics file!\")\n            except KeyError:\n                warnings.warn(\"The reference allele A2 could not be inferred \"\n                              \"from the summary statistics file! Some of the columns needed to infer \"\n                              \"the A2 allele are missing or coded differently than what we expect.\")\n\n        return df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.Plink2SSParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the <code>plink2</code> summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n\n    Initialize the `plink2` summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    self.col_name_converter.update(\n        {\n            '#CHROM': 'CHR',\n            'ID': 'SNP',\n            'P': 'PVAL',\n            'OBS_CT': 'N',\n            'A1_FREQ': 'MAF',\n            'T_STAT': 'Z',\n            'Z_STAT': 'Z'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.Plink2SSParser.parse","title":"<code>parse(file_name, drop_na=True)</code>","text":"<p>Parse a summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>The path to the summary statistics file.</p> required <code>drop_na</code> <p>Drop any entries with missing values.</p> <code>True</code> <p>Returns:</p> Type Description <p>A pandas DataFrame containing the parsed summary statistics.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def parse(self, file_name, drop_na=True):\n    \"\"\"\n    Parse a summary statistics file.\n    :param file_name: The path to the summary statistics file.\n    :param drop_na: Drop any entries with missing values.\n\n    :return: A pandas DataFrame containing the parsed summary statistics.\n    \"\"\"\n\n    df = super().parse(file_name, drop_na=drop_na)\n\n    if 'A2' not in df.columns:\n        try:\n            if 'ALT1' in df.columns:\n                df['A2'] = np.where(df['A1'] == df['ALT1'], df['REF'], df['ALT1'])\n            elif 'ALT' in df.columns:\n                df['A2'] = np.where(df['A1'] == df['ALT'], df['REF'], df['ALT'])\n            else:\n                warnings.warn(\"The reference allele A2 could not be inferred \"\n                              \"from the summary statistics file!\")\n        except KeyError:\n            warnings.warn(\"The reference allele A2 could not be inferred \"\n                          \"from the summary statistics file! Some of the columns needed to infer \"\n                          \"the A2 allele are missing or coded differently than what we expect.\")\n\n    return df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SSFParser","title":"<code>SSFParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics that are formatted according  to the standardized summary statistics format adopted by the GWAS Catalog. This format is  sometimes denoted as <code>GWAS-SSF</code>.</p> <p>Reference and details: https://github.com/EBISPOT/gwas-summary-statistics-standard</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>Plink1SSParser</li> <li>COJOSSParser</li> <li>FastGWASSParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class SSFParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics that are formatted according\n     to the standardized summary statistics format adopted by the GWAS Catalog. This format is\n     sometimes denoted as `GWAS-SSF`.\n\n    Reference and details:\n    https://github.com/EBISPOT/gwas-summary-statistics-standard\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n\n        Initialize the standardized summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        self.col_name_converter.update(\n            {\n                'chromosome': 'CHR',\n                'base_pair_location': 'POS',\n                'rsid': 'SNP',\n                'effect_allele': 'A1',\n                'other_allele': 'A2',\n                'beta': 'BETA',\n                'standard_error': 'SE',\n                'effect_allele_frequency': 'MAF',\n                'p_value': 'PVAL',\n                'n': 'N'\n            }\n        )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SSFParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the standardized summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n\n    Initialize the standardized summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    self.col_name_converter.update(\n        {\n            'chromosome': 'CHR',\n            'base_pair_location': 'POS',\n            'rsid': 'SNP',\n            'effect_allele': 'A1',\n            'other_allele': 'A2',\n            'beta': 'BETA',\n            'standard_error': 'SE',\n            'effect_allele_frequency': 'MAF',\n            'p_value': 'PVAL',\n            'n': 'N'\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SaigeSSParser","title":"<code>SaigeSSParser</code>","text":"<p>               Bases: <code>SumstatsParser</code></p> <p>A specialized class for parsing GWAS summary statistics files generated by the <code>SAIGE</code> software. Reference and details: https://saigegit.github.io/SAIGE-doc/docs/single_step2.html</p> <p>TODO: Ensure that the column names are correct across different trait types and the inference of the sample size is correct.</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>Plink1SSParser</li> <li>COJOSSParser</li> <li>FastGWASSParser</li> <li>SSFParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class SaigeSSParser(SumstatsParser):\n    \"\"\"\n    A specialized class for parsing GWAS summary statistics files generated by the `SAIGE` software.\n    Reference and details:\n    https://saigegit.github.io/SAIGE-doc/docs/single_step2.html\n\n    TODO: Ensure that the column names are correct across different trait types\n    and the inference of the sample size is correct.\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        Initialize the `SAIGE` summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n        super().__init__(col_name_converter, **read_csv_kwargs)\n\n        self.col_name_converter = self.col_name_converter or {}\n\n        # NOTE: SAIGE considers Allele2 to be the effect allele, so\n        # we switch their designation here:\n        self.col_name_converter.update(\n            {\n                'MarkerID': 'SNP',\n                'Allele1': 'A2',\n                'Allele2': 'A1',\n                'AF_Allele2': 'MAF',\n                'AC_Allele2': 'MAC',\n                'Tstat': 'Z',\n                'p.value': 'PVAL',\n            }\n        )\n\n    def parse(self, file_name, drop_na=True):\n        \"\"\"\n        Parse the summary statistics file.\n        :param file_name: The path to the summary statistics file.\n        :param drop_na: Drop any entries with missing values.\n\n        :return: A pandas DataFrame containing the parsed summary statistics.\n        \"\"\"\n\n        df = super().parse(file_name, drop_na=drop_na)\n\n        # Infer the sample size N\n        df['N'] = df['MAC'] / (2.*df['MAF'])\n\n        return df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SaigeSSParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the <code>SAIGE</code> summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    Initialize the `SAIGE` summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n    super().__init__(col_name_converter, **read_csv_kwargs)\n\n    self.col_name_converter = self.col_name_converter or {}\n\n    # NOTE: SAIGE considers Allele2 to be the effect allele, so\n    # we switch their designation here:\n    self.col_name_converter.update(\n        {\n            'MarkerID': 'SNP',\n            'Allele1': 'A2',\n            'Allele2': 'A1',\n            'AF_Allele2': 'MAF',\n            'AC_Allele2': 'MAC',\n            'Tstat': 'Z',\n            'p.value': 'PVAL',\n        }\n    )\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SaigeSSParser.parse","title":"<code>parse(file_name, drop_na=True)</code>","text":"<p>Parse the summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>The path to the summary statistics file.</p> required <code>drop_na</code> <p>Drop any entries with missing values.</p> <code>True</code> <p>Returns:</p> Type Description <p>A pandas DataFrame containing the parsed summary statistics.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def parse(self, file_name, drop_na=True):\n    \"\"\"\n    Parse the summary statistics file.\n    :param file_name: The path to the summary statistics file.\n    :param drop_na: Drop any entries with missing values.\n\n    :return: A pandas DataFrame containing the parsed summary statistics.\n    \"\"\"\n\n    df = super().parse(file_name, drop_na=drop_na)\n\n    # Infer the sample size N\n    df['N'] = df['MAC'] / (2.*df['MAF'])\n\n    return df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SumstatsParser","title":"<code>SumstatsParser</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for parsing summary statistics files that are written by statistical genetics software for Genome-wide Association testing. A common challenge is the fact that different software tools output summary statistics in different formats and with different column names. Thus, this class provides a common interface for parsing summary statistics files from different software tools and aims to make this process as seamless as possible.</p> <p>The class is designed to be extensible, so that users can easily add new parsers for different software tools.</p> <p>See Also</p> <ul> <li>Plink2SSParser</li> <li>Plink1SSParser</li> <li>COJOSSParser</li> <li>FastGWASSParser</li> <li>SSFParser</li> <li>SaigeSSParser</li> </ul> <p>Attributes:</p> Name Type Description <code>col_name_converter</code> <p>A dictionary mapping column names in the original table to magenpy's column names.</p> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' <code>read_csv</code>.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>class SumstatsParser(object):\n    \"\"\"\n    A wrapper class for parsing summary statistics files that are written by statistical genetics software\n    for Genome-wide Association testing. A common challenge is the fact that different software tools\n    output summary statistics in different formats and with different column names. Thus, this class\n    provides a common interface for parsing summary statistics files from different software tools\n    and aims to make this process as seamless as possible.\n\n    The class is designed to be extensible, so that users can easily add new parsers for different software tools.\n\n    !!! seealso \"See Also\"\n        * [Plink2SSParser][magenpy.parsers.sumstats_parsers.Plink2SSParser]\n        * [Plink1SSParser][magenpy.parsers.sumstats_parsers.Plink1SSParser]\n        * [COJOSSParser][magenpy.parsers.sumstats_parsers.COJOSSParser]\n        * [FastGWASSParser][magenpy.parsers.sumstats_parsers.FastGWASSParser]\n        * [SSFParser][magenpy.parsers.sumstats_parsers.SSFParser]\n        * [SaigeSSParser][magenpy.parsers.sumstats_parsers.SaigeSSParser]\n\n    :ivar col_name_converter: A dictionary mapping column names in the original table to magenpy's column names.\n    :ivar read_csv_kwargs: Keyword arguments to pass to pandas' `read_csv`.\n\n    \"\"\"\n\n    def __init__(self, col_name_converter=None, **read_csv_kwargs):\n        \"\"\"\n        Initialize the summary statistics parser.\n\n        :param col_name_converter: A dictionary/string mapping column names\n        in the original table to magenpy's column names for the various\n        summary statistics. If a string, it should be a comma-separated list of\n        key-value pairs (e.g. 'rsid=SNP,pos=POS').\n        :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n        \"\"\"\n\n        if isinstance(col_name_converter, str):\n            self.col_name_converter = {\n                k: v for entry in col_name_converter.split(',') for k, v in [entry.strip().split('=')]\n                if len(entry.strip()) &gt; 0\n            }\n        else:\n            self.col_name_converter = col_name_converter\n\n        self.read_csv_kwargs = read_csv_kwargs\n\n        # If the delimiter is not specified, assume whitespace by default:\n        if 'sep' not in self.read_csv_kwargs and 'delimiter' not in self.read_csv_kwargs:\n            self.read_csv_kwargs['sep'] = r'\\s+'\n\n    def parse(self, file_name, drop_na=True):\n        \"\"\"\n        Parse a summary statistics file.\n        :param file_name: The path to the summary statistics file.\n        :param drop_na: If True, drop any entries with missing values.\n\n        :return: A pandas DataFrame containing the parsed summary statistics.\n        \"\"\"\n\n        df = pd.read_csv(file_name, **self.read_csv_kwargs)\n\n        if drop_na:\n            df = df.dropna()\n\n        if self.col_name_converter is not None:\n            df.rename(columns=self.col_name_converter, inplace=True)\n\n        try:\n            df['POS'] = df['POS'].astype(np.int32)\n        except KeyError:\n            pass\n\n        return df\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SumstatsParser.__init__","title":"<code>__init__(col_name_converter=None, **read_csv_kwargs)</code>","text":"<p>Initialize the summary statistics parser.</p> <p>Parameters:</p> Name Type Description Default <code>col_name_converter</code> <p>A dictionary/string mapping column names in the original table to magenpy's column names for the various summary statistics. If a string, it should be a comma-separated list of key-value pairs (e.g. 'rsid=SNP,pos=POS').</p> <code>None</code> <code>read_csv_kwargs</code> <p>Keyword arguments to pass to pandas' read_csv</p> <code>{}</code> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def __init__(self, col_name_converter=None, **read_csv_kwargs):\n    \"\"\"\n    Initialize the summary statistics parser.\n\n    :param col_name_converter: A dictionary/string mapping column names\n    in the original table to magenpy's column names for the various\n    summary statistics. If a string, it should be a comma-separated list of\n    key-value pairs (e.g. 'rsid=SNP,pos=POS').\n    :param read_csv_kwargs: Keyword arguments to pass to pandas' read_csv\n    \"\"\"\n\n    if isinstance(col_name_converter, str):\n        self.col_name_converter = {\n            k: v for entry in col_name_converter.split(',') for k, v in [entry.strip().split('=')]\n            if len(entry.strip()) &gt; 0\n        }\n    else:\n        self.col_name_converter = col_name_converter\n\n    self.read_csv_kwargs = read_csv_kwargs\n\n    # If the delimiter is not specified, assume whitespace by default:\n    if 'sep' not in self.read_csv_kwargs and 'delimiter' not in self.read_csv_kwargs:\n        self.read_csv_kwargs['sep'] = r'\\s+'\n</code></pre>"},{"location":"api/parsers/sumstats_parsers/#magenpy.parsers.sumstats_parsers.SumstatsParser.parse","title":"<code>parse(file_name, drop_na=True)</code>","text":"<p>Parse a summary statistics file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>The path to the summary statistics file.</p> required <code>drop_na</code> <p>If True, drop any entries with missing values.</p> <code>True</code> <p>Returns:</p> Type Description <p>A pandas DataFrame containing the parsed summary statistics.</p> Source code in <code>magenpy/parsers/sumstats_parsers.py</code> <pre><code>def parse(self, file_name, drop_na=True):\n    \"\"\"\n    Parse a summary statistics file.\n    :param file_name: The path to the summary statistics file.\n    :param drop_na: If True, drop any entries with missing values.\n\n    :return: A pandas DataFrame containing the parsed summary statistics.\n    \"\"\"\n\n    df = pd.read_csv(file_name, **self.read_csv_kwargs)\n\n    if drop_na:\n        df = df.dropna()\n\n    if self.col_name_converter is not None:\n        df.rename(columns=self.col_name_converter, inplace=True)\n\n    try:\n        df['POS'] = df['POS'].astype(np.int32)\n    except KeyError:\n        pass\n\n    return df\n</code></pre>"},{"location":"api/plot/gwa/","title":"Gwa","text":""},{"location":"api/plot/gwa/#magenpy.plot.gwa.manhattan","title":"<code>manhattan(input_data, x_col=None, y_col='NLOG10_PVAL', scatter_kwargs=None, highlight_snps=None, highlight_snps_kwargs=None, chrom_sep_color='#f0f0f0', add_bonf_line=True, bonf_line_kwargs=None)</code>","text":"<p>Generate Manhattan plot where the x-axis is the genomic position (in BP) and the y-axis is the -log10(p-value) or some other statistic of the user's choice.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[GWADataLoader, SumstatsTable, DataFrame]</code> <p>An instance of <code>SumstatsTable</code> / <code>GWADataLoader</code> or a <code>pandas.DataFrame</code> from which data about the positions of the SNPs and their statistics will be extracted. If <code>x_col</code> and <code>y_col</code> are not provided, the function will assume that the input data contains columns named 'POS' and 'NLOG10_PVAL', for the position in base pairs and the -log10(p-value) respectively.</p> required <code>x_col</code> <p>The column name in the input data that contains the x-axis values. Defaults to None. If the user passes <code>GWADataLoader</code> or <code>SumstatsTable</code> objects, we attempt to extract the x-axis values from the 'POS' (position in base pairs) column.</p> <code>None</code> <code>y_col</code> <p>The column name in the input data that contains the y-axis values (default: 'NLOG10_PVAL').</p> <code>'NLOG10_PVAL'</code> <code>scatter_kwargs</code> <p>A dictionary of keyword arguments to pass to the <code>plt.scatter</code> function. This can be used to customize the appearance of the points on the scatter plot.</p> <code>None</code> <code>highlight_snps</code> <p>A list or array of SNP rsIDs to highlight on the scatter plot.</p> <code>None</code> <code>highlight_snps_kwargs</code> <p>A dictionary of keyword arguments to pass to the <code>plt.scatter</code> function to customize the appearance of the highlighted SNPs.</p> <code>None</code> <code>chrom_sep_color</code> <p>The color for the chromosome separator block.</p> <code>'#f0f0f0'</code> <code>add_bonf_line</code> <p>If True, add a line indicating the Bonferroni significance threshold.</p> <code>True</code> <code>bonf_line_kwargs</code> <p>The color of the Bonferroni significance threshold line.</p> <code>None</code> Source code in <code>magenpy/plot/gwa.py</code> <pre><code>def manhattan(input_data: Union[GWADataLoader, SumstatsTable, pd.DataFrame],\n              x_col=None,\n              y_col='NLOG10_PVAL',\n              scatter_kwargs=None,\n              highlight_snps=None,\n              highlight_snps_kwargs=None,\n              chrom_sep_color='#f0f0f0',\n              add_bonf_line=True,\n              bonf_line_kwargs=None):\n\n    \"\"\"\n    Generate Manhattan plot where the x-axis is the genomic position (in BP)\n    and the y-axis is the -log10(p-value) or some other statistic of the user's choice.\n\n    :param input_data: An instance of `SumstatsTable` / `GWADataLoader` or a `pandas.DataFrame` from\n    which data about the positions of the SNPs and their statistics will be extracted. If `x_col` and `y_col` are\n    not provided, the function will assume that the input data contains columns named 'POS' and 'NLOG10_PVAL',\n    for the position in base pairs and the -log10(p-value) respectively.\n    :param x_col: The column name in the input data that contains the x-axis values. Defaults to None. If the\n    user passes `GWADataLoader` or `SumstatsTable` objects, we attempt to extract the x-axis\n    values from the 'POS' (position in base pairs) column.\n    :param y_col: The column name in the input data that contains the y-axis values (default: 'NLOG10_PVAL').\n    :param scatter_kwargs: A dictionary of keyword arguments to pass to the `plt.scatter` function.\n    This can be used to customize the appearance of the points on the scatter plot.\n    :param highlight_snps: A list or array of SNP rsIDs to highlight on the scatter plot.\n    :param highlight_snps_kwargs: A dictionary of keyword arguments to pass to the `plt.scatter` function\n    to customize the appearance of the highlighted SNPs.\n    :param chrom_sep_color: The color for the chromosome separator block.\n    :param add_bonf_line: If True, add a line indicating the Bonferroni significance threshold.\n    :param bonf_line_kwargs: The color of the Bonferroni significance threshold line.\n\n    \"\"\"\n\n    # -------------------------------------------------------\n    # Process the input data:\n\n    if isinstance(input_data, SumstatsTable):\n        if x_col is None:\n            x_col = 'POS'\n        input_data = {c: ss.to_table(col_subset=('CHR', 'SNP', x_col, y_col))\n                      for c, ss in input_data.split_by_chromosome().items()}\n    elif isinstance(input_data, GWADataLoader):\n        if x_col is None:\n            x_col = 'POS'\n        input_data = input_data.to_summary_statistics_table(col_subset=('CHR', 'SNP', x_col, y_col),\n                                                            per_chromosome=True)\n    elif isinstance(input_data, pd.DataFrame):\n\n        # Sanity checks:\n        assert 'CHR' in input_data.columns, \"The input data must contain a column named 'CHR'.\"\n\n        # If the x-axis column is not provided, we assume that the user wishes to plot the\n        # variant rank on the x-axis.\n        if x_col is not None:\n            assert x_col in input_data.columns, f\"The input data must contain a column named '{x_col}'.\"\n        else:\n            x_col = 'Variant order'\n            input_data['Variant order'] = np.arange(1, input_data.shape[0] + 1)\n\n        assert y_col in input_data.columns, f\"The input data must contain a column named '{y_col}'.\"\n\n        if highlight_snps is not None:\n            assert 'SNP' in input_data.columns\n\n        input_data = {c: ss for c, ss in input_data.groupby('CHR')}\n    else:\n        raise ValueError(\"The input data must be an instance of `SumstatsTable`, `GWADataLoader` \"\n                         \"or a `pandas.DataFrame`.\")\n\n    # -------------------------------------------------------\n    # Process the SNPs to be highlighted:\n\n    if highlight_snps is not None:\n        if isinstance(highlight_snps, list):\n            highlight_snps = np.array(highlight_snps)\n\n        highlight_snps = {c: np.in1d(p['SNP'], highlight_snps) for c, p in input_data.items()}\n\n    # -------------------------------------------------------\n    # Add custom scatter plot arguments (if not provided)\n    if scatter_kwargs is None:\n        scatter_kwargs = {'marker': '.', 'alpha': 0.3, 'color': '#808080'}\n    else:\n        # Only update the keys that are not already present in the dictionary:\n        scatter_kwargs = {**scatter_kwargs, **{'marker': '.', 'alpha': 0.3, 'color': '#808080'}}\n\n    if highlight_snps is not None:\n        if highlight_snps_kwargs is None:\n            highlight_snps_kwargs = {'marker': 'o', 'color': 'red', 'zorder': 2}\n        else:\n            # Only update the keys that are not already present in the dictionary:\n            highlight_snps_kwargs = {**highlight_snps_kwargs, **{'marker': 'o', 'color': 'red', 'zorder': 2}}\n\n    # Add custom Bonferroni line arguments (if not provided)\n    if bonf_line_kwargs is None:\n        bonf_line_kwargs = {'color': '#b06a7a', 'ls': '--', 'zorder': 1}\n    else:\n        # Only update the keys that are not already present in the dictionary:\n        bonf_line_kwargs = {**bonf_line_kwargs, **{'color': '#b06a7a', 'ls': '--', 'zorder': 1}}\n\n    # -------------------------------------------------------\n\n    if y_col == 'NLOG10_PVAL' and add_bonf_line:\n        # Add Bonferroni significance threshold line:\n        plt.axhline(-np.log10(0.05 / 1e6), **bonf_line_kwargs)\n\n    # -------------------------------------------------------\n    # Iterate through chromosomes and generate scatter plots:\n\n    starting_pos = 0\n    ticks = []\n    chrom_spacing = .1*min([p[x_col].max() - p[x_col].min() for c, p in input_data.items()])\n\n    unique_chr = sorted(list(input_data.keys()))\n\n    for i, c in enumerate(unique_chr):\n\n        min_pos = input_data[c][x_col].min()\n        max_pos = input_data[c][x_col].max()\n\n        xmin = min_pos + starting_pos\n        xmax = max_pos + starting_pos\n\n        if i % 2 == 1:\n            plt.axvspan(xmin=xmin, xmax=xmax, zorder=0, color=chrom_sep_color)\n\n        ticks.append((xmin + xmax) / 2)\n\n        plt.scatter(input_data[c][x_col] + starting_pos,\n                    input_data[c][y_col],\n                    **scatter_kwargs)\n\n        if highlight_snps is not None:\n            plt.scatter((input_data[c][x_col] + starting_pos)[highlight_snps[c]],\n                        input_data[c][y_col][highlight_snps[c]],\n                        **highlight_snps_kwargs)\n\n        starting_pos += max_pos + chrom_spacing\n\n    plt.xticks(ticks, unique_chr)\n\n    if x_col == 'POS':\n        x_col = 'Genomic Position (BP)'\n\n    plt.xlabel(x_col)\n\n    if y_col == 'NLOG10_PVAL':\n        y_col = \"$-log_{10}$(p-value)\"\n    elif y_col == 'PVAL':\n        y_col = \"p-value\"\n\n    plt.ylabel(y_col)\n    plt.tight_layout()\n</code></pre>"},{"location":"api/plot/gwa/#magenpy.plot.gwa.qq_plot","title":"<code>qq_plot(input_data, statistic='p_value')</code>","text":"<p>Generate a quantile-quantile (QQ) plot for the GWAS summary statistics. The function supports plotting QQ plots for the -log10(p-values) as well as the z-score (if available).</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[GWADataLoader, SumstatsTable]</code> <p>An instance of <code>SumstatsTable</code> or <code>GWADataLoader</code> from which data about the positions of the SNPs will be extracted.</p> required <code>statistic</code> <p>The statistic to generate the QQ plot for. We currently support <code>p_value</code> and <code>z_score</code>.</p> <code>'p_value'</code> Source code in <code>magenpy/plot/gwa.py</code> <pre><code>def qq_plot(input_data: Union[GWADataLoader, SumstatsTable],\n            statistic='p_value'):\n    \"\"\"\n    Generate a quantile-quantile (QQ) plot for the GWAS summary statistics.\n    The function supports plotting QQ plots for the -log10(p-values) as well as\n    the z-score (if available).\n\n    :param input_data: An instance of `SumstatsTable` or `GWADataLoader` from which data about the\n    positions of the SNPs will be extracted.\n    :param statistic: The statistic to generate the QQ plot for. We currently support `p_value` and `z_score`.\n    \"\"\"\n\n    import scipy.stats as stats\n\n    if statistic == 'p_value':\n\n        if isinstance(input_data, SumstatsTable):\n            p_val = input_data.negative_log10_p_value\n            m = input_data.m\n        elif isinstance(input_data, GWADataLoader):\n            p_val = np.concatenate([ss.negative_log10_p_value\n                                    for ss in input_data.sumstats_table.values()])\n            m = input_data.m\n        else:\n            raise ValueError(\"The input data must be an instance of `SumstatsTable` or `GWADataLoader`.\")\n\n        plt.scatter(-np.log10(np.arange(1, m + 1) / m), np.sort(p_val)[::-1])\n\n        line = np.linspace(0., p_val.max() + 0.1*p_val.max())\n        plt.plot(line, line, c='red')\n        plt.xlabel(\"Expected $-log_{10}$(p-value)\")\n        plt.ylabel(\"Observed $-log_{10}$(p-value)\")\n\n    elif statistic == 'z_score':\n        if isinstance(input_data, SumstatsTable):\n            z_scs = input_data.z_score\n        elif isinstance(input_data, GWADataLoader):\n            z_scs = np.concatenate([ss.z_score for ss in input_data.sumstats_table.values()])\n        else:\n            raise ValueError(\"The input data must be an instance of `SumstatsTable` or `GWADataLoader`.\")\n\n        stats.probplot(z_scs, dist=\"norm\", plot=plt)\n        plt.show()\n    else:\n        raise NotImplementedError(f\"No QQ plot can be generated for the statistic: {statistic}\")\n</code></pre>"},{"location":"api/plot/ld/","title":"Ld","text":""},{"location":"api/plot/ld/#magenpy.plot.ld.plot_ld_matrix","title":"<code>plot_ld_matrix(ldm, variant_subset=None, start_row=None, end_row=None, symmetric=False, cmap='RdBu', include_colorbar=True)</code>","text":"<p>Plot a heatmap representing the LD matrix or portions of it.</p> <p>Parameters:</p> Name Type Description Default <code>ldm</code> <code>LDMatrix</code> <p>An instance of <code>LDMatrix</code>.</p> required <code>variant_subset</code> <p>A list of variant rsIDs to subset the LD matrix.</p> <code>None</code> <code>start_row</code> <p>The starting row index for the LD matrix plot.</p> <code>None</code> <code>end_row</code> <p>The ending row index (not inclusive) for the LD matrix plot.</p> <code>None</code> <code>symmetric</code> <p>If True, plot the symmetric version of the LD matrix. Otherwise, plot the upper triangular part.</p> <code>False</code> <code>cmap</code> <p>The color map for the LD matrix plot.</p> <code>'RdBu'</code> <code>include_colorbar</code> <p>If True, include a colorbar in the plot.</p> <code>True</code> Source code in <code>magenpy/plot/ld.py</code> <pre><code>def plot_ld_matrix(ldm: LDMatrix,\n                   variant_subset=None,\n                   start_row=None,\n                   end_row=None,\n                   symmetric=False,\n                   cmap='RdBu',\n                   include_colorbar=True):\n    \"\"\"\n    Plot a heatmap representing the LD matrix or portions of it.\n\n    :param ldm: An instance of `LDMatrix`.\n    :param variant_subset: A list of variant rsIDs to subset the LD matrix.\n    :param start_row: The starting row index for the LD matrix plot.\n    :param end_row: The ending row index (not inclusive) for the LD matrix plot.\n    :param symmetric: If True, plot the symmetric version of the LD matrix.\n    Otherwise, plot the upper triangular part.\n    :param cmap: The color map for the LD matrix plot.\n    :param include_colorbar: If True, include a colorbar in the plot.\n    \"\"\"\n\n    curr_mask = None\n\n    if variant_subset is not None:\n        curr_mask = ldm.get_mask()\n        ldm.reset_mask()\n        ldm.filter_snps(variant_subset)\n\n    ld_mat = ldm.load_data(start_row=start_row,\n                           end_row=end_row,\n                           return_symmetric=symmetric,\n                           return_square=True,\n                           return_as_csr=True,\n                           dtype=np.float32).todense()\n\n    plt.imshow(ld_mat, cmap=cmap, vmin=-1., vmax=1., interpolation='none')\n\n    if include_colorbar:\n        plt.colorbar()\n\n    # Reset the original mask:\n    if curr_mask is not None:\n        ldm.set_mask(curr_mask)\n\n    plt.axis('off')\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/","title":"AnnotatedPhenotypeSimulator","text":""},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator","title":"<code>AnnotatedPhenotypeSimulator</code>","text":"<p>               Bases: <code>PhenotypeSimulator</code></p> <p>Simulate complex traits by incorporating genomic functional annotations into the mixture densities that govern the effect size of each variant on the trait.</p> <p>Warning</p> <p>This code is experimental and needs much further validation.</p> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>class AnnotatedPhenotypeSimulator(PhenotypeSimulator):\n    \"\"\"\n    Simulate complex traits by incorporating genomic functional\n    annotations into the mixture densities that govern the effect size\n    of each variant on the trait.\n\n    !!! warning\n        This code is experimental and needs much further validation.\n\n    \"\"\"\n\n    def __init__(self, bed_files, **kwargs):\n        \"\"\"\n        Create an instance of the AnnotatedPhenotypeSimulator class.\n\n        :param bed_files: A list of BED files that contain the genotype data.\n        :param kwargs: Additional keyword arguments for the PhenotypeSimulator class.\n        \"\"\"\n\n        super().__init__(bed_files, **kwargs)\n\n        # For now, we will restrict to 2 mixture components.\n        assert self.n_components == 2\n\n        self.w_h2 = None  # The annotation weights for the per-SNP heritability\n        self.w_pi = None  # The annotation weights for the per-SNP causal probability\n\n    def set_w_h2(self, w_h2):\n        \"\"\"\n        Set the annotation weights for the per SNP heritability\n        :param w_h2: A vector of weights for each annotation.\n        \"\"\"\n\n        assert len(w_h2) == self.n_annotations\n\n        self.w_h2 = w_h2\n        self.set_per_snp_heritability()\n\n    def simulate_w_h2(self, enrichment=None):\n        \"\"\"\n        Simulate the annotation weights for the per-SNP heritability\n        \"\"\"\n        raise NotImplementedError\n\n    def set_w_pi(self, w_pi):\n        \"\"\"\n        Set the annotation weights for the per SNP causal probability\n        :param w_pi: A vector of weights for each annotation.\n        \"\"\"\n\n        assert len(w_pi) == self.n_annotations\n\n        self.w_pi = w_pi\n        self.set_per_snp_mixture_probability()\n\n    def simulate_w_pi(self, enrichment=None):\n        \"\"\"\n        Simulate the annotation weights for the per-SNP causal probability\n\n        :param enrichment: A dictionary of enrichment values where the\n        key is the annotation and the value is the enrichment\n        \"\"\"\n\n        enrichment = enrichment or {}\n        enr = []\n        for annot in self.annotation[self.chromosomes[0]].annotations:\n            try:\n                enr.append(enrichment[annot])\n            except KeyError:\n                enr.append(1.)\n\n        self.w_pi = np.log(np.array(enr))\n\n    def set_per_snp_heritability(self):\n        \"\"\"\n        Set the per-SNP heritability values using the annotation weights.\n        \"\"\"\n\n        if self.w_h2 is None:\n            return super().set_per_snp_heritability()\n\n        self.per_snp_h2 = {}\n\n        for c in self.chromosomes:\n            self.per_snp_h2[c] = np.clip(np.dot(self.annotation[c].values(), self.w_h2),\n                                         a_min=0., a_max=np.inf)\n\n    def set_per_snp_mixture_probability(self):\n        \"\"\"\n        Set the per-SNP mixture probabilities using the annotation weights.\n        \"\"\"\n\n        if self.w_pi is None:\n            return super().set_per_snp_mixture_probability()\n\n        self.per_snp_pi = {}\n\n        for c in self.chromosomes:\n            prob = 1./(1. + np.exp(-np.dot(self.annotation[c].values(add_intercept=True),\n                                           np.concatenate([[np.log(self.pi[1])], self.w_pi]))))\n            self.per_snp_pi[c] = np.array([1. - prob, prob]).T\n\n    def get_heritability_enrichment(self):\n        \"\"\"\n        Estimate the enrichment of heritability per annotation.\n        \"\"\"\n\n        tabs = self.to_true_beta_table(per_chromosome=True)\n        total_heritability = sum([tab['Heritability'].sum() for c, tab in tabs.items()])\n\n        heritability_per_binary_annot = {\n            bin_annot: 0. for bin_annot in self.annotation[self.chromosomes[0]].binary_annotations\n        }\n\n        n_variants_per_binary_annot = {\n            bin_annot: 0 for bin_annot in heritability_per_binary_annot\n        }\n\n        for c, c_size in self.shapes.items():\n            for bin_annot in self.annotation[c].binary_annotations:\n                annot_idx = self.annotation[c].get_binary_annotation_index(bin_annot)\n                heritability_per_binary_annot[bin_annot] += tabs[c].iloc[np.array(annot_idx), :]['Heritability'].sum()\n                n_variants_per_binary_annot[bin_annot] += len(annot_idx)\n\n        return {\n            bin_annot: (ba_h2/total_heritability) / (n_variants_per_binary_annot[bin_annot] / self.m)\n            for bin_annot, ba_h2 in heritability_per_binary_annot.items()\n        }\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.__init__","title":"<code>__init__(bed_files, **kwargs)</code>","text":"<p>Create an instance of the AnnotatedPhenotypeSimulator class.</p> <p>Parameters:</p> Name Type Description Default <code>bed_files</code> <p>A list of BED files that contain the genotype data.</p> required <code>kwargs</code> <p>Additional keyword arguments for the PhenotypeSimulator class.</p> <code>{}</code> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def __init__(self, bed_files, **kwargs):\n    \"\"\"\n    Create an instance of the AnnotatedPhenotypeSimulator class.\n\n    :param bed_files: A list of BED files that contain the genotype data.\n    :param kwargs: Additional keyword arguments for the PhenotypeSimulator class.\n    \"\"\"\n\n    super().__init__(bed_files, **kwargs)\n\n    # For now, we will restrict to 2 mixture components.\n    assert self.n_components == 2\n\n    self.w_h2 = None  # The annotation weights for the per-SNP heritability\n    self.w_pi = None  # The annotation weights for the per-SNP causal probability\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.get_heritability_enrichment","title":"<code>get_heritability_enrichment()</code>","text":"<p>Estimate the enrichment of heritability per annotation.</p> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def get_heritability_enrichment(self):\n    \"\"\"\n    Estimate the enrichment of heritability per annotation.\n    \"\"\"\n\n    tabs = self.to_true_beta_table(per_chromosome=True)\n    total_heritability = sum([tab['Heritability'].sum() for c, tab in tabs.items()])\n\n    heritability_per_binary_annot = {\n        bin_annot: 0. for bin_annot in self.annotation[self.chromosomes[0]].binary_annotations\n    }\n\n    n_variants_per_binary_annot = {\n        bin_annot: 0 for bin_annot in heritability_per_binary_annot\n    }\n\n    for c, c_size in self.shapes.items():\n        for bin_annot in self.annotation[c].binary_annotations:\n            annot_idx = self.annotation[c].get_binary_annotation_index(bin_annot)\n            heritability_per_binary_annot[bin_annot] += tabs[c].iloc[np.array(annot_idx), :]['Heritability'].sum()\n            n_variants_per_binary_annot[bin_annot] += len(annot_idx)\n\n    return {\n        bin_annot: (ba_h2/total_heritability) / (n_variants_per_binary_annot[bin_annot] / self.m)\n        for bin_annot, ba_h2 in heritability_per_binary_annot.items()\n    }\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.set_per_snp_heritability","title":"<code>set_per_snp_heritability()</code>","text":"<p>Set the per-SNP heritability values using the annotation weights.</p> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def set_per_snp_heritability(self):\n    \"\"\"\n    Set the per-SNP heritability values using the annotation weights.\n    \"\"\"\n\n    if self.w_h2 is None:\n        return super().set_per_snp_heritability()\n\n    self.per_snp_h2 = {}\n\n    for c in self.chromosomes:\n        self.per_snp_h2[c] = np.clip(np.dot(self.annotation[c].values(), self.w_h2),\n                                     a_min=0., a_max=np.inf)\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.set_per_snp_mixture_probability","title":"<code>set_per_snp_mixture_probability()</code>","text":"<p>Set the per-SNP mixture probabilities using the annotation weights.</p> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def set_per_snp_mixture_probability(self):\n    \"\"\"\n    Set the per-SNP mixture probabilities using the annotation weights.\n    \"\"\"\n\n    if self.w_pi is None:\n        return super().set_per_snp_mixture_probability()\n\n    self.per_snp_pi = {}\n\n    for c in self.chromosomes:\n        prob = 1./(1. + np.exp(-np.dot(self.annotation[c].values(add_intercept=True),\n                                       np.concatenate([[np.log(self.pi[1])], self.w_pi]))))\n        self.per_snp_pi[c] = np.array([1. - prob, prob]).T\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.set_w_h2","title":"<code>set_w_h2(w_h2)</code>","text":"<p>Set the annotation weights for the per SNP heritability</p> <p>Parameters:</p> Name Type Description Default <code>w_h2</code> <p>A vector of weights for each annotation.</p> required Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def set_w_h2(self, w_h2):\n    \"\"\"\n    Set the annotation weights for the per SNP heritability\n    :param w_h2: A vector of weights for each annotation.\n    \"\"\"\n\n    assert len(w_h2) == self.n_annotations\n\n    self.w_h2 = w_h2\n    self.set_per_snp_heritability()\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.set_w_pi","title":"<code>set_w_pi(w_pi)</code>","text":"<p>Set the annotation weights for the per SNP causal probability</p> <p>Parameters:</p> Name Type Description Default <code>w_pi</code> <p>A vector of weights for each annotation.</p> required Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def set_w_pi(self, w_pi):\n    \"\"\"\n    Set the annotation weights for the per SNP causal probability\n    :param w_pi: A vector of weights for each annotation.\n    \"\"\"\n\n    assert len(w_pi) == self.n_annotations\n\n    self.w_pi = w_pi\n    self.set_per_snp_mixture_probability()\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.simulate_w_h2","title":"<code>simulate_w_h2(enrichment=None)</code>","text":"<p>Simulate the annotation weights for the per-SNP heritability</p> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def simulate_w_h2(self, enrichment=None):\n    \"\"\"\n    Simulate the annotation weights for the per-SNP heritability\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/simulation/AnnotatedPhenotypeSimulator/#magenpy.simulation.AnnotatedPhenotypeSimulator.AnnotatedPhenotypeSimulator.simulate_w_pi","title":"<code>simulate_w_pi(enrichment=None)</code>","text":"<p>Simulate the annotation weights for the per-SNP causal probability</p> <p>Parameters:</p> Name Type Description Default <code>enrichment</code> <p>A dictionary of enrichment values where the key is the annotation and the value is the enrichment</p> <code>None</code> Source code in <code>magenpy/simulation/AnnotatedPhenotypeSimulator.py</code> <pre><code>def simulate_w_pi(self, enrichment=None):\n    \"\"\"\n    Simulate the annotation weights for the per-SNP causal probability\n\n    :param enrichment: A dictionary of enrichment values where the\n    key is the annotation and the value is the enrichment\n    \"\"\"\n\n    enrichment = enrichment or {}\n    enr = []\n    for annot in self.annotation[self.chromosomes[0]].annotations:\n        try:\n            enr.append(enrichment[annot])\n        except KeyError:\n            enr.append(1.)\n\n    self.w_pi = np.log(np.array(enr))\n</code></pre>"},{"location":"api/simulation/MultiCohortPhenotypeSimulator/","title":"MultiCohortPhenotypeSimulator","text":""},{"location":"api/simulation/MultiCohortPhenotypeSimulator/#magenpy.simulation.MultiCohortPhenotypeSimulator.MultiCohortPhenotypeSimulator","title":"<code>MultiCohortPhenotypeSimulator</code>","text":"<p>               Bases: <code>GWADataLoader</code></p> <p>A module for simulating GWAS data for separate cohorts or clusters of the data. This includes scenarios such as multi-population or multi-ethnic datasets, or  datasets that can be stratified by a discrete variable.</p> <p>Warning</p> <p>This code is experimental and needs much further validation.</p> Source code in <code>magenpy/simulation/MultiCohortPhenotypeSimulator.py</code> <pre><code>class MultiCohortPhenotypeSimulator(GWADataLoader):\n    \"\"\"\n    A module for simulating GWAS data for separate cohorts or clusters of the data.\n    This includes scenarios such as multi-population or multi-ethnic datasets, or \n    datasets that can be stratified by a discrete variable.\n\n    !!! warning\n        This code is experimental and needs much further validation.\n\n    \"\"\"\n\n    def __init__(self,\n                 bed_files,\n                 cluster_assignments_file,\n                 prop_shared_causal=1.,\n                 rho=1.,\n                 **kwargs):\n        \"\"\"\n        Simulate phenotypes using the linear additive model while accounting \n        for heterogeneous genetic architectures across cohorts.\n\n        :param bed_files: A path (or list of paths) to PLINK BED files.\n        :param cluster_assignments_file: A file mapping each sample in the BED files to their corresponding \n        cohort or cluster.\n        :param prop_shared_causal: Proportion of causal variants that are shared across clusters.\n        :param rho: The correlation coefficient for the effect size across clusters.\n        \"\"\"\n\n        super().__init__(bed_files, **kwargs)\n\n        from ..parsers.misc_parsers import parse_cluster_assignment_file\n\n        self.cluster_table = parse_cluster_assignment_file(cluster_assignments_file)\n\n        # Proportion of causal snps that are shared\n        self.prop_shared_causal = prop_shared_causal\n\n        # Rho can be either a scalar or a matrix that determines the patterns of\n        # correlations between effect sizes in different clusters.\n        if np.issubdtype(type(rho), np.floating):\n            self.rho = rho*np.ones(shape=(len(self.clusters), len(self.clusters)))\n            np.fill_diagonal(self.rho, 1.)\n        else:\n            self.rho = rho\n\n        # Reference cluster\n        self.ref_cluster = None\n\n        # A dictionary of GWAS simulators for each cluster\n        self.cluster_simulators = {}\n\n        for c in self.clusters:\n            if self.ref_cluster is None:\n                self.ref_cluster = c\n\n            self.cluster_simulators[c] = PhenotypeSimulator(bed_files,\n                                                            keep_samples=self.get_samples_in_cluster(c),\n                                                            **kwargs)\n\n    @property\n    def clusters(self):\n        return self.cluster_table['Cluster'].unique()\n\n    def get_samples_in_cluster(self, cluster):\n        return self.cluster_table.loc[self.cluster_table['Cluster'] == cluster, 'IID'].values\n\n    def set_reference_cluster(self, c):\n        self.ref_cluster = c\n\n    def simulate_causal_status(self):\n\n        # The reference simulator:\n        ref_sim = self.cluster_simulators[self.ref_cluster]\n\n        # Simulate causal snps in reference cluster:\n        ref_sim.simulate_mixture_assignment()\n\n        # Get the causal snps in reference cluster:\n        ref_causal = {\n            c: np.where(a)[0]\n            for c, a in ref_sim.get_causal_status().items()\n        }\n\n        for c in self.clusters:\n            # For each cluster that is not the reference,\n            # update their causal snps according to our draw for\n            # the reference cluster\n            if c != self.ref_cluster:\n\n                new_mixture = ref_sim.mixture_assignment.copy()\n\n                if self.prop_shared_causal &lt; 1.:\n                    for ch, ref_c in ref_causal.items():\n\n                        # The number of shared causal snps for Chromosome `ch`:\n                        n_shared_causal = int(np.floor(self.prop_shared_causal * len(ref_c)))\n\n                        # Number of snps to flip:\n                        n_flip = len(ref_c) - n_shared_causal\n\n                        # Randomly decide which snps to \"turn off\":\n                        for i in np.random.choice(ref_c, size=n_flip, replace=False):\n                            new_mixture[ch][i] = new_mixture[ch][i][::-1]\n                            # With probability p, switch on some other randomly chosen SNP:\n                            # NOTE: If the number of SNPs is small, there's a small chance\n                            # that this may flip the same SNP multiple times.\n                            if np.random.uniform() &lt; ref_sim.pis[1]:\n                                new_i = np.random.choice(self.shapes[ch])\n                                new_mixture[ch][new_i] = new_mixture[ch][new_i][::-1]\n\n                self.cluster_simulators[c].set_mixture_assignment(\n                    new_mixture\n                )\n\n    def simulate_beta(self):\n\n        for c in self.clusters:\n            self.cluster_simulators[c].beta = {}\n\n        for ch, c_size in self.shapes.items():\n            # Draw the beta from a multivariate normal distribution with covariance\n            # as specified in the matrix `rho`.\n            betas = np.random.multivariate_normal(np.zeros(self.rho.shape[0]), cov=self.rho, size=c_size)\n            for i, c in enumerate(self.clusters):\n                self.cluster_simulators[c].beta[ch] = (\n                        self.cluster_simulators[c].get_causal_status()[ch].astype(np.int32)*betas[:, i]\n                )\n\n    def simulate(self, perform_gwas=False):\n\n        self.simulate_causal_status()\n        self.simulate_beta()\n\n        iids = self.sample_table.iid\n\n        phenotypes = pd.Series(np.zeros_like(iids), index=iids)\n\n        for c in self.clusters:\n            self.cluster_simulators[c].simulate(reset_beta=False)\n            phenotypes[self.cluster_simulators[c].sample_table.iid] = self.cluster_simulators[c].sample_table.phenotype\n\n        self.set_phenotype(phenotypes)\n\n        # Perform GWAS on the pooled sample:\n        if perform_gwas:\n            self.perform_gwas()\n</code></pre>"},{"location":"api/simulation/MultiCohortPhenotypeSimulator/#magenpy.simulation.MultiCohortPhenotypeSimulator.MultiCohortPhenotypeSimulator.__init__","title":"<code>__init__(bed_files, cluster_assignments_file, prop_shared_causal=1.0, rho=1.0, **kwargs)</code>","text":"<p>Simulate phenotypes using the linear additive model while accounting  for heterogeneous genetic architectures across cohorts.</p> <p>Parameters:</p> Name Type Description Default <code>bed_files</code> <p>A path (or list of paths) to PLINK BED files.</p> required <code>cluster_assignments_file</code> <p>A file mapping each sample in the BED files to their corresponding  cohort or cluster.</p> required <code>prop_shared_causal</code> <p>Proportion of causal variants that are shared across clusters.</p> <code>1.0</code> <code>rho</code> <p>The correlation coefficient for the effect size across clusters.</p> <code>1.0</code> Source code in <code>magenpy/simulation/MultiCohortPhenotypeSimulator.py</code> <pre><code>def __init__(self,\n             bed_files,\n             cluster_assignments_file,\n             prop_shared_causal=1.,\n             rho=1.,\n             **kwargs):\n    \"\"\"\n    Simulate phenotypes using the linear additive model while accounting \n    for heterogeneous genetic architectures across cohorts.\n\n    :param bed_files: A path (or list of paths) to PLINK BED files.\n    :param cluster_assignments_file: A file mapping each sample in the BED files to their corresponding \n    cohort or cluster.\n    :param prop_shared_causal: Proportion of causal variants that are shared across clusters.\n    :param rho: The correlation coefficient for the effect size across clusters.\n    \"\"\"\n\n    super().__init__(bed_files, **kwargs)\n\n    from ..parsers.misc_parsers import parse_cluster_assignment_file\n\n    self.cluster_table = parse_cluster_assignment_file(cluster_assignments_file)\n\n    # Proportion of causal snps that are shared\n    self.prop_shared_causal = prop_shared_causal\n\n    # Rho can be either a scalar or a matrix that determines the patterns of\n    # correlations between effect sizes in different clusters.\n    if np.issubdtype(type(rho), np.floating):\n        self.rho = rho*np.ones(shape=(len(self.clusters), len(self.clusters)))\n        np.fill_diagonal(self.rho, 1.)\n    else:\n        self.rho = rho\n\n    # Reference cluster\n    self.ref_cluster = None\n\n    # A dictionary of GWAS simulators for each cluster\n    self.cluster_simulators = {}\n\n    for c in self.clusters:\n        if self.ref_cluster is None:\n            self.ref_cluster = c\n\n        self.cluster_simulators[c] = PhenotypeSimulator(bed_files,\n                                                        keep_samples=self.get_samples_in_cluster(c),\n                                                        **kwargs)\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/","title":"PhenotypeSimulator","text":""},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator","title":"<code>PhenotypeSimulator</code>","text":"<p>               Bases: <code>GWADataLoader</code></p> <p>A wrapper class that supports simulating complex traits with a variety of genetic architectures and heritability values, using the standard linear model. The basic implementation supports simulating effect sizes from a sparse Gaussian mixture density, allowing some variants to have larger effects than others. The class also supports simulating binary phenotypes (case-control) by thresholding the continuous phenotype at a specified threshold.</p> <p>To be concrete, the generative model for the simulation is as follows:</p> <p>1) Simulate the mixture assignment for each variant based on the mixing proportions <code>pi</code>. 2) Simulate the effect sizes for each variant from the corresponding Gaussian density that they were assigned. 3) Compute the polygenic score for each individual based on the simulated effect sizes. 4) Simulate the residual component of the phenotype, in such a way that the total heritability is preserved.</p> <p>See Also</p> <ul> <li>GWADataLoader</li> </ul> <p>Attributes:</p> Name Type Description <code>pi</code> <p>The mixing proportions for the Gaussian mixture density.</p> <code>h2</code> <p>The trait SNP heritability, or proportion of variance explained by SNPs.</p> <code>d</code> <p>The variance multipliers for each component of the Gaussian mixture density.</p> <code>prevalence</code> <p>The (disease) prevalence for binary (case-control) phenotypes.</p> <code>per_snp_h2</code> <p>The per-SNP heritability for each variant in the dataset.</p> <code>per_snp_pi</code> <p>The per-SNP mixing proportions for each variant in the dataset.</p> <code>beta</code> <p>The effect sizes for each variant in the dataset.</p> <code>mixture_assignment</code> <p>The assignment of each variant to a mixture component.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>class PhenotypeSimulator(GWADataLoader):\n    \"\"\"\n    A wrapper class that supports simulating complex traits with a variety of\n    genetic architectures and heritability values, using the standard linear model. The\n    basic implementation supports simulating effect sizes from a sparse Gaussian mixture density,\n    allowing some variants to have larger effects than others. The class also supports simulating\n    binary phenotypes (case-control) by thresholding the continuous phenotype at a specified threshold.\n\n    To be concrete, the generative model for the simulation is as follows:\n\n    1) Simulate the mixture assignment for each variant based on the mixing proportions `pi`.\n    2) Simulate the effect sizes for each variant from the corresponding Gaussian density that they were assigned.\n    3) Compute the polygenic score for each individual based on the simulated effect sizes.\n    4) Simulate the residual component of the phenotype, in such a way that the total heritability is preserved.\n\n    !!! seealso \"See Also\"\n        * [GWADataLoader][magenpy.GWADataLoader.GWADataLoader]\n\n    :ivar pi: The mixing proportions for the Gaussian mixture density.\n    :ivar h2: The trait SNP heritability, or proportion of variance explained by SNPs.\n    :ivar d: The variance multipliers for each component of the Gaussian mixture density.\n    :ivar prevalence: The (disease) prevalence for binary (case-control) phenotypes.\n    :ivar per_snp_h2: The per-SNP heritability for each variant in the dataset.\n    :ivar per_snp_pi: The per-SNP mixing proportions for each variant in the dataset.\n    :ivar beta: The effect sizes for each variant in the dataset.\n    :ivar mixture_assignment: The assignment of each variant to a mixture component.\n\n    \"\"\"\n\n    def __init__(self,\n                 bed_files,\n                 h2=0.2,\n                 pi=0.1,\n                 d=(0., 1.),\n                 prevalence=0.15,\n                 **kwargs):\n        \"\"\"\n        Initialize the PhenotypeSimulator object with the necessary parameters.\n\n        :param bed_files: A path (or list of paths) to PLINK BED files containing the genotype information.\n        :param h2: The trait SNP heritability, or proportion of variance explained by SNPs.\n        :param pi: The mixing proportions for the mixture of Gaussians (our model for the distribution of effect sizes).\n        If a float is provided, it is converted to a tuple (1-pi, pi), where pi is the proportion of causal variants.\n        :param d:  The variance multipliers for each component of the Gaussian mixture density. By default,\n        all components have the same variance multiplier.\n        :param prevalence: The (disease) prevalence for binary (case-control) phenotypes.\n        \"\"\"\n\n        super().__init__(bed_files, **kwargs)\n\n        # If pi is float, convert it to a tuple:\n        if isinstance(pi, float):\n            pi = (1. - pi, pi)\n\n        self.pi = pi\n        self.h2 = h2\n        self.prevalence = prevalence\n\n        # Sanity checks:\n        assert 0. &lt;= self.h2 &lt;= 1.\n        assert round(sum(self.pi), 1) == 1.\n        assert 0. &lt; self.prevalence &lt; 1.\n\n        self.d = np.array(d)\n\n        self.per_snp_h2 = None\n        self.per_snp_pi = None\n        self.beta = None\n        self.mixture_assignment = None\n\n    @property\n    def n_components(self):\n        \"\"\"\n        :return: The number of Gaussian mixture components for the effect size distribution.\n        \"\"\"\n        return len(self.pi)\n\n    def set_pi(self, new_pi):\n        \"\"\"\n        Set the mixture proportions (proportion of variants in each\n        Gaussian mixture component).\n        \"\"\"\n        self.pi = new_pi\n        self.set_per_snp_mixture_probability()\n\n    def set_h2(self, new_h2):\n        \"\"\"\n        Set the total heritability (proportion of additive variance due to SNPs) for the trait\n        \"\"\"\n        self.h2 = new_h2\n        self.set_per_snp_heritability()\n\n    def set_per_snp_mixture_probability(self):\n        \"\"\"\n        Set the per-SNP mixing proportions for each variant in the dataset.\n        This is a convenience method that may come in handy for more flexible generative models.\n        \"\"\"\n\n        self.per_snp_pi = {}\n\n        for c, c_size in self.shapes.items():\n            self.per_snp_pi[c] = np.repeat(np.array([self.pi]), c_size, axis=0)\n\n    def set_per_snp_heritability(self):\n        \"\"\"\n        Set the per-SNP heritability (effect size variance) for each variant in the dataset.\n        This is a convenience method that may come in handy for more flexible generative models.\n        \"\"\"\n\n        assert self.mixture_assignment is not None\n\n        # Estimate the global sigma_beta_sq based on the\n        # pre-specified heritability, the mixture proportions `pi`,\n        # and the prior multipliers `d`.\n\n        combined_assignments = np.concatenate([self.mixture_assignment[c] for c in self.chromosomes])\n\n        sigma_beta_sq = self.h2 / (combined_assignments*self.d).sum()\n\n        self.per_snp_h2 = {}\n\n        for c, c_size in self.shapes.items():\n            self.per_snp_h2[c] = sigma_beta_sq*self.d[np.where(self.mixture_assignment[c])[1]]\n\n    def get_causal_status(self):\n        \"\"\"\n        :return: A dictionary where the keys are the chromosome numbers\n        and the values are binary vectors indicating which SNPs are\n        causal for the simulated phenotype.\n\n        :raises AssertionError: If the mixture assignment is not set.\n        \"\"\"\n\n        assert self.mixture_assignment is not None\n\n        try:\n            zero_index = list(self.d).index(0)\n        except ValueError:\n            # If all SNPs are causal:\n            return {c: np.repeat(True, c_size) for c, c_size in self.shapes.items()}\n\n        causal_status = {}\n\n        for c, mix_a in self.mixture_assignment.items():\n            causal_status[c] = np.where(mix_a)[1] != zero_index\n\n        return causal_status\n\n    def set_causal_snps(self, causal_snps):\n        \"\"\"\n        A utility method to set the causal variants in the simulation based on an array or\n        list of SNPs specified by the user. The method takes an iterable (e.g. list or array) of `causal_snps`\n        and then creates a new mixture assignment object where only the `causal_snps`\n        contribute to the phenotype.\n\n        :param causal_snps: A list or array of SNP rsIDs.\n        :raises ValueError: If all mixture components are causal.\n\n        \"\"\"\n\n        # Get the index of the mixture component whose multiplier is zero (i.e. the null component):\n        try:\n            zero_index = list(self.d).index(0)\n        except ValueError:\n            raise ValueError(\"Cannot set causal variants when all mixture components are causal. Modify \"\n                             \"the mixture multipliers `d` to enable this functionality.\")\n\n        # Get the indices of the non-null mixture components:\n        nonzero_indices = [i for i, d in enumerate(self.d) if d != 0.]\n\n        # Get the mixture proportions for the non-null components and normalize them so they sum to 1:\n        pis = np.array(self.pi)[nonzero_indices]\n        pis /= pis.sum()\n\n        # Initialize new mixture assignment object:\n        new_assignment = {c: np.zeros((s, self.n_components)) for c, s in self.shapes.items()}\n\n        from ..utils.compute_utils import intersect_arrays\n\n        n_causal_set = 0\n\n        for c, snps in self.snps.items():\n\n            # Intersect the list of causal SNPs with the SNPs on chromosome `c`:\n            snp_idx = intersect_arrays(snps, causal_snps, return_index=True)\n\n            if len(snp_idx) &gt; 0:\n                n_causal_set += len(snp_idx)\n                # For the causal SNPs, assign them randomly to the causal components\n                new_assignment[c][snp_idx, np.random.choice(nonzero_indices,\n                                                            size=len(snp_idx),\n                                                            p=pis)] = 1\n                # For the remaining SNPs, assign them to the null component:\n                new_assignment[c][:, zero_index] = np.abs(new_assignment[c].sum(axis=1) - 1)\n\n        if n_causal_set &lt; len(causal_snps):\n            warnings.warn(\"Not all causal SNPs are represented in the genotype matrix! \"\n                          f\"User passed a list of {len(causal_snps)} SNPs, only matched {n_causal_set}.\")\n\n        self.set_mixture_assignment(new_assignment)\n\n    def set_mixture_assignment(self, new_assignment):\n        \"\"\"\n        Set the mixture assignments according to user-provided dictionary. The mixture\n        assignment indicates which mixture component the effect size of a particular\n        variant comes from.\n        :param new_assignment: A dictionary where the keys are the chromosomes and\n        the values are the mixture assignment for each SNP on that chromosome.\n        \"\"\"\n\n        # Check that the shapes match pre-specified information:\n        for c, c_size in self.shapes.items():\n            assert new_assignment[c].shape == (c_size, self.n_components)\n\n        self.mixture_assignment = new_assignment\n\n    def simulate_mixture_assignment(self):\n        \"\"\"\n        Simulate assigning SNPs to the various mixture components\n        with probabilities given by mixing proportions `pi`.\n        \"\"\"\n\n        if self.per_snp_pi is None or len(self.per_snp_pi) &lt; 1:\n            self.set_per_snp_mixture_probability()\n\n        self.mixture_assignment = {}\n\n        from ..utils.model_utils import multinomial_rvs\n\n        for c, c_size in self.shapes.items():\n\n            self.mixture_assignment[c] = multinomial_rvs(1, self.per_snp_pi[c])\n\n        return self.mixture_assignment\n\n    def set_beta(self, new_beta):\n        \"\"\"\n        Set the variant effect sizes (beta) according to user-provided dictionary.\n\n        :param new_beta: A dictionary where the keys are the chromosomes and\n        the values are the beta (effect size) for each SNP on that chromosome.\n        \"\"\"\n\n        # Check that the shapes match pre-specified information:\n        for c, c_size in self.shapes.items():\n            assert len(new_beta[c]) == c_size\n\n        self.beta = new_beta\n\n    def simulate_beta(self):\n        \"\"\"\n        Simulate the causal effect size for variants included\n        in the dataset. Here, the variant effect size is drawn from\n        a Gaussian density with mean zero and scale given by\n        the root of per-SNP heritability.\n        \"\"\"\n\n        if self.per_snp_h2 is None or len(self.per_snp_h2) &lt; 1:\n            self.set_per_snp_heritability()\n\n        self.beta = {}\n\n        for c, c_size in self.shapes.items():\n\n            self.beta[c] = np.random.normal(loc=0.0,\n                                            scale=np.sqrt(self.per_snp_h2[c]),\n                                            size=c_size)\n\n        return self.beta\n\n    def simulate_phenotype(self):\n        \"\"\"\n        Simulate complex phenotypes for the samples present in the genotype matrix, given their\n        genotype information and fixed effect sizes `beta` that were simulated previous steps.\n\n        Given the simulated effect sizes, the phenotype is generated as follows:\n\n        `Y = XB + e`\n\n        Where `Y` is the vector of phenotypes, `X` is the genotype matrix, `B` is the vector of effect sizes,\n        and `e` represents the residual effects.\n        \"\"\"\n\n        assert self.beta is not None\n\n        # Compute the polygenic score given the simulated/provided beta:\n        # NOTE: For this formulation to work, it's important to standardize the genotype.\n        pgs = self.score(self.beta, standardize_genotype=True)\n\n        # Sample the environmental/residual component:\n        e = np.random.normal(loc=0., scale=np.sqrt(1. - self.h2), size=self.sample_size)\n\n        # The final simulated phenotype is a combination of\n        # the polygenic score + the residual component:\n        y = pgs + e\n\n        if self.phenotype_likelihood == 'binomial':\n            # If the simulated phenotype is to be binary,\n            # use the threshold model to determine positives/negatives\n            # based on the prevalence of the phenotype in the population:\n\n            from ..stats.transforms.phenotype import standardize\n\n            y = standardize(y)\n\n            from scipy.stats import norm\n\n            cutoff = norm.ppf(1. - self.prevalence)\n            new_y = np.zeros_like(y, dtype=int)\n            new_y[y &gt; cutoff] = 1\n        else:\n            new_y = y\n\n        self.set_phenotype(new_y)\n\n        return new_y\n\n    def simulate(self,\n                 reset_beta=True,\n                 reset_mixture_assignment=True,\n                 perform_gwas=False):\n        \"\"\"\n        A convenience method to simulate all the components of the generative model.\n        Specifically, the simulation follows the standard linear model, where the phenotype is \n        dependent on the genotype + environmental components that are assumed to be uncorrelated:\n\n        `Y = XB + e`\n\n        Where `Y` is the vector of phenotypes, `X` is the genotype matrix, `B` is the vector of effect sizes, \n        and `e` represents the residual effects. The generative model proceeds by:\n\n         1) Drawing the effect sizes `beta` from a Gaussian mixture density.\n         2) Drawing the residual effect from an isotropic Gaussian density.\n         3) Setting the phenotype according to the equation above. \n\n        :param reset_beta: If True, reset the effect sizes by drawing new ones from the prior density.\n        :param reset_mixture_assignment: If True, reset the assignment of SNPs to mixture components. Set to False\n        if you'd like to keep the same configuration of causal SNPs.\n        :param perform_gwas: If True, automatically perform genome-wide association on the newly simulated phenotype.\n        \"\"\"\n\n        # Simulate the mixture assignment:\n        if self.mixture_assignment is None or reset_mixture_assignment:\n            self.simulate_mixture_assignment()\n\n        # Set the per-SNP heritability based on the mixture assignment:\n        self.set_per_snp_heritability()\n\n        # Simulate betas based on per-SNP heritability\n        if self.beta is None or reset_beta:\n            self.simulate_beta()\n\n        # Simulate the phenotype\n        self.simulate_phenotype()\n\n        if perform_gwas:\n            # Perform genome-wide association testing...\n            self.perform_gwas()\n\n    def to_true_beta_table(self, per_chromosome=False):\n        \"\"\"\n        Export the simulated true effect sizes and causal status into a pandas dataframe.\n        :param per_chromosome: If True, return a dictionary of tables for each chromosome separately.\n\n        :return: A pandas DataFrame with the true effect sizes and causal status for each variant.\n        \"\"\"\n\n        assert self.beta is not None\n\n        eff_tables = {}\n        causal_status = self.get_causal_status()\n\n        for c in self.chromosomes:\n\n            eff_tables[c] = pd.DataFrame({\n                'CHR': c,\n                'SNP': self.genotype[c].snps,\n                'A1': self.genotype[c].a1,\n                'A2': self.genotype[c].a2,\n                'MixtureComponent': np.where(self.mixture_assignment[c] == 1)[1],\n                'Heritability': self.per_snp_h2[c],\n                'BETA': self.beta[c].flatten(),\n                'Causal': causal_status[c],\n            })\n\n        if per_chromosome:\n            return eff_tables\n        else:\n            return pd.concat(list(eff_tables.values()))\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.n_components","title":"<code>n_components</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of Gaussian mixture components for the effect size distribution.</p>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.__init__","title":"<code>__init__(bed_files, h2=0.2, pi=0.1, d=(0.0, 1.0), prevalence=0.15, **kwargs)</code>","text":"<p>Initialize the PhenotypeSimulator object with the necessary parameters.</p> <p>Parameters:</p> Name Type Description Default <code>bed_files</code> <p>A path (or list of paths) to PLINK BED files containing the genotype information.</p> required <code>h2</code> <p>The trait SNP heritability, or proportion of variance explained by SNPs.</p> <code>0.2</code> <code>pi</code> <p>The mixing proportions for the mixture of Gaussians (our model for the distribution of effect sizes). If a float is provided, it is converted to a tuple (1-pi, pi), where pi is the proportion of causal variants.</p> <code>0.1</code> <code>d</code> <p>The variance multipliers for each component of the Gaussian mixture density. By default, all components have the same variance multiplier.</p> <code>(0.0, 1.0)</code> <code>prevalence</code> <p>The (disease) prevalence for binary (case-control) phenotypes.</p> <code>0.15</code> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def __init__(self,\n             bed_files,\n             h2=0.2,\n             pi=0.1,\n             d=(0., 1.),\n             prevalence=0.15,\n             **kwargs):\n    \"\"\"\n    Initialize the PhenotypeSimulator object with the necessary parameters.\n\n    :param bed_files: A path (or list of paths) to PLINK BED files containing the genotype information.\n    :param h2: The trait SNP heritability, or proportion of variance explained by SNPs.\n    :param pi: The mixing proportions for the mixture of Gaussians (our model for the distribution of effect sizes).\n    If a float is provided, it is converted to a tuple (1-pi, pi), where pi is the proportion of causal variants.\n    :param d:  The variance multipliers for each component of the Gaussian mixture density. By default,\n    all components have the same variance multiplier.\n    :param prevalence: The (disease) prevalence for binary (case-control) phenotypes.\n    \"\"\"\n\n    super().__init__(bed_files, **kwargs)\n\n    # If pi is float, convert it to a tuple:\n    if isinstance(pi, float):\n        pi = (1. - pi, pi)\n\n    self.pi = pi\n    self.h2 = h2\n    self.prevalence = prevalence\n\n    # Sanity checks:\n    assert 0. &lt;= self.h2 &lt;= 1.\n    assert round(sum(self.pi), 1) == 1.\n    assert 0. &lt; self.prevalence &lt; 1.\n\n    self.d = np.array(d)\n\n    self.per_snp_h2 = None\n    self.per_snp_pi = None\n    self.beta = None\n    self.mixture_assignment = None\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.get_causal_status","title":"<code>get_causal_status()</code>","text":"<p>Returns:</p> Type Description <p>A dictionary where the keys are the chromosome numbers and the values are binary vectors indicating which SNPs are causal for the simulated phenotype.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the mixture assignment is not set.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def get_causal_status(self):\n    \"\"\"\n    :return: A dictionary where the keys are the chromosome numbers\n    and the values are binary vectors indicating which SNPs are\n    causal for the simulated phenotype.\n\n    :raises AssertionError: If the mixture assignment is not set.\n    \"\"\"\n\n    assert self.mixture_assignment is not None\n\n    try:\n        zero_index = list(self.d).index(0)\n    except ValueError:\n        # If all SNPs are causal:\n        return {c: np.repeat(True, c_size) for c, c_size in self.shapes.items()}\n\n    causal_status = {}\n\n    for c, mix_a in self.mixture_assignment.items():\n        causal_status[c] = np.where(mix_a)[1] != zero_index\n\n    return causal_status\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_beta","title":"<code>set_beta(new_beta)</code>","text":"<p>Set the variant effect sizes (beta) according to user-provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>new_beta</code> <p>A dictionary where the keys are the chromosomes and the values are the beta (effect size) for each SNP on that chromosome.</p> required Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_beta(self, new_beta):\n    \"\"\"\n    Set the variant effect sizes (beta) according to user-provided dictionary.\n\n    :param new_beta: A dictionary where the keys are the chromosomes and\n    the values are the beta (effect size) for each SNP on that chromosome.\n    \"\"\"\n\n    # Check that the shapes match pre-specified information:\n    for c, c_size in self.shapes.items():\n        assert len(new_beta[c]) == c_size\n\n    self.beta = new_beta\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_causal_snps","title":"<code>set_causal_snps(causal_snps)</code>","text":"<p>A utility method to set the causal variants in the simulation based on an array or list of SNPs specified by the user. The method takes an iterable (e.g. list or array) of <code>causal_snps</code> and then creates a new mixture assignment object where only the <code>causal_snps</code> contribute to the phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>causal_snps</code> <p>A list or array of SNP rsIDs.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If all mixture components are causal.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_causal_snps(self, causal_snps):\n    \"\"\"\n    A utility method to set the causal variants in the simulation based on an array or\n    list of SNPs specified by the user. The method takes an iterable (e.g. list or array) of `causal_snps`\n    and then creates a new mixture assignment object where only the `causal_snps`\n    contribute to the phenotype.\n\n    :param causal_snps: A list or array of SNP rsIDs.\n    :raises ValueError: If all mixture components are causal.\n\n    \"\"\"\n\n    # Get the index of the mixture component whose multiplier is zero (i.e. the null component):\n    try:\n        zero_index = list(self.d).index(0)\n    except ValueError:\n        raise ValueError(\"Cannot set causal variants when all mixture components are causal. Modify \"\n                         \"the mixture multipliers `d` to enable this functionality.\")\n\n    # Get the indices of the non-null mixture components:\n    nonzero_indices = [i for i, d in enumerate(self.d) if d != 0.]\n\n    # Get the mixture proportions for the non-null components and normalize them so they sum to 1:\n    pis = np.array(self.pi)[nonzero_indices]\n    pis /= pis.sum()\n\n    # Initialize new mixture assignment object:\n    new_assignment = {c: np.zeros((s, self.n_components)) for c, s in self.shapes.items()}\n\n    from ..utils.compute_utils import intersect_arrays\n\n    n_causal_set = 0\n\n    for c, snps in self.snps.items():\n\n        # Intersect the list of causal SNPs with the SNPs on chromosome `c`:\n        snp_idx = intersect_arrays(snps, causal_snps, return_index=True)\n\n        if len(snp_idx) &gt; 0:\n            n_causal_set += len(snp_idx)\n            # For the causal SNPs, assign them randomly to the causal components\n            new_assignment[c][snp_idx, np.random.choice(nonzero_indices,\n                                                        size=len(snp_idx),\n                                                        p=pis)] = 1\n            # For the remaining SNPs, assign them to the null component:\n            new_assignment[c][:, zero_index] = np.abs(new_assignment[c].sum(axis=1) - 1)\n\n    if n_causal_set &lt; len(causal_snps):\n        warnings.warn(\"Not all causal SNPs are represented in the genotype matrix! \"\n                      f\"User passed a list of {len(causal_snps)} SNPs, only matched {n_causal_set}.\")\n\n    self.set_mixture_assignment(new_assignment)\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_h2","title":"<code>set_h2(new_h2)</code>","text":"<p>Set the total heritability (proportion of additive variance due to SNPs) for the trait</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_h2(self, new_h2):\n    \"\"\"\n    Set the total heritability (proportion of additive variance due to SNPs) for the trait\n    \"\"\"\n    self.h2 = new_h2\n    self.set_per_snp_heritability()\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_mixture_assignment","title":"<code>set_mixture_assignment(new_assignment)</code>","text":"<p>Set the mixture assignments according to user-provided dictionary. The mixture assignment indicates which mixture component the effect size of a particular variant comes from.</p> <p>Parameters:</p> Name Type Description Default <code>new_assignment</code> <p>A dictionary where the keys are the chromosomes and the values are the mixture assignment for each SNP on that chromosome.</p> required Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_mixture_assignment(self, new_assignment):\n    \"\"\"\n    Set the mixture assignments according to user-provided dictionary. The mixture\n    assignment indicates which mixture component the effect size of a particular\n    variant comes from.\n    :param new_assignment: A dictionary where the keys are the chromosomes and\n    the values are the mixture assignment for each SNP on that chromosome.\n    \"\"\"\n\n    # Check that the shapes match pre-specified information:\n    for c, c_size in self.shapes.items():\n        assert new_assignment[c].shape == (c_size, self.n_components)\n\n    self.mixture_assignment = new_assignment\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_per_snp_heritability","title":"<code>set_per_snp_heritability()</code>","text":"<p>Set the per-SNP heritability (effect size variance) for each variant in the dataset. This is a convenience method that may come in handy for more flexible generative models.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_per_snp_heritability(self):\n    \"\"\"\n    Set the per-SNP heritability (effect size variance) for each variant in the dataset.\n    This is a convenience method that may come in handy for more flexible generative models.\n    \"\"\"\n\n    assert self.mixture_assignment is not None\n\n    # Estimate the global sigma_beta_sq based on the\n    # pre-specified heritability, the mixture proportions `pi`,\n    # and the prior multipliers `d`.\n\n    combined_assignments = np.concatenate([self.mixture_assignment[c] for c in self.chromosomes])\n\n    sigma_beta_sq = self.h2 / (combined_assignments*self.d).sum()\n\n    self.per_snp_h2 = {}\n\n    for c, c_size in self.shapes.items():\n        self.per_snp_h2[c] = sigma_beta_sq*self.d[np.where(self.mixture_assignment[c])[1]]\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_per_snp_mixture_probability","title":"<code>set_per_snp_mixture_probability()</code>","text":"<p>Set the per-SNP mixing proportions for each variant in the dataset. This is a convenience method that may come in handy for more flexible generative models.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_per_snp_mixture_probability(self):\n    \"\"\"\n    Set the per-SNP mixing proportions for each variant in the dataset.\n    This is a convenience method that may come in handy for more flexible generative models.\n    \"\"\"\n\n    self.per_snp_pi = {}\n\n    for c, c_size in self.shapes.items():\n        self.per_snp_pi[c] = np.repeat(np.array([self.pi]), c_size, axis=0)\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.set_pi","title":"<code>set_pi(new_pi)</code>","text":"<p>Set the mixture proportions (proportion of variants in each Gaussian mixture component).</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def set_pi(self, new_pi):\n    \"\"\"\n    Set the mixture proportions (proportion of variants in each\n    Gaussian mixture component).\n    \"\"\"\n    self.pi = new_pi\n    self.set_per_snp_mixture_probability()\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.simulate","title":"<code>simulate(reset_beta=True, reset_mixture_assignment=True, perform_gwas=False)</code>","text":"<p>A convenience method to simulate all the components of the generative model. Specifically, the simulation follows the standard linear model, where the phenotype is  dependent on the genotype + environmental components that are assumed to be uncorrelated:</p> <p><code>Y = XB + e</code></p> <p>Where <code>Y</code> is the vector of phenotypes, <code>X</code> is the genotype matrix, <code>B</code> is the vector of effect sizes,  and <code>e</code> represents the residual effects. The generative model proceeds by:</p> <p>1) Drawing the effect sizes <code>beta</code> from a Gaussian mixture density.  2) Drawing the residual effect from an isotropic Gaussian density.  3) Setting the phenotype according to the equation above. </p> <p>Parameters:</p> Name Type Description Default <code>reset_beta</code> <p>If True, reset the effect sizes by drawing new ones from the prior density.</p> <code>True</code> <code>reset_mixture_assignment</code> <p>If True, reset the assignment of SNPs to mixture components. Set to False if you'd like to keep the same configuration of causal SNPs.</p> <code>True</code> <code>perform_gwas</code> <p>If True, automatically perform genome-wide association on the newly simulated phenotype.</p> <code>False</code> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def simulate(self,\n             reset_beta=True,\n             reset_mixture_assignment=True,\n             perform_gwas=False):\n    \"\"\"\n    A convenience method to simulate all the components of the generative model.\n    Specifically, the simulation follows the standard linear model, where the phenotype is \n    dependent on the genotype + environmental components that are assumed to be uncorrelated:\n\n    `Y = XB + e`\n\n    Where `Y` is the vector of phenotypes, `X` is the genotype matrix, `B` is the vector of effect sizes, \n    and `e` represents the residual effects. The generative model proceeds by:\n\n     1) Drawing the effect sizes `beta` from a Gaussian mixture density.\n     2) Drawing the residual effect from an isotropic Gaussian density.\n     3) Setting the phenotype according to the equation above. \n\n    :param reset_beta: If True, reset the effect sizes by drawing new ones from the prior density.\n    :param reset_mixture_assignment: If True, reset the assignment of SNPs to mixture components. Set to False\n    if you'd like to keep the same configuration of causal SNPs.\n    :param perform_gwas: If True, automatically perform genome-wide association on the newly simulated phenotype.\n    \"\"\"\n\n    # Simulate the mixture assignment:\n    if self.mixture_assignment is None or reset_mixture_assignment:\n        self.simulate_mixture_assignment()\n\n    # Set the per-SNP heritability based on the mixture assignment:\n    self.set_per_snp_heritability()\n\n    # Simulate betas based on per-SNP heritability\n    if self.beta is None or reset_beta:\n        self.simulate_beta()\n\n    # Simulate the phenotype\n    self.simulate_phenotype()\n\n    if perform_gwas:\n        # Perform genome-wide association testing...\n        self.perform_gwas()\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.simulate_beta","title":"<code>simulate_beta()</code>","text":"<p>Simulate the causal effect size for variants included in the dataset. Here, the variant effect size is drawn from a Gaussian density with mean zero and scale given by the root of per-SNP heritability.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def simulate_beta(self):\n    \"\"\"\n    Simulate the causal effect size for variants included\n    in the dataset. Here, the variant effect size is drawn from\n    a Gaussian density with mean zero and scale given by\n    the root of per-SNP heritability.\n    \"\"\"\n\n    if self.per_snp_h2 is None or len(self.per_snp_h2) &lt; 1:\n        self.set_per_snp_heritability()\n\n    self.beta = {}\n\n    for c, c_size in self.shapes.items():\n\n        self.beta[c] = np.random.normal(loc=0.0,\n                                        scale=np.sqrt(self.per_snp_h2[c]),\n                                        size=c_size)\n\n    return self.beta\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.simulate_mixture_assignment","title":"<code>simulate_mixture_assignment()</code>","text":"<p>Simulate assigning SNPs to the various mixture components with probabilities given by mixing proportions <code>pi</code>.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def simulate_mixture_assignment(self):\n    \"\"\"\n    Simulate assigning SNPs to the various mixture components\n    with probabilities given by mixing proportions `pi`.\n    \"\"\"\n\n    if self.per_snp_pi is None or len(self.per_snp_pi) &lt; 1:\n        self.set_per_snp_mixture_probability()\n\n    self.mixture_assignment = {}\n\n    from ..utils.model_utils import multinomial_rvs\n\n    for c, c_size in self.shapes.items():\n\n        self.mixture_assignment[c] = multinomial_rvs(1, self.per_snp_pi[c])\n\n    return self.mixture_assignment\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.simulate_phenotype","title":"<code>simulate_phenotype()</code>","text":"<p>Simulate complex phenotypes for the samples present in the genotype matrix, given their genotype information and fixed effect sizes <code>beta</code> that were simulated previous steps.</p> <p>Given the simulated effect sizes, the phenotype is generated as follows:</p> <p><code>Y = XB + e</code></p> <p>Where <code>Y</code> is the vector of phenotypes, <code>X</code> is the genotype matrix, <code>B</code> is the vector of effect sizes, and <code>e</code> represents the residual effects.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def simulate_phenotype(self):\n    \"\"\"\n    Simulate complex phenotypes for the samples present in the genotype matrix, given their\n    genotype information and fixed effect sizes `beta` that were simulated previous steps.\n\n    Given the simulated effect sizes, the phenotype is generated as follows:\n\n    `Y = XB + e`\n\n    Where `Y` is the vector of phenotypes, `X` is the genotype matrix, `B` is the vector of effect sizes,\n    and `e` represents the residual effects.\n    \"\"\"\n\n    assert self.beta is not None\n\n    # Compute the polygenic score given the simulated/provided beta:\n    # NOTE: For this formulation to work, it's important to standardize the genotype.\n    pgs = self.score(self.beta, standardize_genotype=True)\n\n    # Sample the environmental/residual component:\n    e = np.random.normal(loc=0., scale=np.sqrt(1. - self.h2), size=self.sample_size)\n\n    # The final simulated phenotype is a combination of\n    # the polygenic score + the residual component:\n    y = pgs + e\n\n    if self.phenotype_likelihood == 'binomial':\n        # If the simulated phenotype is to be binary,\n        # use the threshold model to determine positives/negatives\n        # based on the prevalence of the phenotype in the population:\n\n        from ..stats.transforms.phenotype import standardize\n\n        y = standardize(y)\n\n        from scipy.stats import norm\n\n        cutoff = norm.ppf(1. - self.prevalence)\n        new_y = np.zeros_like(y, dtype=int)\n        new_y[y &gt; cutoff] = 1\n    else:\n        new_y = y\n\n    self.set_phenotype(new_y)\n\n    return new_y\n</code></pre>"},{"location":"api/simulation/PhenotypeSimulator/#magenpy.simulation.PhenotypeSimulator.PhenotypeSimulator.to_true_beta_table","title":"<code>to_true_beta_table(per_chromosome=False)</code>","text":"<p>Export the simulated true effect sizes and causal status into a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>per_chromosome</code> <p>If True, return a dictionary of tables for each chromosome separately.</p> <code>False</code> <p>Returns:</p> Type Description <p>A pandas DataFrame with the true effect sizes and causal status for each variant.</p> Source code in <code>magenpy/simulation/PhenotypeSimulator.py</code> <pre><code>def to_true_beta_table(self, per_chromosome=False):\n    \"\"\"\n    Export the simulated true effect sizes and causal status into a pandas dataframe.\n    :param per_chromosome: If True, return a dictionary of tables for each chromosome separately.\n\n    :return: A pandas DataFrame with the true effect sizes and causal status for each variant.\n    \"\"\"\n\n    assert self.beta is not None\n\n    eff_tables = {}\n    causal_status = self.get_causal_status()\n\n    for c in self.chromosomes:\n\n        eff_tables[c] = pd.DataFrame({\n            'CHR': c,\n            'SNP': self.genotype[c].snps,\n            'A1': self.genotype[c].a1,\n            'A2': self.genotype[c].a2,\n            'MixtureComponent': np.where(self.mixture_assignment[c] == 1)[1],\n            'Heritability': self.per_snp_h2[c],\n            'BETA': self.beta[c].flatten(),\n            'Causal': causal_status[c],\n        })\n\n    if per_chromosome:\n        return eff_tables\n    else:\n        return pd.concat(list(eff_tables.values()))\n</code></pre>"},{"location":"api/stats/gwa/utils/","title":"Utils","text":""},{"location":"api/stats/gwa/utils/#magenpy.stats.gwa.utils.inflation_factor","title":"<code>inflation_factor(sumstats_input)</code>","text":"<p>Compute the genomic control (GC) inflation factor (also known as lambda) from GWAS summary statistics.</p> <p>The inflation factor can be used to detect and correct inflation in the test statistics.</p> <p>Parameters:</p> Name Type Description Default <code>sumstats_input</code> <code>Union[GWADataLoader, SumstatsTable, array]</code> <p>The input can be one of three classes of objects: A GWADataLoader object, a SumstatsTable object, or a numpy array of chi-squared statistics to compute the inflation factor.</p> required <p>Returns:</p> Type Description <p>The inflation factor (lambda) computed from the chi-squared statistics.</p> Source code in <code>magenpy/stats/gwa/utils.py</code> <pre><code>def inflation_factor(sumstats_input: Union[GWADataLoader, SumstatsTable, np.array]):\n    \"\"\"\n    Compute the genomic control (GC) inflation factor (also known as lambda)\n    from GWAS summary statistics.\n\n    The inflation factor can be used to detect and correct inflation in the test statistics.\n\n    :param sumstats_input: The input can be one of three classes of objects: A GWADataLoader object,\n    a SumstatsTable object, or a numpy array of chi-squared statistics to compute the inflation factor.\n\n    :return: The inflation factor (lambda) computed from the chi-squared statistics.\n    \"\"\"\n\n    if isinstance(sumstats_input, GWADataLoader):\n        chisq = np.concatenate([ss.get_chisq_statistic() for ss in sumstats_input.sumstats_table.values()])\n    elif isinstance(sumstats_input, SumstatsTable):\n        chisq = sumstats_input.get_chisq_statistic()\n    else:\n        chisq = sumstats_input\n\n    from scipy.stats import chi2\n\n    return np.median(chisq) / chi2.median(1)\n</code></pre>"},{"location":"api/stats/gwa/utils/#magenpy.stats.gwa.utils.perform_gwa_plink1p9","title":"<code>perform_gwa_plink1p9(genotype_matrix, temp_dir='temp', **phenotype_transform_kwargs)</code>","text":"<p>Perform genome-wide association testing using plink 1.9 This function takes a GenotypeMatrix object and gwas-related flags and calls plink to perform GWA on the genotype and phenotype data referenced by the GenotypeMatrix object.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A plinkBEDGenotypeMatrix object.</p> required <code>temp_dir</code> <p>Path to a directory where we keep intermediate temporary files from plink.</p> <code>'temp'</code> <code>phenotype_transform_kwargs</code> <p>Keyword arguments to pass to the <code>chained_transform</code> function. These arguments include the following options to transform the phenotype before performing GWAS: <code>adjust_covariates</code>, <code>standardize_phenotype</code>, <code>rint_phenotype</code>, and <code>outlier_sigma_threshold</code>. NOTE: These transformations are only applied to continuous phenotypes (<code>likelihood='gaussian'</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>A SumstatsTable object containing the summary statistics from the association tests.</p> Source code in <code>magenpy/stats/gwa/utils.py</code> <pre><code>def perform_gwa_plink1p9(genotype_matrix,\n                         temp_dir='temp',\n                         **phenotype_transform_kwargs):\n    \"\"\"\n    Perform genome-wide association testing using plink 1.9\n    This function takes a GenotypeMatrix object and gwas-related flags and\n    calls plink to perform GWA on the genotype and phenotype data referenced\n    by the GenotypeMatrix object.\n\n    :param genotype_matrix: A plinkBEDGenotypeMatrix object.\n    :param temp_dir: Path to a directory where we keep intermediate temporary files from plink.\n    :param phenotype_transform_kwargs: Keyword arguments to pass to the `chained_transform` function. These arguments\n    include the following options to transform the phenotype before performing GWAS:\n    `adjust_covariates`, `standardize_phenotype`, `rint_phenotype`, and `outlier_sigma_threshold`. NOTE: These\n    transformations are only applied to continuous phenotypes (`likelihood='gaussian'`).\n\n    :return: A SumstatsTable object containing the summary statistics from the association tests.\n    \"\"\"\n\n    from ...GenotypeMatrix import plinkBEDGenotypeMatrix\n    from ...utils.executors import plink1Executor\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink1 = plink1Executor()\n\n    s_table = genotype_matrix.sample_table\n\n    if s_table.phenotype_likelihood is None:\n        warnings.warn(\"The phenotype likelihood is not specified! \"\n                      \"Assuming that the phenotype is continuous...\")\n\n    # Transform the phenotype:\n    phenotype, mask = chained_transform(s_table, **phenotype_transform_kwargs)\n\n    # Prepare the phenotype table to pass to plink:\n    phe_table = s_table.get_phenotype_table()\n\n    # If the likelihood is binomial, transform the phenotype into\n    # plink's coding for case/control (1/2) rather than (0/1).\n    if s_table.phenotype_likelihood == 'binomial':\n        phe_table['phenotype'] += 1\n    else:\n        phe_table = phe_table.loc[mask, :]\n        phe_table['phenotype'] = phenotype\n\n    # Output phenotype table:\n    phe_fname = osp.join(temp_dir, \"pheno.txt\")\n    phe_table.to_csv(phe_fname, sep=\"\\t\", index=False, header=False)\n\n    # Process covariates:\n    if s_table.phenotype_likelihood == 'binomial' and 'adjust_covariates' in phenotype_transform_kwargs and \\\n            phenotype_transform_kwargs['adjust_covariates']:\n\n        covar_fname = osp.join(temp_dir, \"covar.txt\")\n        covar = s_table.get_covariates_table().loc[mask, :]\n        covar.to_csv(covar_fname, sep=\"\\t\", index=False, header=False)\n    else:\n        covar_fname = None\n\n    # Determine regression type based on phenotype likelihood:\n    plink_reg_type = ['linear', 'logistic'][s_table.phenotype_likelihood == 'binomial']\n\n    # Output subset of SNPs to perform association tests on:\n    snp_keepfile = osp.join(temp_dir, \"variants.keep\")\n    pd.DataFrame({'SNP': genotype_matrix.snps}).to_csv(\n        snp_keepfile, index=False, header=False\n    )\n\n    # Output file:\n    plink_output = osp.join(temp_dir, \"output\")\n\n    cmd = [\n        f\"--bfile {genotype_matrix.bed_file}\",\n        f\"--extract {snp_keepfile}\",\n        f\"--{plink_reg_type} hide-covar\",\n        f\"--pheno {phe_fname}\",\n        f\"--out {plink_output}\"\n    ]\n\n    if covar_fname is not None:\n        cmd.append(f'--covar {covar_fname}')\n\n    plink1.execute(cmd)\n\n    output_fname = plink_output + f\".PHENO1.assoc.{plink_reg_type}\"\n\n    if not osp.isfile(output_fname):\n        if plink_reg_type == 'logistic' and osp.isfile(output_fname + \".hybrid\"):\n            output_fname += \".hybrid\"\n        else:\n            raise FileNotFoundError\n\n    # Read the summary statistics file from plink:\n    ss_table = SumstatsTable.from_file(output_fname, sumstats_format='plink1.9')\n    # Infer the reference allele:\n    ss_table.infer_a2(genotype_matrix.snp_table)\n\n    # Make sure that the effect allele is encoded properly:\n    ss_table.match(genotype_matrix.snp_table, correct_flips=True)\n\n    return ss_table\n</code></pre>"},{"location":"api/stats/gwa/utils/#magenpy.stats.gwa.utils.perform_gwa_plink2","title":"<code>perform_gwa_plink2(genotype_matrix, temp_dir='temp', **phenotype_transform_kwargs)</code>","text":"<p>Perform genome-wide association testing using plink 2.0 This function takes a GenotypeMatrix object and gwas-related flags and calls plink to perform GWA on the genotype and phenotype data referenced by the GenotypeMatrix object.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A plinkBEDGenotypeMatrix object.</p> required <code>temp_dir</code> <p>Path to a directory where we keep intermediate temporary files from plink.</p> <code>'temp'</code> <code>phenotype_transform_kwargs</code> <p>Keyword arguments to pass to the <code>chained_transform</code> function. These arguments include the following options to transform the phenotype before performing GWAS: <code>adjust_covariates</code>, <code>standardize_phenotype</code>, <code>rint_phenotype</code>, and <code>outlier_sigma_threshold</code>. NOTE: These transformations are only applied to continuous phenotypes (<code>likelihood='gaussian'</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>A SumstatsTable object containing the summary statistics from the association tests.</p> Source code in <code>magenpy/stats/gwa/utils.py</code> <pre><code>def perform_gwa_plink2(genotype_matrix,\n                       temp_dir='temp',\n                       **phenotype_transform_kwargs):\n    \"\"\"\n\n    Perform genome-wide association testing using plink 2.0\n    This function takes a GenotypeMatrix object and gwas-related flags and\n    calls plink to perform GWA on the genotype and phenotype data referenced\n    by the GenotypeMatrix object.\n\n    :param genotype_matrix: A plinkBEDGenotypeMatrix object.\n    :param temp_dir: Path to a directory where we keep intermediate temporary files from plink.\n    :param phenotype_transform_kwargs: Keyword arguments to pass to the `chained_transform` function. These arguments\n    include the following options to transform the phenotype before performing GWAS:\n    `adjust_covariates`, `standardize_phenotype`, `rint_phenotype`, and `outlier_sigma_threshold`. NOTE: These\n    transformations are only applied to continuous phenotypes (`likelihood='gaussian'`).\n\n    :return: A SumstatsTable object containing the summary statistics from the association tests.\n    \"\"\"\n\n    from ...GenotypeMatrix import plinkBEDGenotypeMatrix\n    from ...utils.executors import plink2Executor\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink2 = plink2Executor()\n\n    s_table = genotype_matrix.sample_table\n\n    if s_table.phenotype_likelihood is None:\n        warnings.warn(\"The phenotype likelihood is not specified! \"\n                      \"Assuming that the phenotype is continuous...\")\n\n    # It can happen sometimes that some interfaces call this function\n    # with the `standardize_genotype` flag set to True. We remove this\n    # flag from the phenotype transformation kwargs to avoid errors:\n    if 'standardize_genotype' in phenotype_transform_kwargs:\n        del phenotype_transform_kwargs['standardize_genotype']\n\n    # Transform the phenotype:\n    phenotype, mask = chained_transform(s_table, **phenotype_transform_kwargs)\n\n    # Prepare the phenotype table to pass to plink:\n    phe_table = s_table.get_phenotype_table()\n\n    # If the likelihood is binomial, transform the phenotype into\n    # plink's coding for case/control (1/2) rather than (0/1).\n    if s_table.phenotype_likelihood == 'binomial':\n        phe_table['phenotype'] += 1\n    else:\n        phe_table = phe_table.loc[mask, :]\n        phe_table['phenotype'] = phenotype\n\n    # Output phenotype table:\n    phe_fname = osp.join(temp_dir, \"pheno.txt\")\n    phe_table.to_csv(phe_fname, sep=\"\\t\", index=False, header=False)\n\n    # Process covariates:\n    if s_table.phenotype_likelihood == 'binomial' and 'adjust_covariates' in phenotype_transform_kwargs and \\\n            phenotype_transform_kwargs['adjust_covariates']:\n\n        covar_fname = osp.join(temp_dir, \"covar.txt\")\n        covar = s_table.get_covariates_table().loc[mask, :]\n        covar.to_csv(covar_fname, sep=\"\\t\", index=False, header=False)\n        covar_modifier = ''\n    else:\n        covar_fname = None\n        covar_modifier = ' allow-no-covars'\n\n    # Determine regression type based on phenotype likelihood:\n    plink_reg_type = ['linear', 'logistic'][s_table.phenotype_likelihood == 'binomial']\n\n    # Output subset of SNPs to perform association tests on:\n    snp_keepfile = osp.join(temp_dir, \"variants.keep\")\n    pd.DataFrame({'SNP': genotype_matrix.snps}).to_csv(\n        snp_keepfile, index=False, header=False\n    )\n\n    # Output file:\n    plink_output = osp.join(temp_dir, \"output\")\n\n    cmd = [\n        f\"--bfile {genotype_matrix.bed_file}\",\n        f\"--extract {snp_keepfile}\",\n        f\"--{plink_reg_type} hide-covar{covar_modifier} cols=chrom,pos,alt1,ref,a1freq,nobs,beta,se,tz,p\",\n        f\"--pheno {phe_fname}\",\n        f\"--out {plink_output}\"\n    ]\n\n    if covar_fname is not None:\n        cmd.append(f'--covar {covar_fname}')\n\n    plink2.execute(cmd)\n\n    output_fname = plink_output + f\".PHENO1.glm.{plink_reg_type}\"\n\n    if not osp.isfile(output_fname):\n        if plink_reg_type == 'logistic' and osp.isfile(output_fname + \".hybrid\"):\n            output_fname += \".hybrid\"\n        else:\n            raise FileNotFoundError\n\n    # Read the summary statistics file from plink:\n    ss_table = SumstatsTable.from_file(output_fname, sumstats_format='plink2')\n    # Make sure that the effect allele is encoded properly:\n    ss_table.match(genotype_matrix.snp_table, correct_flips=True)\n\n    return ss_table\n</code></pre>"},{"location":"api/stats/gwa/utils/#magenpy.stats.gwa.utils.perform_gwa_xarray","title":"<code>perform_gwa_xarray(genotype_matrix, standardize_genotype=False, **phenotype_transform_kwargs)</code>","text":"<p>Perform genome-wide association testing using xarray and the PyData ecosystem. This function takes a GenotypeMatrix object and gwas-related flags and calls performs (simple) GWA on the genotype and phenotype data referenced by the GenotypeMatrix object. This function only implements GWA testing for continuous phenotypes. For other functionality (e.g. case-control GWAS), please use <code>plink</code> as a backend or consult other GWAS software (e.g. GCTA or REGENIE).</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A GenotypeMatrix object.</p> required <code>standardize_genotype</code> <p>If True, the genotype matrix will be standardized such that the columns (i.e. SNPs) have zero mean and unit variance.</p> <code>False</code> <code>phenotype_transform_kwargs</code> <p>Keyword arguments to pass to the <code>chained_transform</code> function. These arguments include the following options to transform the phenotype before performing GWAS: <code>adjust_covariates</code>, <code>standardize_phenotype</code>, <code>rint_phenotype</code>, and <code>outlier_sigma_threshold</code>. NOTE: These transformations are only applied to continuous phenotypes (<code>likelihood='gaussian'</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>A SumstatsTable object containing the summary statistics from the association tests.</p> Source code in <code>magenpy/stats/gwa/utils.py</code> <pre><code>def perform_gwa_xarray(genotype_matrix,\n                       standardize_genotype=False,\n                       **phenotype_transform_kwargs):\n    \"\"\"\n    Perform genome-wide association testing using xarray and the PyData ecosystem.\n    This function takes a GenotypeMatrix object and gwas-related flags and\n    calls performs (simple) GWA on the genotype and phenotype data referenced\n    by the GenotypeMatrix object. This function only implements GWA testing for\n    continuous phenotypes. For other functionality (e.g. case-control GWAS),\n    please use `plink` as a backend or consult other GWAS software (e.g. GCTA or REGENIE).\n\n    :param genotype_matrix: A GenotypeMatrix object.\n    :param standardize_genotype: If True, the genotype matrix will be standardized such that the columns (i.e. SNPs)\n    have zero mean and unit variance.\n    :param phenotype_transform_kwargs: Keyword arguments to pass to the `chained_transform` function. These arguments\n    include the following options to transform the phenotype before performing GWAS:\n    `adjust_covariates`, `standardize_phenotype`, `rint_phenotype`, and `outlier_sigma_threshold`. NOTE: These\n    transformations are only applied to continuous phenotypes (`likelihood='gaussian'`).\n\n    :return: A SumstatsTable object containing the summary statistics from the association tests.\n    \"\"\"\n\n    # Sanity checks:\n\n    # Check that the genotype matrix is an xarrayGenotypeMatrix object.\n    from ...GenotypeMatrix import xarrayGenotypeMatrix\n    assert isinstance(genotype_matrix, xarrayGenotypeMatrix)\n\n    # Check that the phenotype likelihood is set correctly and that the phenotype is continuous.\n    if genotype_matrix.sample_table.phenotype_likelihood is None:\n        warnings.warn(\"The phenotype likelihood is not specified! \"\n                      \"Assuming that the phenotype is continuous...\")\n    elif genotype_matrix.sample_table.phenotype_likelihood == 'binomial':\n        raise ValueError(\"The xarray backend currently does not support performing association \"\n                         \"testing on binary (case-control) phenotypes! Try setting the backend to `plink` or \"\n                         \"use external software (e.g. GCTA or REGENIE) for performing GWAS.\")\n\n    # -----------------------------------------------------------\n\n    # Get the SNP table from the genotype_matrix object:\n    sumstats_table = genotype_matrix.get_snp_table(\n        ['CHR', 'SNP', 'POS', 'A1', 'A2']\n    )\n\n    # -----------------------------------------------------------\n\n    # Transform the phenotype:\n    phenotype, mask = chained_transform(genotype_matrix.sample_table, **phenotype_transform_kwargs)\n\n    # -----------------------------------------------------------\n    # Prepare the genotype data for association testing:\n\n    # Apply the mask to the genotype matrix:\n    xr_mat = genotype_matrix.xr_mat[mask, :]\n\n    # Compute sample size per SNP:\n    n_per_snp = xr_mat.shape[0] - xr_mat.isnull().sum(axis=0).compute().values\n\n    # Compute minor allele frequency per SNP:\n    maf = xr_mat.sum(axis=0).compute().values / (2 * n_per_snp)\n\n    # Standardize or center the genotype matrix (account for missing values):\n    if standardize_genotype:\n        from ..transforms.genotype import standardize\n        xr_mat = standardize(xr_mat)\n    else:\n        xr_mat = (xr_mat - 2.*maf)\n\n    xr_mat = xr_mat.fillna(0.)\n\n    # Compute the sum of squares per SNP:\n    sum_x_sq = (xr_mat**2).sum(axis=0).compute().values\n\n    # -----------------------------------------------------------\n    # Compute quantities for association testing:\n\n    slope = np.dot(xr_mat.T, phenotype - phenotype.mean()) / sum_x_sq\n    intercept = phenotype.mean()\n\n    y_hat = xr_mat*slope + intercept\n\n    s2 = ((phenotype.reshape(-1, 1) - y_hat)**2).sum(axis=0) / (n_per_snp - 2)\n\n    se = np.sqrt(s2 / sum_x_sq)\n\n    # -----------------------------------------------------------\n    # Populate the data in the summary statistics table:\n\n    sumstats_table['MAF'] = maf\n    sumstats_table['N'] = n_per_snp\n    sumstats_table['BETA'] = slope\n    sumstats_table['SE'] = se\n\n    ss_table = SumstatsTable(sumstats_table)\n    # Trigger computing z-score and p-values from the BETA and SE columns:\n    _, _ = ss_table.z_score, ss_table.pval\n\n    return ss_table\n</code></pre>"},{"location":"api/stats/h2/ldsc/","title":"Ldsc","text":""},{"location":"api/stats/h2/ldsc/#magenpy.stats.h2.ldsc.LDSCRegression","title":"<code>LDSCRegression</code>","text":"<p>               Bases: <code>object</code></p> <p>Perform LD Score Regression using the jackknife method.</p> Source code in <code>magenpy/stats/h2/ldsc.py</code> <pre><code>class LDSCRegression(object):\n    \"\"\"\n    Perform LD Score Regression using the jackknife method.\n    \"\"\"\n\n    def __init__(self, gdl: GWADataLoader, n_blocks=200, max_chisq=None):\n        \"\"\"\n        :param gdl: An instance of GWADataLoader\n        :param n_blocks: The number of blocks to use for the jackknife method.\n        :param max_chisq: The maximum Chi-Squared statistic to consider.\n        \"\"\"\n\n        self.gdl = gdl\n        self.n_blocks = n_blocks\n\n        # ...\n\n    def fit(self):\n        \"\"\"\n        Perform LD Score Regression estimation using the jackknife method.\n\n        :raises NotImplementedError: If method is not implemented.\n        \"\"\"\n\n        raise NotImplementedError\n</code></pre>"},{"location":"api/stats/h2/ldsc/#magenpy.stats.h2.ldsc.LDSCRegression.__init__","title":"<code>__init__(gdl, n_blocks=200, max_chisq=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gdl</code> <code>GWADataLoader</code> <p>An instance of GWADataLoader</p> required <code>n_blocks</code> <p>The number of blocks to use for the jackknife method.</p> <code>200</code> <code>max_chisq</code> <p>The maximum Chi-Squared statistic to consider.</p> <code>None</code> Source code in <code>magenpy/stats/h2/ldsc.py</code> <pre><code>def __init__(self, gdl: GWADataLoader, n_blocks=200, max_chisq=None):\n    \"\"\"\n    :param gdl: An instance of GWADataLoader\n    :param n_blocks: The number of blocks to use for the jackknife method.\n    :param max_chisq: The maximum Chi-Squared statistic to consider.\n    \"\"\"\n\n    self.gdl = gdl\n    self.n_blocks = n_blocks\n</code></pre>"},{"location":"api/stats/h2/ldsc/#magenpy.stats.h2.ldsc.LDSCRegression.fit","title":"<code>fit()</code>","text":"<p>Perform LD Score Regression estimation using the jackknife method.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If method is not implemented.</p> Source code in <code>magenpy/stats/h2/ldsc.py</code> <pre><code>def fit(self):\n    \"\"\"\n    Perform LD Score Regression estimation using the jackknife method.\n\n    :raises NotImplementedError: If method is not implemented.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"api/stats/h2/ldsc/#magenpy.stats.h2.ldsc.simple_ldsc","title":"<code>simple_ldsc(gdl)</code>","text":"<p>Provides an estimate of SNP heritability from summary statistics using a simplified version of the LD Score Regression framework. E[X_j^2] = h^2*l_j + int Where the response is the Chi-Squared statistic for SNP j and the variable is its LD score.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <code>GWADataLoader</code> <p>An instance of <code>GWADataLoader</code> with the LD information and summary statistics initialized properly.</p> required <p>Returns:</p> Type Description <p>The estimated SNP heritability.</p> Source code in <code>magenpy/stats/h2/ldsc.py</code> <pre><code>def simple_ldsc(gdl: GWADataLoader):\n    \"\"\"\n    Provides an estimate of SNP heritability from summary statistics using\n    a simplified version of the LD Score Regression framework.\n    E[X_j^2] = h^2*l_j + int\n    Where the response is the Chi-Squared statistic for SNP j\n    and the variable is its LD score.\n\n    :param gdl: An instance of `GWADataLoader` with the LD information and\n    summary statistics initialized properly.\n\n    :return: The estimated SNP heritability.\n    \"\"\"\n\n    # Check data types:\n    assert gdl.ld is not None and gdl.sumstats_table is not None\n\n    ld_score = []\n    chi_sq = []\n    sample_size = []\n\n    for c in gdl.chromosomes:\n        ld_score.append(gdl.ld[c].ld_score)\n        chi_sq.append(gdl.sumstats_table[c].get_chisq_statistic())\n        sample_size.append(gdl.sumstats_table[c].n_per_snp.max())\n\n    ld_score = np.concatenate(ld_score)\n    chi_sq = np.concatenate(chi_sq)\n    sample_size = max(sample_size)\n\n    return (chi_sq.mean() - 1.) * len(ld_score) / (ld_score.mean() * sample_size)\n</code></pre>"},{"location":"api/stats/ld/estimator/","title":"Estimator","text":""},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.BlockLD","title":"<code>BlockLD</code>","text":"<p>               Bases: <code>SampleLD</code></p> <p>A wrapper class to facilitate computing block-based Linkage-Disequilibrium (LD) matrices. Block-based LD matrices are a way to reduce the memory requirements of the LD matrix by computing the pairwise correlation coefficients only between SNPs that are within the same LD block.</p> <p>LD blocks can be inferred by external software tools, such as <code>LDetect</code> of Berisa and Pickrell (2016):</p> <p>Berisa T, Pickrell JK. Approximately independent linkage disequilibrium blocks in human populations. Bioinformatics. 2016 Jan 15;32(2):283-5. doi: 10.1093/bioinformatics/btv546. Epub 2015 Sep 22. PMID: 26395773; PMCID: PMC4731402.</p> <p>The <code>BlockLD</code> estimator requires the LD blocks to be provided as input. The LD blocks are a Bx2 matrix where B is the number of blocks and the columns are the start and end of each block, respectively.</p> <p>See Also</p> <ul> <li>WindowedLD</li> <li>ShrinkageLD</li> </ul> <p>Attributes:</p> Name Type Description <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> <code>ld_boundaries</code> <p>The LD boundaries for each variant in the LD matrix.</p> <code>ld_blocks</code> <p>The LD blocks, a Bx2 matrix where B is the number of blocks and the columns are the start and end of each block, respectively.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>class BlockLD(SampleLD):\n    \"\"\"\n    A wrapper class to facilitate computing block-based Linkage-Disequilibrium (LD) matrices.\n    Block-based LD matrices are a way to reduce the memory requirements of the LD matrix by\n    computing the pairwise correlation coefficients only between SNPs that are within the same LD block.\n\n    LD blocks can be inferred by external software tools, such as `LDetect` of Berisa and Pickrell (2016):\n\n    Berisa T, Pickrell JK. Approximately independent linkage disequilibrium blocks in human populations.\n    Bioinformatics. 2016 Jan 15;32(2):283-5. doi: 10.1093/bioinformatics/btv546.\n    Epub 2015 Sep 22. PMID: 26395773; PMCID: PMC4731402.\n\n    The `BlockLD` estimator requires the LD blocks to be provided as input. The LD blocks are a Bx2 matrix\n    where B is the number of blocks and the columns are the start and end of each block, respectively.\n\n    !!! seealso \"See Also\"\n        * [WindowedLD][magenpy.stats.ld.estimator.WindowedLD]\n        * [ShrinkageLD][magenpy.stats.ld.estimator.ShrinkageLD]\n\n    :ivar genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :ivar ld_boundaries: The LD boundaries for each variant in the LD matrix.\n    :ivar ld_blocks: The LD blocks, a Bx2 matrix where B is the number of blocks and the columns are\n    the start and end of each block, respectively.\n\n    \"\"\"\n\n    estimator_id = 'block'\n\n    def __init__(self,\n                 genotype_matrix,\n                 ld_blocks=None,\n                 ld_blocks_file=None):\n        \"\"\"\n        Initialize the block-based LD estimator with a genotype matrix and LD blocks.\n\n        :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n        :param ld_blocks: The LD blocks, a Bx2 matrix where B is the number of blocks and the\n        columns are the start and end of each block in units of base pair, respectively.\n        :param ld_blocks_file: The path to the LD blocks file\n        \"\"\"\n\n        assert ld_blocks_file is not None or ld_blocks is not None\n\n        super().__init__(genotype_matrix=genotype_matrix)\n\n        if ld_blocks is None:\n            from ...parsers.misc_parsers import parse_ld_block_data\n            self.ld_blocks = parse_ld_block_data(ld_blocks_file)[self.genotype_matrix.chromosome]\n        else:\n            self.ld_blocks = ld_blocks\n\n        from ...utils.model_utils import map_variants_to_genomic_blocks\n\n        # Map variants to their associated genomic blocks:\n        variants_to_blocks = map_variants_to_genomic_blocks(\n            pd.DataFrame({'SNP': self.genotype_matrix.snps,\n                          'bp_pos': self.genotype_matrix.bp_pos}),\n            pd.DataFrame(self.ld_blocks, columns=['block_start', 'block_end']).assign(\n                group=np.arange(len(self.ld_blocks))),\n            variant_pos_col='bp_pos',\n            filter_unmatched=True\n        )\n\n        # Split the genotype matrix by the blocks:\n        split_geno_matrices = self.genotype_matrix.split_by_variants(variants_to_blocks)\n\n        self._block_estimators = {\n            i: SampleLD(geno_matrix) for i, geno_matrix in split_geno_matrices.items()\n        }\n\n    def compute_ld_boundaries(self):\n        \"\"\"\n        Compute the per-SNP Linkage-Disequilibrium (LD) boundaries for the block-based estimator.\n\n        :return: A 2xM matrix of LD boundaries.\n        \"\"\"\n\n        if self.ld_boundaries is None:\n            from .c_utils import find_ld_block_boundaries\n            self.ld_boundaries = find_ld_block_boundaries(self.genotype_matrix.bp_pos, self.ld_blocks)\n\n        return self.ld_boundaries\n\n    def compute(self,\n                output_dir,\n                overwrite=True,\n                delete_original=True,\n                dtype='int16',\n                compressor_name='zstd',\n                compression_level=7,\n                compute_spectral_properties=False) -&gt; LDMatrix:\n        \"\"\"\n\n        Compute the block-based LD matrix and store in Zarr array format.\n\n        :param output_dir: The path where to store the resulting LD matrix.\n        :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n        :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n        :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n        the LD matrix.\n        :param dtype: The data type for the entries of the LD matrix.\n        :param compressor_name: The name of the compressor to use for the LD matrix.\n        :param compression_level: The compression level to use for the LD matrix (1-9).\n\n        :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n        \"\"\"\n\n        ld_mats = []\n\n        # TODO: Parallelize this loop\n\n        temp_dir = tempfile.TemporaryDirectory(dir=self.temp_dir, prefix=self.temp_dir_prefix)\n\n        for i, block_estimator in self._block_estimators.items():\n\n            block_output_dir = osp.join(temp_dir.name,\n                                        f'chr_{self.genotype_matrix.chromosome}_block_{i}/')\n\n            makedir(block_output_dir)\n\n            ld_mats.append(\n                block_estimator.compute(\n                    block_output_dir,\n                    overwrite=overwrite,\n                    delete_original=delete_original,\n                    dtype=dtype,\n                    compressor_name=compressor_name,\n                    compression_level=compression_level,\n                    compute_spectral_properties=compute_spectral_properties\n                )\n            )\n\n        # If the user requested computing the spectral properties, we need to obtain\n        # the minimum eigenvalue from the blocks:\n        if compute_spectral_properties:\n\n            extremal_eigs = pd.DataFrame([ld.get_store_attr('Spectral properties')['Extremal']\n                                          for ld in ld_mats])\n            blocks = np.insert(np.cumsum([ld.stored_n_snps for ld in ld_mats]), 0, 0)\n            block_starts = blocks[:-1]\n            block_ends = blocks[1:]\n\n        # Combine the LD matrices for the individual blocks into a single LD matrix:\n        from .utils import combine_ld_matrices\n\n        output_dir = osp.join(output_dir, f'chr_{self.genotype_matrix.chromosome}/')\n\n        ld_mat = combine_ld_matrices(ld_mats,\n                                     output_dir,\n                                     overwrite=overwrite,\n                                     delete_original=delete_original)\n\n        # Populate the attributes of the LDMatrix object:\n        ld_mat.set_store_attr('LD estimator', 'block')\n        ld_mat.set_store_attr('Estimator properties', {\n            'LD blocks': self.ld_blocks.tolist()\n        })\n\n        ld_mat.set_store_attr('Chromosome', int(self.genotype_matrix.chromosome))\n        ld_mat.set_store_attr('Sample size', int(self.genotype_matrix.sample_size))\n\n        if self.genotype_matrix.genome_build is not None:\n            ld_mat.set_store_attr('Genome build', self.genotype_matrix.genome_build)\n\n        if compute_spectral_properties:\n\n            spectral_prop = {\n                'Extremal': {\n                    'min': extremal_eigs['min'].min(),\n                    'max': extremal_eigs['max'].max()\n                },\n                'Eigenvalues per block': {\n                    'min': list(extremal_eigs['min']),\n                    'max': list(extremal_eigs['max']),\n                    'block_start': list(block_starts),\n                    'block_end': list(block_ends)\n                }\n            }\n\n            ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n        if ld_mat.validate_ld_matrix():\n            temp_dir.cleanup()\n            return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.BlockLD.__init__","title":"<code>__init__(genotype_matrix, ld_blocks=None, ld_blocks_file=None)</code>","text":"<p>Initialize the block-based LD estimator with a genotype matrix and LD blocks.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> required <code>ld_blocks</code> <p>The LD blocks, a Bx2 matrix where B is the number of blocks and the columns are the start and end of each block in units of base pair, respectively.</p> <code>None</code> <code>ld_blocks_file</code> <p>The path to the LD blocks file</p> <code>None</code> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def __init__(self,\n             genotype_matrix,\n             ld_blocks=None,\n             ld_blocks_file=None):\n    \"\"\"\n    Initialize the block-based LD estimator with a genotype matrix and LD blocks.\n\n    :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :param ld_blocks: The LD blocks, a Bx2 matrix where B is the number of blocks and the\n    columns are the start and end of each block in units of base pair, respectively.\n    :param ld_blocks_file: The path to the LD blocks file\n    \"\"\"\n\n    assert ld_blocks_file is not None or ld_blocks is not None\n\n    super().__init__(genotype_matrix=genotype_matrix)\n\n    if ld_blocks is None:\n        from ...parsers.misc_parsers import parse_ld_block_data\n        self.ld_blocks = parse_ld_block_data(ld_blocks_file)[self.genotype_matrix.chromosome]\n    else:\n        self.ld_blocks = ld_blocks\n\n    from ...utils.model_utils import map_variants_to_genomic_blocks\n\n    # Map variants to their associated genomic blocks:\n    variants_to_blocks = map_variants_to_genomic_blocks(\n        pd.DataFrame({'SNP': self.genotype_matrix.snps,\n                      'bp_pos': self.genotype_matrix.bp_pos}),\n        pd.DataFrame(self.ld_blocks, columns=['block_start', 'block_end']).assign(\n            group=np.arange(len(self.ld_blocks))),\n        variant_pos_col='bp_pos',\n        filter_unmatched=True\n    )\n\n    # Split the genotype matrix by the blocks:\n    split_geno_matrices = self.genotype_matrix.split_by_variants(variants_to_blocks)\n\n    self._block_estimators = {\n        i: SampleLD(geno_matrix) for i, geno_matrix in split_geno_matrices.items()\n    }\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.BlockLD.compute","title":"<code>compute(output_dir, overwrite=True, delete_original=True, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False)</code>","text":"<p>Compute the block-based LD matrix and store in Zarr array format.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>The path where to store the resulting LD matrix.</p> required <code>overwrite</code> <p>If True, overwrite any existing LD matrices in <code>temp_dir</code> and <code>output_dir</code>.</p> <code>True</code> <code>delete_original</code> <p>If True, deletes dense or intermediate LD matrices generated along the way.</p> <code>True</code> <code>compute_spectral_properties</code> <p>If True, compute and store information about the eigenvalues of the LD matrix.</p> <code>False</code> <code>dtype</code> <p>The data type for the entries of the LD matrix.</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the LD matrix.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the LD matrix (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <code>LDMatrix</code> <p>An instance of <code>LDMatrix</code> encapsulating the computed LD matrix, its attributes, and metadata.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute(self,\n            output_dir,\n            overwrite=True,\n            delete_original=True,\n            dtype='int16',\n            compressor_name='zstd',\n            compression_level=7,\n            compute_spectral_properties=False) -&gt; LDMatrix:\n    \"\"\"\n\n    Compute the block-based LD matrix and store in Zarr array format.\n\n    :param output_dir: The path where to store the resulting LD matrix.\n    :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n    :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n    :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n    the LD matrix.\n    :param dtype: The data type for the entries of the LD matrix.\n    :param compressor_name: The name of the compressor to use for the LD matrix.\n    :param compression_level: The compression level to use for the LD matrix (1-9).\n\n    :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n    \"\"\"\n\n    ld_mats = []\n\n    # TODO: Parallelize this loop\n\n    temp_dir = tempfile.TemporaryDirectory(dir=self.temp_dir, prefix=self.temp_dir_prefix)\n\n    for i, block_estimator in self._block_estimators.items():\n\n        block_output_dir = osp.join(temp_dir.name,\n                                    f'chr_{self.genotype_matrix.chromosome}_block_{i}/')\n\n        makedir(block_output_dir)\n\n        ld_mats.append(\n            block_estimator.compute(\n                block_output_dir,\n                overwrite=overwrite,\n                delete_original=delete_original,\n                dtype=dtype,\n                compressor_name=compressor_name,\n                compression_level=compression_level,\n                compute_spectral_properties=compute_spectral_properties\n            )\n        )\n\n    # If the user requested computing the spectral properties, we need to obtain\n    # the minimum eigenvalue from the blocks:\n    if compute_spectral_properties:\n\n        extremal_eigs = pd.DataFrame([ld.get_store_attr('Spectral properties')['Extremal']\n                                      for ld in ld_mats])\n        blocks = np.insert(np.cumsum([ld.stored_n_snps for ld in ld_mats]), 0, 0)\n        block_starts = blocks[:-1]\n        block_ends = blocks[1:]\n\n    # Combine the LD matrices for the individual blocks into a single LD matrix:\n    from .utils import combine_ld_matrices\n\n    output_dir = osp.join(output_dir, f'chr_{self.genotype_matrix.chromosome}/')\n\n    ld_mat = combine_ld_matrices(ld_mats,\n                                 output_dir,\n                                 overwrite=overwrite,\n                                 delete_original=delete_original)\n\n    # Populate the attributes of the LDMatrix object:\n    ld_mat.set_store_attr('LD estimator', 'block')\n    ld_mat.set_store_attr('Estimator properties', {\n        'LD blocks': self.ld_blocks.tolist()\n    })\n\n    ld_mat.set_store_attr('Chromosome', int(self.genotype_matrix.chromosome))\n    ld_mat.set_store_attr('Sample size', int(self.genotype_matrix.sample_size))\n\n    if self.genotype_matrix.genome_build is not None:\n        ld_mat.set_store_attr('Genome build', self.genotype_matrix.genome_build)\n\n    if compute_spectral_properties:\n\n        spectral_prop = {\n            'Extremal': {\n                'min': extremal_eigs['min'].min(),\n                'max': extremal_eigs['max'].max()\n            },\n            'Eigenvalues per block': {\n                'min': list(extremal_eigs['min']),\n                'max': list(extremal_eigs['max']),\n                'block_start': list(block_starts),\n                'block_end': list(block_ends)\n            }\n        }\n\n        ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n    if ld_mat.validate_ld_matrix():\n        temp_dir.cleanup()\n        return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.BlockLD.compute_ld_boundaries","title":"<code>compute_ld_boundaries()</code>","text":"<p>Compute the per-SNP Linkage-Disequilibrium (LD) boundaries for the block-based estimator.</p> <p>Returns:</p> Type Description <p>A 2xM matrix of LD boundaries.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute_ld_boundaries(self):\n    \"\"\"\n    Compute the per-SNP Linkage-Disequilibrium (LD) boundaries for the block-based estimator.\n\n    :return: A 2xM matrix of LD boundaries.\n    \"\"\"\n\n    if self.ld_boundaries is None:\n        from .c_utils import find_ld_block_boundaries\n        self.ld_boundaries = find_ld_block_boundaries(self.genotype_matrix.bp_pos, self.ld_blocks)\n\n    return self.ld_boundaries\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.SampleLD","title":"<code>SampleLD</code>","text":"<p>               Bases: <code>object</code></p> <p>A basic wrapper class to facilitate computing Linkage-Disequilibrium (LD) matrices.</p> <p>Linkage-Disequilibrium (LD) is a measure of the SNP-by-SNP pairwise correlation between genetic variants in a population. LD tends to decay with genomic distance, and the rate of decay is influenced by many factors. Therefore, LD matrices are often diagonally-dominant.</p> <p>This class <code>SampleLD</code> provides a basic interface to compute sample correlation coefficient between  all variants defined in a genotype matrix. The resulting LD matrix is a square and dense matrix.</p> <p>For sparse LD matrices, consider using the <code>WindowedLD</code>, <code>ShrinkageLD</code> or <code>BlockLD</code> estimators instead.</p> <p>!!! seealso \"See Also\"     * WindowedLD     * ShrinkageLD     * BlockLD</p> <p>:ivar genotype_matrix: The genotype matrix, an instance of <code>GenotypeMatrix</code> or its children.  :ivar ld_boundaries: The LD boundaries for each variant in the LD matrix.  :ivar temp_dir: A temporary directory to store intermediate files and results.  :ivar temp_dir_prefix: A prefix for the temporary directory.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>class SampleLD(object):\n    \"\"\"\n    A basic wrapper class to facilitate computing Linkage-Disequilibrium (LD) matrices.\n\n    Linkage-Disequilibrium (LD) is a measure of the SNP-by-SNP pairwise correlation between\n    genetic variants in a population. LD tends to decay with genomic distance, and the rate\n    of decay is influenced by many factors. Therefore, LD matrices are often diagonally-dominant.\n\n    This class `SampleLD` provides a basic interface to compute sample correlation coefficient between\n     all variants defined in a genotype matrix. The resulting LD matrix is a square and dense matrix.\n\n     For sparse LD matrices, consider using the `WindowedLD`, `ShrinkageLD` or `BlockLD` estimators instead.\n\n     !!! seealso \"See Also\"\n        * [WindowedLD][magenpy.stats.ld.estimator.WindowedLD]\n        * [ShrinkageLD][magenpy.stats.ld.estimator.ShrinkageLD]\n        * [BlockLD][magenpy.stats.ld.estimator.BlockLD]\n\n     :ivar genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix` or its children.\n     :ivar ld_boundaries: The LD boundaries for each variant in the LD matrix.\n     :ivar temp_dir: A temporary directory to store intermediate files and results.\n     :ivar temp_dir_prefix: A prefix for the temporary directory.\n\n    \"\"\"\n\n    estimator_id = 'sample'\n\n    def __init__(self, genotype_matrix):\n        \"\"\"\n        Initialize the LD estimator with a genotype matrix.\n        :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n        \"\"\"\n\n        self.genotype_matrix = genotype_matrix\n        self.ld_boundaries = None\n\n        self.temp_dir = self.genotype_matrix.temp_dir\n        self.temp_dir_prefix = self.genotype_matrix.temp_dir_prefix + 'ld_'\n\n        # Ensure that the genotype matrix has data for a single chromosome only:\n        if self.genotype_matrix.chromosome is None:\n            raise Exception(\"`magenpy` does not support computing inter-chromosomal LD matrices! \"\n                            \"You may need to split the genotype matrix by chromosome. \"\n                            \"See GenotypeMatrix.split_by_chromosome().\")\n\n    def compute_ld_boundaries(self):\n        \"\"\"\n         Compute the Linkage-Disequilibrium (LD) boundaries. LD boundaries define the window\n         for which we compute the correlation coefficient between the focal SNP and all other SNPs in\n         the genome. Typically, this window is local, since the LD decays exponentially with\n         genomic distance.\n\n         The LD boundaries are a 2xM matrix, where M is the number of SNPs on the chromosome.\n         The first row contains the start position for the window and the second row contains\n         the end position.\n\n         For the sample LD matrix, we simply take the entire square matrix as our window,\n         so the start position is 0 and end position is M for all SNPs.\n\n         :return: A 2xM matrix of LD boundaries.\n        \"\"\"\n\n        if self.ld_boundaries is None:\n\n            m = self.genotype_matrix.n_snps\n            self.ld_boundaries = np.array((np.zeros(m),\n                                           np.ones(m)*m)).astype(np.int64)\n\n        return self.ld_boundaries\n\n    def compute_plink_window_thresholds(self, ld_boundaries=None):\n        \"\"\"\n        Computes the LD window thresholds to pass to plink1.9 for computing LD matrices.\n        Unfortunately, plink1.9 sets some default values for the window size and it\n        is important to set all the thresholds to obtain the desired shape for the\n        LD matrix.\n\n        :param ld_boundaries: The LD boundaries for which to compute the thresholds. If not passed,\n        we compute the LD boundaries using the `compute_ld_boundaries` method.\n\n        :return: A dictionary containing the window size thresholds for plink1.9.\n\n        \"\"\"\n\n        if ld_boundaries is None:\n            ld_boundaries = self.compute_ld_boundaries()\n\n        threshold_dict = {}\n\n        # (1) Determine maximum window size (Maximum number of neighbors on each side):\n        try:\n            threshold_dict['window_size'] = getattr(self, \"window_size\")\n            assert threshold_dict['window_size'] is not None\n        except (AttributeError, AssertionError):\n            threshold_dict['window_size'] = np.abs(ld_boundaries -\n                                                   np.arange(ld_boundaries.shape[1])).max()\n\n        # (2) Determine the maximum window size in kilobases + Centi Morgan (if available):\n\n        positional_bounds = np.array([ld_boundaries[0, :], ld_boundaries[1, :] - 1])\n\n        try:\n            threshold_dict['kb_window_size'] = getattr(self, \"kb_window_size\")\n            assert threshold_dict['kb_window_size'] is not None\n        except (AttributeError, AssertionError):\n            kb_pos = .001 * self.genotype_matrix.bp_pos\n            kb_bounds = kb_pos[positional_bounds]\n            threshold_dict['kb_window_size'] = np.abs(kb_bounds - kb_pos).max()\n\n        # (3) centi Morgan:\n        try:\n            threshold_dict['cm_window_size'] = getattr(self, \"cm_window_size\")\n            assert threshold_dict['cm_window_size'] is not None\n        except (AttributeError, AssertionError):\n            try:\n                # Checks if cm_pos is available in the genotype matrix:\n                cm_pos = self.genotype_matrix.cm_pos\n                cm_bounds = self.genotype_matrix.cm_pos[positional_bounds]\n                threshold_dict['cm_window_size'] = np.abs(cm_bounds - cm_pos).max()\n            except KeyError:\n                del threshold_dict['cm_window_size']\n\n        if self.estimator_id == 'sample':\n            # If we're using the sample estimator, then expand the plink thresholds slightly\n            # to avoid the program dropping LD between variants at the extreme ends:\n            for key, val in threshold_dict.items():\n                threshold_dict[key] = val*1.05\n                if isinstance(val, int):\n                    threshold_dict[key] = int(threshold_dict[key])\n\n        return threshold_dict\n\n    def compute(self,\n                output_dir,\n                overwrite=True,\n                delete_original=True,\n                dtype='int16',\n                compressor_name='zstd',\n                compression_level=7,\n                compute_spectral_properties=False) -&gt; LDMatrix:\n        \"\"\"\n        A utility method to compute the LD matrix and store in Zarr array format.\n        The computes the LD matrix and stores it in Zarr array format, set its attributes,\n        and performs simple validation at the end.\n\n        :param output_dir: The path where to store the resulting LD matrix.\n        :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n        :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n        :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n        :param compressor_name: The name of the compressor to use for the LD matrix.\n        :param compression_level: The compression level to use for the LD matrix (1-9).\n        :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n        the LD matrix.\n\n        :return: An instance of `LDMatrix` containing the computed LD matrix.\n\n        \"\"\"\n\n        from .utils import compute_ld_xarray, compute_ld_plink1p9\n        from ...GenotypeMatrix import xarrayGenotypeMatrix, plinkBEDGenotypeMatrix\n\n        assert str(dtype) in ('float32', 'float64', 'int8', 'int16')\n\n        # Create the temporary directory using tempfile:\n        temp_dir = tempfile.TemporaryDirectory(dir=self.temp_dir, prefix=self.temp_dir_prefix)\n\n        ld_boundaries = self.compute_ld_boundaries()\n\n        if isinstance(self.genotype_matrix, xarrayGenotypeMatrix):\n            ld_mat = compute_ld_xarray(self.genotype_matrix,\n                                       ld_boundaries,\n                                       output_dir,\n                                       temp_dir=temp_dir.name,\n                                       overwrite=overwrite,\n                                       delete_original=delete_original,\n                                       dtype=dtype,\n                                       compressor_name=compressor_name,\n                                       compression_level=compression_level)\n        elif isinstance(self.genotype_matrix, plinkBEDGenotypeMatrix):\n\n            # Compute the window size thresholds to pass to plink 1.9:\n            window_size_thersh = self.compute_plink_window_thresholds(ld_boundaries)\n\n            ld_mat = compute_ld_plink1p9(self.genotype_matrix,\n                                         ld_boundaries,\n                                         output_dir,\n                                         window_size_thersh,\n                                         trim_boundaries=self.estimator_id not in ('sample', 'windowed'),\n                                         temp_dir=temp_dir.name,\n                                         overwrite=overwrite,\n                                         dtype=dtype,\n                                         compressor_name=compressor_name,\n                                         compression_level=compression_level)\n        else:\n            raise NotImplementedError\n\n        # Add attributes to the LDMatrix object:\n        ld_mat.set_store_attr('Chromosome', int(self.genotype_matrix.chromosome))\n        ld_mat.set_store_attr('Sample size', int(self.genotype_matrix.sample_size))\n        ld_mat.set_store_attr('LD estimator', 'sample')\n\n        if self.genotype_matrix.genome_build is not None:\n            ld_mat.set_store_attr('Genome build', self.genotype_matrix.genome_build)\n\n        ld_mat.set_metadata('snps', self.genotype_matrix.snps, overwrite=overwrite)\n        ld_mat.set_metadata('bp', self.genotype_matrix.bp_pos, overwrite=overwrite)\n        ld_mat.set_metadata('maf', self.genotype_matrix.maf, overwrite=overwrite)\n        ld_mat.set_metadata('a1', self.genotype_matrix.a1, overwrite=overwrite)\n        ld_mat.set_metadata('a2', self.genotype_matrix.a2, overwrite=overwrite)\n\n        try:\n            ld_mat.set_metadata('cm', self.genotype_matrix.cm_pos, overwrite=overwrite)\n        except KeyError:\n            pass\n\n        ld_mat.set_metadata('ldscore', ld_mat.compute_ld_scores(), overwrite=overwrite)\n\n        if compute_spectral_properties:\n\n            extreme_eigs = ld_mat.estimate_extremal_eigenvalues()\n\n            ld_mat.set_store_attr('Spectral properties', {\n                'Extremal': extreme_eigs\n            })\n\n        if ld_mat.validate_ld_matrix():\n            temp_dir.cleanup()\n            return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.SampleLD.__init__","title":"<code>__init__(genotype_matrix)</code>","text":"<p>Initialize the LD estimator with a genotype matrix.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> required Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def __init__(self, genotype_matrix):\n    \"\"\"\n    Initialize the LD estimator with a genotype matrix.\n    :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    \"\"\"\n\n    self.genotype_matrix = genotype_matrix\n    self.ld_boundaries = None\n\n    self.temp_dir = self.genotype_matrix.temp_dir\n    self.temp_dir_prefix = self.genotype_matrix.temp_dir_prefix + 'ld_'\n\n    # Ensure that the genotype matrix has data for a single chromosome only:\n    if self.genotype_matrix.chromosome is None:\n        raise Exception(\"`magenpy` does not support computing inter-chromosomal LD matrices! \"\n                        \"You may need to split the genotype matrix by chromosome. \"\n                        \"See GenotypeMatrix.split_by_chromosome().\")\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.SampleLD.compute","title":"<code>compute(output_dir, overwrite=True, delete_original=True, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False)</code>","text":"<p>A utility method to compute the LD matrix and store in Zarr array format. The computes the LD matrix and stores it in Zarr array format, set its attributes, and performs simple validation at the end.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>The path where to store the resulting LD matrix.</p> required <code>overwrite</code> <p>If True, overwrite any existing LD matrices in <code>temp_dir</code> and <code>output_dir</code>.</p> <code>True</code> <code>delete_original</code> <p>If True, deletes dense or intermediate LD matrices generated along the way.</p> <code>True</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the LD matrix.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the LD matrix (1-9).</p> <code>7</code> <code>compute_spectral_properties</code> <p>If True, compute and store information about the eigenvalues of the LD matrix.</p> <code>False</code> <p>Returns:</p> Type Description <code>LDMatrix</code> <p>An instance of <code>LDMatrix</code> containing the computed LD matrix.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute(self,\n            output_dir,\n            overwrite=True,\n            delete_original=True,\n            dtype='int16',\n            compressor_name='zstd',\n            compression_level=7,\n            compute_spectral_properties=False) -&gt; LDMatrix:\n    \"\"\"\n    A utility method to compute the LD matrix and store in Zarr array format.\n    The computes the LD matrix and stores it in Zarr array format, set its attributes,\n    and performs simple validation at the end.\n\n    :param output_dir: The path where to store the resulting LD matrix.\n    :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n    :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n    and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor to use for the LD matrix.\n    :param compression_level: The compression level to use for the LD matrix (1-9).\n    :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n    the LD matrix.\n\n    :return: An instance of `LDMatrix` containing the computed LD matrix.\n\n    \"\"\"\n\n    from .utils import compute_ld_xarray, compute_ld_plink1p9\n    from ...GenotypeMatrix import xarrayGenotypeMatrix, plinkBEDGenotypeMatrix\n\n    assert str(dtype) in ('float32', 'float64', 'int8', 'int16')\n\n    # Create the temporary directory using tempfile:\n    temp_dir = tempfile.TemporaryDirectory(dir=self.temp_dir, prefix=self.temp_dir_prefix)\n\n    ld_boundaries = self.compute_ld_boundaries()\n\n    if isinstance(self.genotype_matrix, xarrayGenotypeMatrix):\n        ld_mat = compute_ld_xarray(self.genotype_matrix,\n                                   ld_boundaries,\n                                   output_dir,\n                                   temp_dir=temp_dir.name,\n                                   overwrite=overwrite,\n                                   delete_original=delete_original,\n                                   dtype=dtype,\n                                   compressor_name=compressor_name,\n                                   compression_level=compression_level)\n    elif isinstance(self.genotype_matrix, plinkBEDGenotypeMatrix):\n\n        # Compute the window size thresholds to pass to plink 1.9:\n        window_size_thersh = self.compute_plink_window_thresholds(ld_boundaries)\n\n        ld_mat = compute_ld_plink1p9(self.genotype_matrix,\n                                     ld_boundaries,\n                                     output_dir,\n                                     window_size_thersh,\n                                     trim_boundaries=self.estimator_id not in ('sample', 'windowed'),\n                                     temp_dir=temp_dir.name,\n                                     overwrite=overwrite,\n                                     dtype=dtype,\n                                     compressor_name=compressor_name,\n                                     compression_level=compression_level)\n    else:\n        raise NotImplementedError\n\n    # Add attributes to the LDMatrix object:\n    ld_mat.set_store_attr('Chromosome', int(self.genotype_matrix.chromosome))\n    ld_mat.set_store_attr('Sample size', int(self.genotype_matrix.sample_size))\n    ld_mat.set_store_attr('LD estimator', 'sample')\n\n    if self.genotype_matrix.genome_build is not None:\n        ld_mat.set_store_attr('Genome build', self.genotype_matrix.genome_build)\n\n    ld_mat.set_metadata('snps', self.genotype_matrix.snps, overwrite=overwrite)\n    ld_mat.set_metadata('bp', self.genotype_matrix.bp_pos, overwrite=overwrite)\n    ld_mat.set_metadata('maf', self.genotype_matrix.maf, overwrite=overwrite)\n    ld_mat.set_metadata('a1', self.genotype_matrix.a1, overwrite=overwrite)\n    ld_mat.set_metadata('a2', self.genotype_matrix.a2, overwrite=overwrite)\n\n    try:\n        ld_mat.set_metadata('cm', self.genotype_matrix.cm_pos, overwrite=overwrite)\n    except KeyError:\n        pass\n\n    ld_mat.set_metadata('ldscore', ld_mat.compute_ld_scores(), overwrite=overwrite)\n\n    if compute_spectral_properties:\n\n        extreme_eigs = ld_mat.estimate_extremal_eigenvalues()\n\n        ld_mat.set_store_attr('Spectral properties', {\n            'Extremal': extreme_eigs\n        })\n\n    if ld_mat.validate_ld_matrix():\n        temp_dir.cleanup()\n        return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.SampleLD.compute_ld_boundaries","title":"<code>compute_ld_boundaries()</code>","text":"<p>Compute the Linkage-Disequilibrium (LD) boundaries. LD boundaries define the window for which we compute the correlation coefficient between the focal SNP and all other SNPs in the genome. Typically, this window is local, since the LD decays exponentially with genomic distance.</p> <p>The LD boundaries are a 2xM matrix, where M is the number of SNPs on the chromosome. The first row contains the start position for the window and the second row contains the end position.</p> <p>For the sample LD matrix, we simply take the entire square matrix as our window, so the start position is 0 and end position is M for all SNPs.</p> <p>Returns:</p> Type Description <p>A 2xM matrix of LD boundaries.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute_ld_boundaries(self):\n    \"\"\"\n     Compute the Linkage-Disequilibrium (LD) boundaries. LD boundaries define the window\n     for which we compute the correlation coefficient between the focal SNP and all other SNPs in\n     the genome. Typically, this window is local, since the LD decays exponentially with\n     genomic distance.\n\n     The LD boundaries are a 2xM matrix, where M is the number of SNPs on the chromosome.\n     The first row contains the start position for the window and the second row contains\n     the end position.\n\n     For the sample LD matrix, we simply take the entire square matrix as our window,\n     so the start position is 0 and end position is M for all SNPs.\n\n     :return: A 2xM matrix of LD boundaries.\n    \"\"\"\n\n    if self.ld_boundaries is None:\n\n        m = self.genotype_matrix.n_snps\n        self.ld_boundaries = np.array((np.zeros(m),\n                                       np.ones(m)*m)).astype(np.int64)\n\n    return self.ld_boundaries\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.SampleLD.compute_plink_window_thresholds","title":"<code>compute_plink_window_thresholds(ld_boundaries=None)</code>","text":"<p>Computes the LD window thresholds to pass to plink1.9 for computing LD matrices. Unfortunately, plink1.9 sets some default values for the window size and it is important to set all the thresholds to obtain the desired shape for the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ld_boundaries</code> <p>The LD boundaries for which to compute the thresholds. If not passed, we compute the LD boundaries using the <code>compute_ld_boundaries</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary containing the window size thresholds for plink1.9.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute_plink_window_thresholds(self, ld_boundaries=None):\n    \"\"\"\n    Computes the LD window thresholds to pass to plink1.9 for computing LD matrices.\n    Unfortunately, plink1.9 sets some default values for the window size and it\n    is important to set all the thresholds to obtain the desired shape for the\n    LD matrix.\n\n    :param ld_boundaries: The LD boundaries for which to compute the thresholds. If not passed,\n    we compute the LD boundaries using the `compute_ld_boundaries` method.\n\n    :return: A dictionary containing the window size thresholds for plink1.9.\n\n    \"\"\"\n\n    if ld_boundaries is None:\n        ld_boundaries = self.compute_ld_boundaries()\n\n    threshold_dict = {}\n\n    # (1) Determine maximum window size (Maximum number of neighbors on each side):\n    try:\n        threshold_dict['window_size'] = getattr(self, \"window_size\")\n        assert threshold_dict['window_size'] is not None\n    except (AttributeError, AssertionError):\n        threshold_dict['window_size'] = np.abs(ld_boundaries -\n                                               np.arange(ld_boundaries.shape[1])).max()\n\n    # (2) Determine the maximum window size in kilobases + Centi Morgan (if available):\n\n    positional_bounds = np.array([ld_boundaries[0, :], ld_boundaries[1, :] - 1])\n\n    try:\n        threshold_dict['kb_window_size'] = getattr(self, \"kb_window_size\")\n        assert threshold_dict['kb_window_size'] is not None\n    except (AttributeError, AssertionError):\n        kb_pos = .001 * self.genotype_matrix.bp_pos\n        kb_bounds = kb_pos[positional_bounds]\n        threshold_dict['kb_window_size'] = np.abs(kb_bounds - kb_pos).max()\n\n    # (3) centi Morgan:\n    try:\n        threshold_dict['cm_window_size'] = getattr(self, \"cm_window_size\")\n        assert threshold_dict['cm_window_size'] is not None\n    except (AttributeError, AssertionError):\n        try:\n            # Checks if cm_pos is available in the genotype matrix:\n            cm_pos = self.genotype_matrix.cm_pos\n            cm_bounds = self.genotype_matrix.cm_pos[positional_bounds]\n            threshold_dict['cm_window_size'] = np.abs(cm_bounds - cm_pos).max()\n        except KeyError:\n            del threshold_dict['cm_window_size']\n\n    if self.estimator_id == 'sample':\n        # If we're using the sample estimator, then expand the plink thresholds slightly\n        # to avoid the program dropping LD between variants at the extreme ends:\n        for key, val in threshold_dict.items():\n            threshold_dict[key] = val*1.05\n            if isinstance(val, int):\n                threshold_dict[key] = int(threshold_dict[key])\n\n    return threshold_dict\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.ShrinkageLD","title":"<code>ShrinkageLD</code>","text":"<p>               Bases: <code>SampleLD</code></p> <p>A wrapper class to facilitate computing shrinkage-based Linkage-Disequilibrium (LD) matrices. Shrinkage LD matrices are a way to reduce noise in the LD matrix by shrinking the off-diagonal pairwise correlation coefficients towards zero. This is useful for reducing the noise in the LD matrix and improving the quality of downstream analyses.</p> <p>The shrinkage estimator implemented uses the shrinking procedure derived in:</p> <p>Wen X, Stephens M. USING LINEAR PREDICTORS TO IMPUTE ALLELE FREQUENCIES FROM SUMMARY OR POOLED GENOTYPE DATA. Ann Appl Stat. 2010 Sep;4(3):1158-1182. doi: 10.1214/10-aoas338. PMID: 21479081; PMCID: PMC3072818.</p> <p>Computing the shrinkage intensity requires specifying the effective population size (Ne) and the sample size used to infer the genetic map. In addition, it requires specifying a threshold below which the LD is set to zero.</p> <p>Note</p> <p>The threshold may be adjusted depending on the requested storage data type.</p> <p>See Also</p> <ul> <li>WindowedLD</li> <li>BlockLD</li> </ul> <p>Attributes:</p> Name Type Description <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> <code>ld_boundaries</code> <p>The LD boundaries for each variant in the LD matrix.</p> <code>genetic_map_ne</code> <p>The effective population size (Ne) from which the genetic map is derived.</p> <code>genetic_map_sample_size</code> <p>The sample size used to infer the genetic map.</p> <code>threshold</code> <p>The shrinkage cutoff below which the LD is set to zero.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>class ShrinkageLD(SampleLD):\n    \"\"\"\n    A wrapper class to facilitate computing shrinkage-based Linkage-Disequilibrium (LD) matrices.\n    Shrinkage LD matrices are a way to reduce noise in the LD matrix by shrinking the off-diagonal pairwise\n    correlation coefficients towards zero. This is useful for reducing the noise in the LD matrix and\n    improving the quality of downstream analyses.\n\n    The shrinkage estimator implemented uses the shrinking procedure derived in:\n\n    Wen X, Stephens M. USING LINEAR PREDICTORS TO IMPUTE ALLELE FREQUENCIES FROM SUMMARY OR POOLED GENOTYPE DATA.\n    Ann Appl Stat. 2010 Sep;4(3):1158-1182. doi: 10.1214/10-aoas338. PMID: 21479081; PMCID: PMC3072818.\n\n    Computing the shrinkage intensity requires specifying the effective population size (Ne) and the sample size\n    used to infer the genetic map. In addition, it requires specifying a threshold below which the LD is set to zero.\n\n    !!! note\n        The threshold may be adjusted depending on the requested storage data type.\n\n    !!! seealso \"See Also\"\n        * [WindowedLD][magenpy.stats.ld.estimator.WindowedLD]\n        * [BlockLD][magenpy.stats.ld.estimator.BlockLD]\n\n    :ivar genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :ivar ld_boundaries: The LD boundaries for each variant in the LD matrix.\n    :ivar genetic_map_ne: The effective population size (Ne) from which the genetic map is derived.\n    :ivar genetic_map_sample_size: The sample size used to infer the genetic map.\n    :ivar threshold: The shrinkage cutoff below which the LD is set to zero.\n\n    \"\"\"\n\n    estimator_id = 'shrinkage'\n\n    def __init__(self,\n                 genotype_matrix,\n                 genetic_map_ne,\n                 genetic_map_sample_size,\n                 threshold=1e-3):\n        \"\"\"\n\n        Initialize the shrinkage LD estimator with a genotype matrix and shrinkage parameters.\n\n        :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n        :param genetic_map_ne: The effective population size (Ne) from which the genetic map is derived.\n        :param genetic_map_sample_size: The sample size used to infer the genetic map.\n        :param threshold: The shrinkage cutoff below which the LD is set to zero.\n        \"\"\"\n\n        super().__init__(genotype_matrix=genotype_matrix)\n\n        self.genetic_map_ne = genetic_map_ne\n        self.genetic_map_sample_size = genetic_map_sample_size\n        self.threshold = threshold\n\n    def compute_ld_boundaries(self):\n        \"\"\"\n        Compute the shrinkage-based Linkage-Disequilibrium (LD) boundaries.\n\n        :return: A 2xM matrix of LD boundaries.\n        \"\"\"\n\n        if self.ld_boundaries is None:\n            from .c_utils import find_shrinkage_ld_boundaries\n            self.ld_boundaries = find_shrinkage_ld_boundaries(self.genotype_matrix.cm_pos,\n                                                              self.genetic_map_ne,\n                                                              self.genetic_map_sample_size,\n                                                              self.threshold)\n\n        return self.ld_boundaries\n\n    def compute(self,\n                output_dir,\n                overwrite=True,\n                delete_original=True,\n                dtype='int16',\n                compressor_name='zstd',\n                compression_level=7,\n                compute_spectral_properties=False,\n                chunk_size=1000) -&gt; LDMatrix:\n        \"\"\"\n\n        !!! note\n            The threshold is adjusted depending on the requested storage data type.\n\n        !!! note\n            LD Scores are computed before applying shrinkage.\n\n        :param output_dir: The path where to store the resulting LD matrix.\n        :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n        :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n        :param dtype: The data type for the entries of the LD matrix.\n        :param compressor_name: The name of the compressor to use for the LD matrix.\n        :param compression_level: The compression level to use for the LD matrix (1-9).\n        :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n        the LD matrix.\n        :param chunk_size: An optional parameter that sets the maximum number of rows processed simultaneously.\n        The smaller the `chunk_size`, the less memory requirements needed for the shrinkage step.\n\n        :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n\n        \"\"\"\n\n        # Adjust the threshold depending on the requested storage data type:\n        if np.issubdtype(dtype, np.integer):\n            threshold = max(self.threshold, 1./np.iinfo(dtype).max)\n        else:\n            threshold = self.threshold\n\n        ld_mat = super().compute(output_dir,\n                                 overwrite=overwrite,\n                                 delete_original=delete_original,\n                                 compute_spectral_properties=False,  # Compute after shrinkage if requested\n                                 dtype=dtype,\n                                 compressor_name=compressor_name,\n                                 compression_level=compression_level)\n\n        from .utils import shrink_ld_matrix\n\n        ld_mat = shrink_ld_matrix(ld_mat,\n                                  self.genotype_matrix.cm_pos,\n                                  self.genotype_matrix.maf_var,\n                                  self.genetic_map_ne,\n                                  self.genetic_map_sample_size,\n                                  threshold,\n                                  chunk_size=chunk_size)\n\n        ld_mat.set_store_attr('LD estimator', 'shrinkage')\n\n        ld_mat.set_store_attr('Estimator properties', {\n                    'Genetic map Ne': self.genetic_map_ne,\n                    'Genetic map sample size': self.genetic_map_sample_size,\n                    'Threshold': threshold\n                })\n\n        if compute_spectral_properties:\n\n            spectral_prop = {\n                'Extremal': ld_mat.estimate_extremal_eigenvalues()\n            }\n\n            cm_ld_bounds_start = self.genotype_matrix.cm_pos[self.ld_boundaries[0, :]]\n            cm_ld_bounds_end = self.genotype_matrix.cm_pos[self.ld_boundaries[1, :] - 1]\n\n            median_dist_cm = np.median(cm_ld_bounds_end - cm_ld_bounds_start)\n\n            eigs, block_bounds = ld_mat.estimate_extremal_eigenvalues(block_size_cm=median_dist_cm,\n                                                                      return_block_boundaries=True)\n            spectral_prop['Eigenvalues per block'] = {**eigs, **block_bounds}\n\n            ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n        return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.ShrinkageLD.__init__","title":"<code>__init__(genotype_matrix, genetic_map_ne, genetic_map_sample_size, threshold=0.001)</code>","text":"<p>Initialize the shrinkage LD estimator with a genotype matrix and shrinkage parameters.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> required <code>genetic_map_ne</code> <p>The effective population size (Ne) from which the genetic map is derived.</p> required <code>genetic_map_sample_size</code> <p>The sample size used to infer the genetic map.</p> required <code>threshold</code> <p>The shrinkage cutoff below which the LD is set to zero.</p> <code>0.001</code> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def __init__(self,\n             genotype_matrix,\n             genetic_map_ne,\n             genetic_map_sample_size,\n             threshold=1e-3):\n    \"\"\"\n\n    Initialize the shrinkage LD estimator with a genotype matrix and shrinkage parameters.\n\n    :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :param genetic_map_ne: The effective population size (Ne) from which the genetic map is derived.\n    :param genetic_map_sample_size: The sample size used to infer the genetic map.\n    :param threshold: The shrinkage cutoff below which the LD is set to zero.\n    \"\"\"\n\n    super().__init__(genotype_matrix=genotype_matrix)\n\n    self.genetic_map_ne = genetic_map_ne\n    self.genetic_map_sample_size = genetic_map_sample_size\n    self.threshold = threshold\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.ShrinkageLD.compute","title":"<code>compute(output_dir, overwrite=True, delete_original=True, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False, chunk_size=1000)</code>","text":"<p>Note</p> <p>The threshold is adjusted depending on the requested storage data type.</p> <p>Note</p> <p>LD Scores are computed before applying shrinkage.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>The path where to store the resulting LD matrix.</p> required <code>overwrite</code> <p>If True, overwrite any existing LD matrices in <code>temp_dir</code> and <code>output_dir</code>.</p> <code>True</code> <code>delete_original</code> <p>If True, deletes dense or intermediate LD matrices generated along the way.</p> <code>True</code> <code>dtype</code> <p>The data type for the entries of the LD matrix.</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the LD matrix.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the LD matrix (1-9).</p> <code>7</code> <code>compute_spectral_properties</code> <p>If True, compute and store information about the eigenvalues of the LD matrix.</p> <code>False</code> <code>chunk_size</code> <p>An optional parameter that sets the maximum number of rows processed simultaneously. The smaller the <code>chunk_size</code>, the less memory requirements needed for the shrinkage step.</p> <code>1000</code> <p>Returns:</p> Type Description <code>LDMatrix</code> <p>An instance of <code>LDMatrix</code> encapsulating the computed LD matrix, its attributes, and metadata.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute(self,\n            output_dir,\n            overwrite=True,\n            delete_original=True,\n            dtype='int16',\n            compressor_name='zstd',\n            compression_level=7,\n            compute_spectral_properties=False,\n            chunk_size=1000) -&gt; LDMatrix:\n    \"\"\"\n\n    !!! note\n        The threshold is adjusted depending on the requested storage data type.\n\n    !!! note\n        LD Scores are computed before applying shrinkage.\n\n    :param output_dir: The path where to store the resulting LD matrix.\n    :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n    :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n    :param dtype: The data type for the entries of the LD matrix.\n    :param compressor_name: The name of the compressor to use for the LD matrix.\n    :param compression_level: The compression level to use for the LD matrix (1-9).\n    :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n    the LD matrix.\n    :param chunk_size: An optional parameter that sets the maximum number of rows processed simultaneously.\n    The smaller the `chunk_size`, the less memory requirements needed for the shrinkage step.\n\n    :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n\n    \"\"\"\n\n    # Adjust the threshold depending on the requested storage data type:\n    if np.issubdtype(dtype, np.integer):\n        threshold = max(self.threshold, 1./np.iinfo(dtype).max)\n    else:\n        threshold = self.threshold\n\n    ld_mat = super().compute(output_dir,\n                             overwrite=overwrite,\n                             delete_original=delete_original,\n                             compute_spectral_properties=False,  # Compute after shrinkage if requested\n                             dtype=dtype,\n                             compressor_name=compressor_name,\n                             compression_level=compression_level)\n\n    from .utils import shrink_ld_matrix\n\n    ld_mat = shrink_ld_matrix(ld_mat,\n                              self.genotype_matrix.cm_pos,\n                              self.genotype_matrix.maf_var,\n                              self.genetic_map_ne,\n                              self.genetic_map_sample_size,\n                              threshold,\n                              chunk_size=chunk_size)\n\n    ld_mat.set_store_attr('LD estimator', 'shrinkage')\n\n    ld_mat.set_store_attr('Estimator properties', {\n                'Genetic map Ne': self.genetic_map_ne,\n                'Genetic map sample size': self.genetic_map_sample_size,\n                'Threshold': threshold\n            })\n\n    if compute_spectral_properties:\n\n        spectral_prop = {\n            'Extremal': ld_mat.estimate_extremal_eigenvalues()\n        }\n\n        cm_ld_bounds_start = self.genotype_matrix.cm_pos[self.ld_boundaries[0, :]]\n        cm_ld_bounds_end = self.genotype_matrix.cm_pos[self.ld_boundaries[1, :] - 1]\n\n        median_dist_cm = np.median(cm_ld_bounds_end - cm_ld_bounds_start)\n\n        eigs, block_bounds = ld_mat.estimate_extremal_eigenvalues(block_size_cm=median_dist_cm,\n                                                                  return_block_boundaries=True)\n        spectral_prop['Eigenvalues per block'] = {**eigs, **block_bounds}\n\n        ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n    return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.ShrinkageLD.compute_ld_boundaries","title":"<code>compute_ld_boundaries()</code>","text":"<p>Compute the shrinkage-based Linkage-Disequilibrium (LD) boundaries.</p> <p>Returns:</p> Type Description <p>A 2xM matrix of LD boundaries.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute_ld_boundaries(self):\n    \"\"\"\n    Compute the shrinkage-based Linkage-Disequilibrium (LD) boundaries.\n\n    :return: A 2xM matrix of LD boundaries.\n    \"\"\"\n\n    if self.ld_boundaries is None:\n        from .c_utils import find_shrinkage_ld_boundaries\n        self.ld_boundaries = find_shrinkage_ld_boundaries(self.genotype_matrix.cm_pos,\n                                                          self.genetic_map_ne,\n                                                          self.genetic_map_sample_size,\n                                                          self.threshold)\n\n    return self.ld_boundaries\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.WindowedLD","title":"<code>WindowedLD</code>","text":"<p>               Bases: <code>SampleLD</code></p> <p>A wrapper class to facilitate computing windowed Linkage-Disequilibrium (LD) matrices. Windowed LD matrices only record pairwise correlations between variants that are within a certain distance of each other along the chromosome. This is useful for reducing the memory requirements and noise in the LD matrix.</p> <p>The <code>WindowedLD</code> estimator supports a variety of ways for defining the window size:</p> <ul> <li><code>window_size</code>: The number of neighboring SNPs to consider on each side when computing LD.</li> <li><code>kb_window_size</code>: The maximum distance in kilobases to consider when computing LD.</li> <li><code>cm_window_size</code>: The maximum distance in centi Morgan to consider when computing LD.</li> </ul> <p>The LD boundaries computed here are the intersection of the windows defined by the window size around each SNP (<code>window_size</code>), the window size in kilobases (<code>kb_window_size</code>), and the window size in centi Morgan (<code>cm_window_size</code>).</p> <p>See Also</p> <ul> <li>WindowedLD</li> <li>BlockLD</li> </ul> <p>Attributes:</p> Name Type Description <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> <code>ld_boundaries</code> <p>The LD boundaries for each variant in the LD matrix.</p> <code>window_size</code> <p>The number of neighboring SNPs to consider on each side when computing LD.</p> <code>kb_window_size</code> <p>The maximum distance in kilobases to consider when computing LD.</p> <code>cm_window_size</code> <p>The maximum distance in centi Morgan to consider when computing LD.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>class WindowedLD(SampleLD):\n    \"\"\"\n    A wrapper class to facilitate computing windowed Linkage-Disequilibrium (LD) matrices.\n    Windowed LD matrices only record pairwise correlations between variants that are within a certain\n    distance of each other along the chromosome. This is useful for reducing the memory requirements\n    and noise in the LD matrix.\n\n    The `WindowedLD` estimator supports a variety of ways for defining the window size:\n\n    * `window_size`: The number of neighboring SNPs to consider on each side when computing LD.\n    * `kb_window_size`: The maximum distance in kilobases to consider when computing LD.\n    * `cm_window_size`: The maximum distance in centi Morgan to consider when computing LD.\n\n    The LD boundaries computed here are the intersection of the windows defined by the window size around\n    each SNP (`window_size`), the window size in kilobases (`kb_window_size`), and the window size in centi Morgan\n    (`cm_window_size`).\n\n    !!! seealso \"See Also\"\n        * [WindowedLD][magenpy.stats.ld.estimator.ShrinkageLD]\n        * [BlockLD][magenpy.stats.ld.estimator.BlockLD]\n\n    :ivar genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :ivar ld_boundaries: The LD boundaries for each variant in the LD matrix.\n    :ivar window_size: The number of neighboring SNPs to consider on each side when computing LD.\n    :ivar kb_window_size: The maximum distance in kilobases to consider when computing LD.\n    :ivar cm_window_size: The maximum distance in centi Morgan to consider when computing LD.\n\n    \"\"\"\n\n    estimator_id = 'windowed'\n\n    def __init__(self,\n                 genotype_matrix,\n                 window_size=None,\n                 kb_window_size=None,\n                 cm_window_size=None):\n        \"\"\"\n\n        Initialize the windowed LD estimator with a genotype matrix and window size parameters.\n\n        :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n        :param window_size: The number of neighboring SNPs to consider on each side when computing LD.\n        :param kb_window_size: The maximum distance in kilobases to consider when computing LD.\n        :param cm_window_size: The maximum distance in centi Morgan to consider when computing LD.\n        \"\"\"\n\n        super().__init__(genotype_matrix=genotype_matrix)\n\n        assert not all([w is None for w in (window_size, kb_window_size, cm_window_size)])\n\n        self.window_size = window_size\n        self.kb_window_size = kb_window_size\n        self.cm_window_size = cm_window_size\n\n    def compute_ld_boundaries(self):\n        \"\"\"\n         Compute the windowed Linkage-Disequilibrium (LD) boundaries.\n         The LD boundaries computed here are the intersection of the windows defined by\n         the window size around each SNP (`window_size`), the window size in kilobases (`kb_window_size`),\n         and the window size in centi Morgan (`cm_window_size`).\n\n         :return: A 2xM matrix of LD boundaries.\n        \"\"\"\n\n        if self.ld_boundaries is None:\n\n            bounds = []\n\n            m = self.genotype_matrix.n_snps\n            indices = np.arange(m)\n\n            if self.window_size is not None:\n                bounds.append(\n                    np.clip(np.array(\n                        [indices - self.window_size,\n                         indices + self.window_size\n                         ]\n                    ),  a_min=0, a_max=m)\n                )\n\n            from .c_utils import find_windowed_ld_boundaries\n\n            if self.kb_window_size is not None:\n                bounds.append(\n                    find_windowed_ld_boundaries(.001*self.genotype_matrix.bp_pos,\n                                                self.kb_window_size)\n                )\n\n            if self.cm_window_size is not None:\n                bounds.append(\n                    find_windowed_ld_boundaries(self.genotype_matrix.cm_pos,\n                                                self.cm_window_size)\n                )\n\n            if len(bounds) == 1:\n                self.ld_boundaries = bounds[0]\n            else:\n                self.ld_boundaries = np.array([\n                    np.maximum.reduce([b[0, :] for b in bounds]),\n                    np.minimum.reduce([b[1, :] for b in bounds])\n                ])\n\n        return self.ld_boundaries\n\n    def compute(self,\n                output_dir,\n                overwrite=True,\n                delete_original=True,\n                dtype='int16',\n                compressor_name='zstd',\n                compression_level=7,\n                compute_spectral_properties=False) -&gt; LDMatrix:\n        \"\"\"\n\n        Compute the windowed LD matrix and store in Zarr array format.\n\n        :param output_dir: The path where to store the resulting LD matrix.\n        :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n        :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n        :param dtype: The data type for the entries of the LD matrix.\n        :param compressor_name: The name of the compressor to use for the LD matrix.\n        :param compression_level: The compression level to use for the LD matrix (1-9).\n        :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n        the LD matrix.\n\n        :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n        \"\"\"\n\n        ld_mat = super().compute(output_dir,\n                                 overwrite=overwrite,\n                                 delete_original=delete_original,\n                                 compute_spectral_properties=compute_spectral_properties,\n                                 dtype=dtype,\n                                 compressor_name=compressor_name,\n                                 compression_level=compression_level)\n\n        ld_mat.set_store_attr('LD estimator', 'windowed')\n\n        w_properties = {}\n        if self.window_size is not None:\n            w_properties['Window size'] = self.window_size\n\n        if self.kb_window_size is not None:\n            w_properties['Window size (kb)'] = self.kb_window_size\n\n        if self.cm_window_size is not None:\n            w_properties['Window size (cM)'] = self.cm_window_size\n\n        ld_mat.set_store_attr('Estimator properties', w_properties)\n\n        if compute_spectral_properties:\n\n            if 'Spectral properties' in ld_mat.list_store_attributes():\n                spectral_prop = ld_mat.get_store_attr('Spectral properties')\n            else:\n                spectral_prop = {\n                    'Extremal': ld_mat.estimate_extremal_eigenvalues()\n                }\n\n            # Estimate extremal eigenvalues within blocks:\n            # To quantify the impact of sparsification, we increase the window sizes here by 20%:\n\n            if self.window_size is not None:\n                eig_window_size = int(1.2*self.window_size)\n            else:\n                eig_window_size = None\n\n            if self.kb_window_size is not None:\n                eig_kb_window_size = 1.2*self.kb_window_size\n            else:\n                eig_kb_window_size = None\n\n            if self.cm_window_size is not None:\n                eig_cm_window_size = 1.2*self.cm_window_size\n            else:\n                eig_cm_window_size = None\n\n            eigs, block_bounds = ld_mat.estimate_extremal_eigenvalues(\n                block_size=eig_window_size,\n                block_size_kb=eig_kb_window_size,\n                block_size_cm=eig_cm_window_size,\n                return_block_boundaries=True\n            )\n\n            spectral_prop['Eigenvalues per block'] = {**eigs, **block_bounds}\n\n            # Estimate minimum eigenvalues while excluding long-range LD regions (if they're present):\n            n_snps_before = ld_mat.n_snps\n            ld_mat.filter_long_range_ld_regions()\n            n_snps_after = ld_mat.n_snps\n\n            if n_snps_after &lt; n_snps_before:\n                spectral_prop['Extremal (excluding LRLD)'] = ld_mat.estimate_extremal_eigenvalues()\n\n            # Update or set the spectral properties attribute:\n            ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n            # Reset the mask:\n            ld_mat.reset_mask()\n\n        return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.WindowedLD.__init__","title":"<code>__init__(genotype_matrix, window_size=None, kb_window_size=None, cm_window_size=None)</code>","text":"<p>Initialize the windowed LD estimator with a genotype matrix and window size parameters.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>The genotype matrix, an instance of <code>GenotypeMatrix</code>.</p> required <code>window_size</code> <p>The number of neighboring SNPs to consider on each side when computing LD.</p> <code>None</code> <code>kb_window_size</code> <p>The maximum distance in kilobases to consider when computing LD.</p> <code>None</code> <code>cm_window_size</code> <p>The maximum distance in centi Morgan to consider when computing LD.</p> <code>None</code> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def __init__(self,\n             genotype_matrix,\n             window_size=None,\n             kb_window_size=None,\n             cm_window_size=None):\n    \"\"\"\n\n    Initialize the windowed LD estimator with a genotype matrix and window size parameters.\n\n    :param genotype_matrix: The genotype matrix, an instance of `GenotypeMatrix`.\n    :param window_size: The number of neighboring SNPs to consider on each side when computing LD.\n    :param kb_window_size: The maximum distance in kilobases to consider when computing LD.\n    :param cm_window_size: The maximum distance in centi Morgan to consider when computing LD.\n    \"\"\"\n\n    super().__init__(genotype_matrix=genotype_matrix)\n\n    assert not all([w is None for w in (window_size, kb_window_size, cm_window_size)])\n\n    self.window_size = window_size\n    self.kb_window_size = kb_window_size\n    self.cm_window_size = cm_window_size\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.WindowedLD.compute","title":"<code>compute(output_dir, overwrite=True, delete_original=True, dtype='int16', compressor_name='zstd', compression_level=7, compute_spectral_properties=False)</code>","text":"<p>Compute the windowed LD matrix and store in Zarr array format.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>The path where to store the resulting LD matrix.</p> required <code>overwrite</code> <p>If True, overwrite any existing LD matrices in <code>temp_dir</code> and <code>output_dir</code>.</p> <code>True</code> <code>delete_original</code> <p>If True, deletes dense or intermediate LD matrices generated along the way.</p> <code>True</code> <code>dtype</code> <p>The data type for the entries of the LD matrix.</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the LD matrix.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the LD matrix (1-9).</p> <code>7</code> <code>compute_spectral_properties</code> <p>If True, compute and store information about the eigenvalues of the LD matrix.</p> <code>False</code> <p>Returns:</p> Type Description <code>LDMatrix</code> <p>An instance of <code>LDMatrix</code> encapsulating the computed LD matrix, its attributes, and metadata.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute(self,\n            output_dir,\n            overwrite=True,\n            delete_original=True,\n            dtype='int16',\n            compressor_name='zstd',\n            compression_level=7,\n            compute_spectral_properties=False) -&gt; LDMatrix:\n    \"\"\"\n\n    Compute the windowed LD matrix and store in Zarr array format.\n\n    :param output_dir: The path where to store the resulting LD matrix.\n    :param overwrite: If True, overwrite any existing LD matrices in `temp_dir` and `output_dir`.\n    :param delete_original: If True, deletes dense or intermediate LD matrices generated along the way.\n    :param dtype: The data type for the entries of the LD matrix.\n    :param compressor_name: The name of the compressor to use for the LD matrix.\n    :param compression_level: The compression level to use for the LD matrix (1-9).\n    :param compute_spectral_properties: If True, compute and store information about the eigenvalues of\n    the LD matrix.\n\n    :return: An instance of `LDMatrix` encapsulating the computed LD matrix, its attributes, and metadata.\n    \"\"\"\n\n    ld_mat = super().compute(output_dir,\n                             overwrite=overwrite,\n                             delete_original=delete_original,\n                             compute_spectral_properties=compute_spectral_properties,\n                             dtype=dtype,\n                             compressor_name=compressor_name,\n                             compression_level=compression_level)\n\n    ld_mat.set_store_attr('LD estimator', 'windowed')\n\n    w_properties = {}\n    if self.window_size is not None:\n        w_properties['Window size'] = self.window_size\n\n    if self.kb_window_size is not None:\n        w_properties['Window size (kb)'] = self.kb_window_size\n\n    if self.cm_window_size is not None:\n        w_properties['Window size (cM)'] = self.cm_window_size\n\n    ld_mat.set_store_attr('Estimator properties', w_properties)\n\n    if compute_spectral_properties:\n\n        if 'Spectral properties' in ld_mat.list_store_attributes():\n            spectral_prop = ld_mat.get_store_attr('Spectral properties')\n        else:\n            spectral_prop = {\n                'Extremal': ld_mat.estimate_extremal_eigenvalues()\n            }\n\n        # Estimate extremal eigenvalues within blocks:\n        # To quantify the impact of sparsification, we increase the window sizes here by 20%:\n\n        if self.window_size is not None:\n            eig_window_size = int(1.2*self.window_size)\n        else:\n            eig_window_size = None\n\n        if self.kb_window_size is not None:\n            eig_kb_window_size = 1.2*self.kb_window_size\n        else:\n            eig_kb_window_size = None\n\n        if self.cm_window_size is not None:\n            eig_cm_window_size = 1.2*self.cm_window_size\n        else:\n            eig_cm_window_size = None\n\n        eigs, block_bounds = ld_mat.estimate_extremal_eigenvalues(\n            block_size=eig_window_size,\n            block_size_kb=eig_kb_window_size,\n            block_size_cm=eig_cm_window_size,\n            return_block_boundaries=True\n        )\n\n        spectral_prop['Eigenvalues per block'] = {**eigs, **block_bounds}\n\n        # Estimate minimum eigenvalues while excluding long-range LD regions (if they're present):\n        n_snps_before = ld_mat.n_snps\n        ld_mat.filter_long_range_ld_regions()\n        n_snps_after = ld_mat.n_snps\n\n        if n_snps_after &lt; n_snps_before:\n            spectral_prop['Extremal (excluding LRLD)'] = ld_mat.estimate_extremal_eigenvalues()\n\n        # Update or set the spectral properties attribute:\n        ld_mat.set_store_attr('Spectral properties', spectral_prop)\n\n        # Reset the mask:\n        ld_mat.reset_mask()\n\n    return ld_mat\n</code></pre>"},{"location":"api/stats/ld/estimator/#magenpy.stats.ld.estimator.WindowedLD.compute_ld_boundaries","title":"<code>compute_ld_boundaries()</code>","text":"<p>Compute the windowed Linkage-Disequilibrium (LD) boundaries. The LD boundaries computed here are the intersection of the windows defined by the window size around each SNP (<code>window_size</code>), the window size in kilobases (<code>kb_window_size</code>), and the window size in centi Morgan (<code>cm_window_size</code>).</p> <p>Returns:</p> Type Description <p>A 2xM matrix of LD boundaries.</p> Source code in <code>magenpy/stats/ld/estimator.py</code> <pre><code>def compute_ld_boundaries(self):\n    \"\"\"\n     Compute the windowed Linkage-Disequilibrium (LD) boundaries.\n     The LD boundaries computed here are the intersection of the windows defined by\n     the window size around each SNP (`window_size`), the window size in kilobases (`kb_window_size`),\n     and the window size in centi Morgan (`cm_window_size`).\n\n     :return: A 2xM matrix of LD boundaries.\n    \"\"\"\n\n    if self.ld_boundaries is None:\n\n        bounds = []\n\n        m = self.genotype_matrix.n_snps\n        indices = np.arange(m)\n\n        if self.window_size is not None:\n            bounds.append(\n                np.clip(np.array(\n                    [indices - self.window_size,\n                     indices + self.window_size\n                     ]\n                ),  a_min=0, a_max=m)\n            )\n\n        from .c_utils import find_windowed_ld_boundaries\n\n        if self.kb_window_size is not None:\n            bounds.append(\n                find_windowed_ld_boundaries(.001*self.genotype_matrix.bp_pos,\n                                            self.kb_window_size)\n            )\n\n        if self.cm_window_size is not None:\n            bounds.append(\n                find_windowed_ld_boundaries(self.genotype_matrix.cm_pos,\n                                            self.cm_window_size)\n            )\n\n        if len(bounds) == 1:\n            self.ld_boundaries = bounds[0]\n        else:\n            self.ld_boundaries = np.array([\n                np.maximum.reduce([b[0, :] for b in bounds]),\n                np.minimum.reduce([b[1, :] for b in bounds])\n            ])\n\n    return self.ld_boundaries\n</code></pre>"},{"location":"api/stats/ld/utils/","title":"Utils","text":""},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.clump_snps","title":"<code>clump_snps(ldm, statistic=None, rsq_threshold=0.9, extract=True, sort_key=None)</code>","text":"<p>This function takes an LDMatrix object and clumps SNPs based on the <code>stat</code> vector (usually p-value) and the provided r-squared threshold. If two SNPs have an r-squared greater than the threshold, the SNP with the higher <code>stat</code> value is excluded.</p> <p>TODO: Do more testing and come up with more efficient solutions. Perhaps moving the function to cython.</p> <p>Parameters:</p> Name Type Description Default <code>ldm</code> <code>LDMatrix</code> <p>An LDMatrix object</p> required <code>statistic</code> <p>A vector of statistics (e.g. p-values) for each SNP that will determine which SNPs to discard.</p> <code>None</code> <code>rsq_threshold</code> <p>The r^2 threshold to use for filtering variants.</p> <code>0.9</code> <code>extract</code> <p>If True, return remaining SNPs. If False, return removed SNPs.</p> <code>True</code> <code>sort_key</code> <p>The key function for the sorting algorithm that will decide how to sort the <code>statistic</code>. By default, we select the SNP with the minimum value for the <code>statistic</code> (e.g. smaller p-value).</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of SNP rsIDs that are left after clumping (or discarded if <code>extract=False</code>).</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def clump_snps(ldm: LDMatrix,\n               statistic=None,\n               rsq_threshold=.9,\n               extract=True,\n               sort_key=None):\n    \"\"\"\n    This function takes an LDMatrix object and clumps SNPs based\n    on the `stat` vector (usually p-value) and the provided r-squared threshold.\n    If two SNPs have an r-squared greater than the threshold,\n    the SNP with the higher `stat` value is excluded.\n\n    TODO: Do more testing and come up with more efficient solutions.\n    Perhaps moving the function to cython.\n\n    :param ldm: An LDMatrix object\n    :param statistic: A vector of statistics (e.g. p-values) for each SNP that will determine which SNPs to discard.\n    :param rsq_threshold: The r^2 threshold to use for filtering variants.\n    :param extract: If True, return remaining SNPs. If False, return removed SNPs.\n    :param sort_key: The key function for the sorting algorithm that will decide how to sort the `statistic`.\n    By default, we select the SNP with the minimum value for the `statistic` (e.g. smaller p-value).\n\n    :return: A list of SNP rsIDs that are left after clumping (or discarded if `extract=False`).\n    \"\"\"\n\n    if statistic is None:\n        # if a statistic is not provided, then clump SNPs based on their base pair order,\n        # meaning that if two SNPs are highly correlated, we keep the one with smaller base pair position.\n        statistic = ldm.bp_position\n    else:\n        assert len(statistic) == ldm.n_snps\n\n    if sort_key is not None:\n        sort_key = lambda x: sort_key(statistic[x])\n        sorted_idx = sorted(range(len(ldm)), key=sort_key)\n    else:\n        sorted_idx = range(len(ldm))\n\n    snps = ldm.snps\n    keep_snps = np.ones(len(snps), dtype=bool)\n\n    symm_ld = ldm.to_csr(return_symmetric=True, dtype='float32')\n\n    for idx in sorted_idx:\n\n        if not keep_snps[idx]:\n            continue\n\n        r = symm_ld.getrow(idx).toarray().flatten()\n        # Find the SNPs that we need to remove:\n        # We remove SNPs whose squared correlation coefficient with the index SNP is\n        # greater than the specified rsq_threshold:\n        idx_to_remove = np.where((r**2 &gt; rsq_threshold) &amp; keep_snps)[0]\n\n        if len(idx_to_remove) &gt; 0:\n            keep_snps[idx_to_remove] = False\n            # The variant has perfect correlation with itself,\n            # so we need to correct for that here:\n            keep_snps[idx] = True\n\n    if extract:\n        return snps[keep_snps]\n    else:\n        return snps[~keep_snps]\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.combine_ld_matrices","title":"<code>combine_ld_matrices(ld_matrices, output_dir, overwrite=True, delete_original=False)</code>","text":"<p>Combine the Zarr arrays underlying multiple <code>LDMatrix</code> objects into a single store, which will reside in <code>output_dir</code>. This function combines the <code>matrix/data/</code> and <code>matrix/indptr</code> arrays of the Zarr stores, in addition to the metadata, such as SNP rsIDs, basepair position, centi Morgan distance, etc.</p> <p>Warning</p> <p>This function does not copy any of the attributes of the individual <code>LDMatrix</code> objects.</p> <p>Warning</p> <p>This implementation assumes there is no overlap in the set of variants present in the LD matrices.</p> <p>Parameters:</p> Name Type Description Default <code>ld_matrices</code> <p>A list of <code>LDMatrix</code> objects to combine.</p> required <code>output_dir</code> <p>The output directory where to store the combined LD matrix.</p> required <code>overwrite</code> <p>If True, overwrites the output directory if it exists.</p> <code>True</code> <code>delete_original</code> <p>If True, deletes the original LD matrices after combining them.</p> <code>False</code> <p>Returns:</p> Type Description <p>An <code>LDMatrix</code> object pointing to the combined LD matrix.</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def combine_ld_matrices(ld_matrices, output_dir, overwrite=True, delete_original=False):\n    \"\"\"\n    Combine the Zarr arrays underlying multiple `LDMatrix` objects into\n    a single store, which will reside in `output_dir`. This function combines\n    the `matrix/data/` and `matrix/indptr` arrays of the Zarr stores, in addition\n    to the metadata, such as SNP rsIDs, basepair position, centi Morgan distance, etc.\n\n    !!! warning\n        This function does not copy any of the attributes of the individual `LDMatrix` objects.\n\n    !!! warning\n        This implementation assumes there is no overlap in the set of variants present in\n        the LD matrices.\n\n    :param ld_matrices: A list of `LDMatrix` objects to combine.\n    :param output_dir: The output directory where to store the combined LD matrix.\n    :param overwrite: If True, overwrites the output directory if it exists.\n    :param delete_original: If True, deletes the original LD matrices after combining them.\n\n    :return: An `LDMatrix` object pointing to the combined LD matrix.\n    \"\"\"\n\n    # Determine the total number of variants and total number of elements in the `data` array:\n    total_n_snps = sum([ld.stored_n_snps for ld in ld_matrices])\n    total_elements = sum([ld.data.shape[0] for ld in ld_matrices])\n    compressor = ld_matrices[0].compressor\n    metadata = list(ld_matrices[0].zarr_group['metadata'].keys())\n\n    # Create hierarchical storage with zarr groups:\n    store = zarr.DirectoryStore(output_dir)\n    z = zarr.group(store=store, overwrite=overwrite)\n\n    # Create sub-hierarchy to store the data of the sparse LD matrix:\n    mat = z.create_group('matrix')\n    mat.empty('data', shape=total_elements,\n              dtype=ld_matrices[0].stored_dtype,\n              compressor=compressor)\n    mat.zeros('indptr', shape=total_n_snps + 1,\n              dtype=np.int64, compressor=compressor)\n\n    # Create sub-hierarchy to store the metadata for the LD matrix:\n    meta = z.create_group('metadata')\n    for meta_col, meta_zarr in ld_matrices[0].zarr_group['metadata'].items():\n\n        if meta_zarr.dtype == object:\n            meta.empty(meta_col, shape=total_n_snps, dtype=str, compressor=compressor)\n        else:\n            meta.empty(meta_col, shape=total_n_snps,\n                       dtype=meta_zarr.dtype, compressor=compressor)\n\n    curr_n_snps = 0\n    curr_n_entries = 0\n\n    for ld in ld_matrices:\n\n        # Copy the matrix data:\n        mat['data'][curr_n_entries:curr_n_entries + ld.data.shape[0]] = ld.data[:]\n        mat['indptr'][curr_n_snps + 1:curr_n_snps + ld.stored_n_snps + 1] = curr_n_entries + ld.indptr[1:]\n\n        # Copy the metadata:\n\n        for meta_col in metadata:\n            meta[meta_col][curr_n_snps:curr_n_snps + ld.stored_n_snps] = ld.zarr_group[f'metadata/{meta_col}'][:]\n\n        # Update the counters:\n        curr_n_snps += ld.stored_n_snps\n        curr_n_entries += ld.data.shape[0]\n\n        # Delete the original store (if requested):\n\n        if delete_original:\n            delete_ld_store(ld)\n\n    return LDMatrix(z)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.compute_extremal_eigenvalues","title":"<code>compute_extremal_eigenvalues(mat, k=1, which='both', **eigsh_kwargs)</code>","text":"<p>A helper function to compute the extremal eigenvalues (minimum and maximum) of the LD matrix efficiently. This function uses a trick to shift the eigenvalues to the right by the maximum eigenvalue and then computes the smallest eigenvalue of the shifted matrix. This is a more stable and efficient approach for computing the smallest eigenvalue of the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <p>A <code>scipy</code> sparse CSR matrix object or an LDLinearOperator object for which to compute the extremal eigenvalues.</p> required <code>k</code> <p>The number of eigenvalues to compute.</p> <code>1</code> <code>which</code> <p>Which extremal eigenalue to return. Can be 'both', 'min', or 'max'.</p> <code>'both'</code> <code>eigsh_kwargs</code> <p>Additional keyword arguments to pass to the <code>scipy.sparse.linalg.eigsh</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A dictionary containing the extremal eigenvalues of the LD matrix.</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def compute_extremal_eigenvalues(mat, k=1, which='both', **eigsh_kwargs):\n    \"\"\"\n    A helper function to compute the extremal eigenvalues (minimum and maximum) of the LD matrix efficiently.\n    This function uses a trick to shift the eigenvalues to the right by the maximum eigenvalue\n    and then computes the smallest eigenvalue of the shifted matrix. This is a more stable\n    and efficient approach for computing the smallest eigenvalue of the LD matrix.\n\n    :param mat: A `scipy` sparse CSR matrix object or an LDLinearOperator object for which to compute the\n    extremal eigenvalues.\n    :param k: The number of eigenvalues to compute.\n    :param which: Which extremal eigenalue to return. Can be 'both', 'min', or 'max'.\n    :param eigsh_kwargs: Additional keyword arguments to pass to the `scipy.sparse.linalg.eigsh` function.\n\n    :return: A dictionary containing the extremal eigenvalues of the LD matrix.\n    \"\"\"\n\n    assert which in ('both', 'min', 'max')\n\n    if 'maxiter' not in eigsh_kwargs:\n        eigsh_kwargs['maxiter'] = 10000\n\n    if 'tol' not in eigsh_kwargs:\n        eigsh_kwargs['tol'] = 1e-6\n\n    from scipy.sparse.linalg import eigsh, ArpackNoConvergence\n    from scipy.sparse import csr_matrix, identity\n\n    eig_result = {}\n\n    # In all cases we need to compute the largest eigenvalue:\n    eig_result['max'] = eigsh(mat, k=k, which='LM', return_eigenvectors=False, **eigsh_kwargs)\n    max_eig = np.max(eig_result['max'])\n\n    # Compute the minimum eigenvalue (if requested):\n    if which in ('min', 'both'):\n\n        if isinstance(mat, csr_matrix):\n            diag_shift = max_eig*identity(mat.shape[0], format='csr', dtype=mat.dtype)\n            mat -= diag_shift\n        else:\n            if mat.diag_shift is not None:\n                orig_diag_shift = copy.copy(mat.diag_shift)\n                mat.set_diag_shift(mat.diag_shift-max_eig)\n            else:\n                orig_diag_shift = None\n                mat.set_diag_shift(-max_eig)\n\n        # TODO: Try other solvers here?\n        try:\n            lm_max_hat = eigsh(mat, k=k, which='LM', return_eigenvectors=False, **eigsh_kwargs)\n        except ArpackNoConvergence:\n            # If the solver does not converge, we can try to increase the number of iterations and the tolerance:\n            if 'maxiter' in eigsh_kwargs:\n                eigsh_kwargs['maxiter'] *= 2\n\n            if 'tol' in eigsh_kwargs:\n                eigsh_kwargs['tol'] *= 10\n\n            lm_max_hat = eigsh(mat, k=k, which='LM', return_eigenvectors=False, **eigsh_kwargs)\n\n        if isinstance(mat, csr_matrix):\n            mat += diag_shift\n        else:\n            mat.set_diag_shift(orig_diag_shift)\n\n        eig_result['min'] = lm_max_hat + max_eig\n\n    if k == 1:\n        for key in eig_result:\n            eig_result[key] = eig_result[key][0]\n\n    if which == 'both':\n        return eig_result\n    else:\n        return eig_result[which]\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.compute_ld_plink1p9","title":"<code>compute_ld_plink1p9(genotype_matrix, ld_boundaries, output_dir, window_size_thresholds, trim_boundaries=False, temp_dir='temp', overwrite=True, dtype='int16', compressor_name='zstd', compression_level=7)</code>","text":"<p>Compute LD matrices using plink 1.9.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A plinkBEDGenotypeMatrix object</p> required <code>ld_boundaries</code> <p>An 2xM matrix of LD boundaries for every variant.</p> required <code>output_dir</code> <p>The output directory for the final LD matrix file (after processing).</p> required <code>window_size_thresholds</code> <p>A dictionary of the window size thresholds to pass to plink1.9.</p> required <code>trim_boundaries</code> <p>If True, trims the LD rows computed by plink. This is useful in cases where the window size varies per variant, but we ask plink to compute the LD for a fixed (maximum) window size.</p> <code>False</code> <code>temp_dir</code> <p>A temporary directory to store intermediate files (e.g. files created for and by plink).</p> <code>'temp'</code> <code>overwrite</code> <p>If True, it overwrites any LD matrices in <code>output_dir</code>.</p> <code>True</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the Zarr arrays.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the Zarr arrays (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An LDMatrix object</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def compute_ld_plink1p9(genotype_matrix,\n                        ld_boundaries,\n                        output_dir,\n                        window_size_thresholds,\n                        trim_boundaries=False,\n                        temp_dir='temp',\n                        overwrite=True,\n                        dtype='int16',\n                        compressor_name='zstd',\n                        compression_level=7):\n\n    \"\"\"\n    Compute LD matrices using plink 1.9.\n\n    :param genotype_matrix: A plinkBEDGenotypeMatrix object\n    :param ld_boundaries: An 2xM matrix of LD boundaries for every variant.\n    :param output_dir: The output directory for the final LD matrix file (after processing).\n    :param window_size_thresholds: A dictionary of the window size thresholds to pass to plink1.9.\n    :param trim_boundaries: If True, trims the LD rows computed by plink. This is useful in cases where the\n    window size varies per variant, but we ask plink to compute the LD for a fixed (maximum) window size.\n    :param temp_dir: A temporary directory to store intermediate files (e.g. files created for and by plink).\n    :param overwrite: If True, it overwrites any LD matrices in `output_dir`.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor to use for the Zarr arrays.\n    :param compression_level: The compression level to use for the Zarr arrays (1-9).\n\n    :return: An LDMatrix object\n    \"\"\"\n\n    from ...utils.executors import plink1Executor\n    from ...GenotypeMatrix import plinkBEDGenotypeMatrix\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink1 = plink1Executor()\n\n    keep_file = osp.join(temp_dir, 'samples.keep')\n    keep_table = genotype_matrix.sample_table.get_individual_table()\n    keep_table.to_csv(keep_file, index=False, header=False, sep=\"\\t\")\n\n    snp_keepfile = osp.join(temp_dir, 'variants.keep')\n    pd.DataFrame({'SNP': genotype_matrix.snps}).to_csv(\n        snp_keepfile, index=False, header=False\n    )\n\n    plink_output = osp.join(temp_dir, f'chr_{str(genotype_matrix.chromosome)}')\n\n    cmd = [\n        f\"--bfile {genotype_matrix.bed_file.replace('.bed', '')}\",\n        f\"--keep {keep_file}\",\n        f\"--extract {snp_keepfile}\",\n        \"--keep-allele-order\",\n        f\"--out {plink_output}\",\n        \"--r gz\",\n        f\"--ld-window {window_size_thresholds['window_size']}\",\n        f\"--ld-window-kb {window_size_thresholds['kb_window_size']}\"\n    ]\n\n    if 'cm_window_size' in window_size_thresholds:\n        cmd.append(f\"--ld-window-cm {window_size_thresholds['cm_window_size']}\")\n\n    # ---------------------------------------------------------\n    # Test if plink1.9 version is compatible with setting the --ld-window-r2 flag:\n    # This is important to account for due to differences in the behavior of plink1.9\n    # across different versions.\n    # See here for discussion of this behavior: https://github.com/shz9/viprs/issues/3\n\n    plink1.verbose = False\n\n    r2_flag_compatible = True\n\n    from subprocess import CalledProcessError\n\n    try:\n        plink1.execute([\"--r gz\", \"--ld-window-r2 0\"])\n    except CalledProcessError as e:\n        if \"--ld-window-r2 flag cannot be used with --r\" in e.stderr.decode():\n            r2_flag_compatible = False\n\n    if r2_flag_compatible:\n        cmd += [\"--ld-window-r2 0\"]\n\n    plink1.verbose = True\n\n    # ---------------------------------------------------------\n\n    plink1.execute(cmd)\n\n    # Convert from PLINK LD files to Zarr:\n    fin_ld_store = osp.join(output_dir, 'chr_' + str(genotype_matrix.chromosome))\n\n    # Compute the pandas chunk_size\n    # The goal of this is to process chunks of the LD table without overwhelming memory resources:\n    avg_ncols = int((ld_boundaries[1, :] - ld_boundaries[0, :]).mean())\n    # NOTE: Estimate the rows per chunk using float32 because that's how we'll read the data from plink:\n    rows_per_chunk = estimate_rows_per_chunk(ld_boundaries.shape[1], avg_ncols, dtype=np.float32)\n\n    if rows_per_chunk &gt; 0.1*ld_boundaries.shape[1]:\n        pandas_chunksize = None\n    else:\n        pandas_chunksize = rows_per_chunk*avg_ncols // 2\n\n    return LDMatrix.from_plink_table(f\"{plink_output}.ld.gz\",\n                                     genotype_matrix.snps,\n                                     fin_ld_store,\n                                     ld_boundaries=[None, ld_boundaries][trim_boundaries],\n                                     pandas_chunksize=pandas_chunksize,\n                                     overwrite=overwrite,\n                                     dtype=dtype,\n                                     compressor_name=compressor_name,\n                                     compression_level=compression_level)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.compute_ld_xarray","title":"<code>compute_ld_xarray(genotype_matrix, ld_boundaries, output_dir, temp_dir='temp', overwrite=True, delete_original=True, dtype='int16', compressor_name='zstd', compression_level=7)</code>","text":"<p>Compute the Linkage Disequilibrium matrix or snp-by-snp correlation matrix assuming that the genotypes are represented by <code>xarray</code> or <code>dask</code>-like matrix objects. This function computes the entire X'X/N and stores the result on-disk in Zarr arrays. Then, we call the utilities from the <code>LDMatrix</code> class to sparsify the dense matrix according to the parameters specified by the <code>ld_boundaries</code> matrix.</p> <p>Warning</p> <p>We don't recommend using this for large-scale genotype matrices. Use <code>compute_ld_plink1p9</code> instead if you have plink1.9 installed on your system.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>An <code>xarrayGenotypeMatrix</code> object</p> required <code>ld_boundaries</code> <p>An array of LD boundaries for every SNP</p> required <code>output_dir</code> <p>The output directory for the final LD matrix file.</p> required <code>temp_dir</code> <p>A temporary directory where to store intermediate results.</p> <code>'temp'</code> <code>overwrite</code> <p>If True, overwrites LD matrices in <code>temp_dir</code> and <code>output_dir</code>, if they exist.</p> <code>True</code> <code>delete_original</code> <p>If True, it deletes the original dense matrix after generating the sparse alternative.</p> <code>True</code> <code>dtype</code> <p>The data type for the entries of the LD matrix (supported data types are float32, float64 and integer quantized data types int8 and int16).</p> <code>'int16'</code> <code>compressor_name</code> <p>The name of the compressor to use for the Zarr arrays.</p> <code>'zstd'</code> <code>compression_level</code> <p>The compression level to use for the Zarr arrays (1-9).</p> <code>7</code> <p>Returns:</p> Type Description <p>An LDMatrix object</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def compute_ld_xarray(genotype_matrix,\n                      ld_boundaries,\n                      output_dir,\n                      temp_dir='temp',\n                      overwrite=True,\n                      delete_original=True,\n                      dtype='int16',\n                      compressor_name='zstd',\n                      compression_level=7):\n\n    \"\"\"\n    Compute the Linkage Disequilibrium matrix or snp-by-snp\n    correlation matrix assuming that the genotypes are represented\n    by `xarray` or `dask`-like matrix objects. This function computes the\n    entire X'X/N and stores the result on-disk in Zarr arrays. Then, we call the utilities\n    from the `LDMatrix` class to sparsify the dense matrix according to the parameters\n    specified by the `ld_boundaries` matrix.\n\n\n    !!! Warning\n        We don't recommend using this for large-scale genotype matrices.\n        Use `compute_ld_plink1p9` instead if you have plink1.9 installed on your system.\n\n    :param genotype_matrix: An `xarrayGenotypeMatrix` object\n    :param ld_boundaries: An array of LD boundaries for every SNP\n    :param output_dir: The output directory for the final LD matrix file.\n    :param temp_dir: A temporary directory where to store intermediate results.\n    :param overwrite: If True, overwrites LD matrices in `temp_dir` and `output_dir`, if they exist.\n    :param delete_original: If True, it deletes the original dense matrix after generating the sparse alternative.\n    :param dtype: The data type for the entries of the LD matrix (supported data types are float32, float64\n        and integer quantized data types int8 and int16).\n    :param compressor_name: The name of the compressor to use for the Zarr arrays.\n    :param compression_level: The compression level to use for the Zarr arrays (1-9).\n\n    :return: An LDMatrix object\n    \"\"\"\n\n    from ...GenotypeMatrix import xarrayGenotypeMatrix\n\n    assert isinstance(genotype_matrix, xarrayGenotypeMatrix)\n\n    g_data = genotype_matrix.xr_mat.data\n\n    # Re-chunk the array to optimize computational speed and efficiency:\n    # New chunksizes:\n    new_chunksizes = (min(1024, g_data.shape[0]), min(1024, g_data.shape[1]))\n    g_data = g_data.rechunk(new_chunksizes)\n\n    from ..transforms.genotype import standardize\n    import dask.array as da\n\n    # Standardize the genotype matrix and fill missing data with zeros:\n    g_mat = standardize(g_data)\n    g_mat = da.nan_to_num(g_mat)\n\n    # Compute the full LD matrix and store to a temporary directory in the form of Zarr arrays:\n    import warnings\n\n    # Ignore performance-related warnings from Dask:\n    with warnings.catch_warnings():\n\n        if np.issubdtype(np.dtype(dtype), np.integer):\n            # If the requested data type is integer, we need to convert\n            # the data to `float32` to avoid overflow errors when computing the dot product:\n            dot_dtype = np.float32\n        else:\n            dot_dtype = dtype\n\n        warnings.simplefilter(\"ignore\")\n        ld_mat = (da.dot(g_mat.T, g_mat) / genotype_matrix.sample_size).astype(dot_dtype)\n        ld_mat.to_zarr(temp_dir, overwrite=overwrite)\n\n    fin_ld_store = osp.join(output_dir, 'chr_' + str(genotype_matrix.chromosome))\n\n    # Load the dense matrix and transform it to a sparse matrix using utilities implemented in the\n    # `LDMatrix` class:\n    return LDMatrix.from_dense_zarr_matrix(temp_dir,\n                                           ld_boundaries,\n                                           fin_ld_store,\n                                           overwrite=overwrite,\n                                           delete_original=delete_original,\n                                           dtype=dtype,\n                                           compressor_name=compressor_name,\n                                           compression_level=compression_level)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.delete_ld_store","title":"<code>delete_ld_store(ld_mat)</code>","text":"<p>Delete the LD store from disk.</p> <p>Parameters:</p> Name Type Description Default <code>ld_mat</code> <p>An <code>LDMatrix</code> object</p> required Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def delete_ld_store(ld_mat):\n    \"\"\"\n    Delete the LD store from disk.\n    :param ld_mat: An `LDMatrix` object\n    \"\"\"\n\n    try:\n        ld_mat.store.rmdir()\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.estimate_rows_per_chunk","title":"<code>estimate_rows_per_chunk(rows, cols, dtype='int16', mem_size=128)</code>","text":"<p>Estimate the number of rows per chunk for matrices conditional on the desired size of the chunk in MB. The estimator takes as input the number of rows, columns, data type, and projected size of the chunk in memory.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Total number of rows in the matrix.</p> required <code>cols</code> <p>Total number of columns. If sparse matrix with uneven columns, provide average column size.</p> required <code>dtype</code> <p>The data type for the matrix entries.</p> <code>'int16'</code> <code>mem_size</code> <p>Size of the chunk in memory (MB)</p> <code>128</code> <p>Returns:</p> Type Description <p>The estimated number of rows per chunk.</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def estimate_rows_per_chunk(rows, cols, dtype='int16', mem_size=128):\n    \"\"\"\n    Estimate the number of rows per chunk for matrices conditional on the desired size of the chunk in MB.\n    The estimator takes as input the number of rows, columns, data type, and projected size of the chunk in memory.\n\n    :param rows: Total number of rows in the matrix.\n    :param cols: Total number of columns. If sparse matrix with uneven columns, provide average column size.\n    :param dtype: The data type for the matrix entries.\n    :param mem_size: Size of the chunk in memory (MB)\n\n    :return: The estimated number of rows per chunk.\n    \"\"\"\n\n    matrix_size = rows * cols * np.dtype(dtype).itemsize / 1024 ** 2\n    n_chunks = max(1, matrix_size // mem_size)\n\n    return rows // n_chunks\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.expand_snps","title":"<code>expand_snps(seed_snps, ldm, rsq_threshold=0.9)</code>","text":"<p>Given an initial set of SNPs, expand the set by adding \"neighbors\" whose squared correlation with the is higher than a user-specified threshold.</p> <p>Warnings</p> <p>This code is incomplete and untested.</p> <p>Parameters:</p> Name Type Description Default <code>seed_snps</code> <p>An iterable containing initial set of SNP rsIDs.</p> required <code>ldm</code> <p>An <code>LDMatrix</code> object containing SNP-by-SNP correlations.</p> required <code>rsq_threshold</code> <p>The r^2 threshold to use for including variants.</p> <code>0.9</code> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def expand_snps(seed_snps, ldm, rsq_threshold=0.9):\n    \"\"\"\n    Given an initial set of SNPs, expand the set by adding\n    \"neighbors\" whose squared correlation with the is higher than\n    a user-specified threshold.\n\n    !!! warnings\n        This code is incomplete and untested.\n\n    :param seed_snps: An iterable containing initial set of SNP rsIDs.\n    :param ldm: An `LDMatrix` object containing SNP-by-SNP correlations.\n    :param rsq_threshold: The r^2 threshold to use for including variants.\n\n    \"\"\"\n\n    ldm_snps = ldm.snps\n    snp_seed_idx = np.where(np.isin(seed_snps, ldm_snps))\n\n    if len(snp_seed_idx) &lt; 1:\n        print(\"Warning: None of the seed SNPs are present in the LD matrix object!\")\n        return seed_snps\n\n    final_set = set(seed_snps)\n\n    for idx in snp_seed_idx:\n        r, indices = ldm.getrow(idx, return_indices=True)\n        final_set = final_set.union(set(ldm_snps[indices[np.where(r**2 &gt; rsq_threshold)[0]]]))\n\n    return list(final_set)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.move_ld_store","title":"<code>move_ld_store(z_arr, target_path, overwrite=True)</code>","text":"<p>Move an LD store from its current path to the <code>target_path</code></p> <p>Parameters:</p> Name Type Description Default <code>z_arr</code> <p>An LDMatrix object</p> required <code>target_path</code> <p>The target path where to move the LD store</p> required <code>overwrite</code> <p>If True, overwrites the target path if it exists.</p> <code>True</code> <p>Returns:</p> Type Description <p>A Zaarr array object pointing to the new location of the LD store.</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def move_ld_store(z_arr, target_path, overwrite=True):\n    \"\"\"\n    Move an LD store from its current path to the `target_path`\n    :param z_arr: An LDMatrix object\n    :param target_path: The target path where to move the LD store\n    :param overwrite: If True, overwrites the target path if it exists.\n\n    :return: A Zaarr array object pointing to the new location of the LD store.\n    \"\"\"\n\n    source_path = z_arr.store.dir_path()\n\n    if overwrite or not any(os.scandir(target_path)):\n        import shutil\n        shutil.rmtree(target_path, ignore_errors=True)\n        shutil.move(source_path, target_path)\n\n    return zarr.open(target_path)\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.multivariate_normal_conditional_sampling","title":"<code>multivariate_normal_conditional_sampling(ldm, mean=None, cov_scale=None, size=None, method='eigh', threads=1, seed=None, max_block_size=500)</code>","text":"<p>Perform conditional sampling from a multivariate normal distribution where the covariance matrix is a linear scaling of the LD matrix. This function is useful for imputation or generating synthetic data, with realistic LD structure.</p> <p>The function generates a vector of dimension <code>n_snps</code> (i.e. the number of variants represented in the LD matrix) with covariance structure determined by the LD matrix, according to the following formula:</p> <p>x ~ MVN(mean, cov_scale * LD)</p> <p>where <code>mean</code> is the mean vector of the multivariate normal distribution, <code>cov_scale</code> is a scaling factor, and <code>LD</code> is the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ldm</code> <p>An <code>LDMatrix</code> object containing the LD matrix.</p> required <code>mean</code> <p>The mean of the multivariate normal distribution.</p> <code>None</code> <code>cov_scale</code> <p>The scaling factor for the covariance matrix.</p> <code>None</code> <code>size</code> <p>The number of samples to generate.</p> <code>None</code> <code>method</code> <p>The method to use for computing the factor matrix (see documentation for <code>numpy.random.Generator.multivariate_normal</code>).</p> <code>'eigh'</code> <code>threads</code> <p>The number of threads to use for parallel computation.</p> <code>1</code> <code>seed</code> <p>The random seed for reproducibility.</p> <code>None</code> <code>max_block_size</code> <p>The maximum block size to use for computing the conditional samples.</p> <code>500</code> <p>Returns:</p> Type Description <p>A vector of samples from the conditional multivariate normal distribution.</p> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def multivariate_normal_conditional_sampling(ldm,\n                                             mean=None,\n                                             cov_scale=None,\n                                             size=None,\n                                             method='eigh',\n                                             threads=1,\n                                             seed=None,\n                                             max_block_size=500):\n    \"\"\"\n    Perform conditional sampling from a multivariate normal distribution where the covariance\n    matrix is a linear scaling of the LD matrix. This function is useful for imputation or\n    generating synthetic data, with realistic LD structure.\n\n    The function generates a vector of dimension `n_snps` (i.e. the number of variants represented\n    in the LD matrix) with covariance structure determined by the LD matrix, according to the\n    following formula:\n\n    x ~ MVN(mean, cov_scale * LD)\n\n    where `mean` is the mean vector of the multivariate normal distribution, `cov_scale` is a scaling factor,\n    and `LD` is the LD matrix.\n\n    :param ldm: An `LDMatrix` object containing the LD matrix.\n    :param mean: The mean of the multivariate normal distribution.\n    :param cov_scale: The scaling factor for the covariance matrix.\n    :param size: The number of samples to generate.\n    :param method: The method to use for computing the factor matrix (see documentation for\n    `numpy.random.Generator.multivariate_normal`).\n    :param threads: The number of threads to use for parallel computation.\n    :param seed: The random seed for reproducibility.\n    :param max_block_size: The maximum block size to use for computing the conditional samples.\n\n    :return: A vector of samples from the conditional multivariate normal distribution.\n    \"\"\"\n\n    if mean is not None:\n        assert mean.shape[0] == ldm.n_snps\n\n    if cov_scale is None:\n        cov_scale = 1.\n\n    rng = np.random.default_rng(seed)\n\n    def sample_block(cov):\n\n        try:\n            result = rng.multivariate_normal(np.zeros(cov.shape[0]),\n                                             cov,\n                                             check_valid='ignore',\n                                             size=size,\n                                             method=method)\n        except np.linalg.LinAlgError as e:\n            if method == 'cholesky':\n                # If cholesky, try again with eigen decomposition:\n                result = rng.multivariate_normal(np.zeros(cov.shape[0]),\n                                                 cov,\n                                                 size=size,\n                                                 method='eigh')\n            else:\n                raise e\n\n        del cov  # Free up memory\n        return result\n\n    parallel = Parallel(n_jobs=threads, backend='threading')\n\n    with parallel:\n        x = parallel(\n            delayed(sample_block)(cov_scale*cov_block) for cov_block in ldm.iter_blocks(\n                return_type='numpy',\n                return_symmetric=True,\n                dtype='float32',\n                max_block_size=max_block_size,\n            )\n        )\n\n    if size is not None:\n        x = np.concatenate(x, axis=1)\n    else:\n        x = np.concatenate(x)\n\n    if mean is not None:\n        return mean + x\n    else:\n        return x\n</code></pre>"},{"location":"api/stats/ld/utils/#magenpy.stats.ld.utils.shrink_ld_matrix","title":"<code>shrink_ld_matrix(ld_mat_obj, cm_pos, maf_var, genmap_ne, genmap_sample_size, shrinkage_cutoff=0.001, phased_haplotype=False, chunk_size=1000)</code>","text":"<p>Shrink the entries of the LD matrix using the shrinkage estimator described in Lloyd-Jones (2019) and Wen and Stephens (2010). The estimator is also implemented in the RSS software by Xiang Zhu:</p> <p>https://github.com/stephenslab/rss/blob/master/misc/get_corr.R</p> <p>TODO: Re-write this function without using CSR data structures. Also, consider saving shrinkage factor per pair of variants instead of shrunk LD entries?</p> <p>Parameters:</p> Name Type Description Default <code>ld_mat_obj</code> <p>An <code>LDMatrix</code> object encapsulating the LD matrix whose entries we wish to shrink.</p> required <code>cm_pos</code> <p>The position of each variant in the LD matrix in centi Morgan.</p> required <code>maf_var</code> <p>A vector of the variance in minor allele frequency (MAF) for each SNP in the LD matrix. Should be equivalent to 2pj(1. - pj), where pj is the MAF of SNP j.</p> required <code>genmap_ne</code> <p>The effective population size for the genetic map.</p> required <code>genmap_sample_size</code> <p>The sample size used to estimate the genetic map.</p> required <code>shrinkage_cutoff</code> <p>The cutoff value below which we assume that the shrinkage factor is zero.</p> <code>0.001</code> <code>phased_haplotype</code> <p>A flag indicating whether the LD was calculated from phased haplotypes.</p> <code>False</code> <code>chunk_size</code> <p>An optional parameter that sets the maximum number of rows processed simultaneously. The smaller the <code>chunk_size</code>, the less memory requirements needed for this step.</p> <code>1000</code> Source code in <code>magenpy/stats/ld/utils.py</code> <pre><code>def shrink_ld_matrix(ld_mat_obj,\n                     cm_pos,\n                     maf_var,\n                     genmap_ne,\n                     genmap_sample_size,\n                     shrinkage_cutoff=1e-3,\n                     phased_haplotype=False,\n                     chunk_size=1000):\n\n    \"\"\"\n    Shrink the entries of the LD matrix using the shrinkage estimator\n    described in Lloyd-Jones (2019) and Wen and Stephens (2010). The estimator\n    is also implemented in the RSS software by Xiang Zhu:\n\n    https://github.com/stephenslab/rss/blob/master/misc/get_corr.R\n\n    TODO: Re-write this function without using CSR data structures.\n    Also, consider saving shrinkage factor per pair of variants instead of shrunk LD entries?\n\n    :param ld_mat_obj: An `LDMatrix` object encapsulating the LD matrix whose entries we wish to shrink.\n    :param cm_pos: The position of each variant in the LD matrix in centi Morgan.\n    :param maf_var: A vector of the variance in minor allele frequency (MAF) for each SNP in the LD matrix. Should be\n    equivalent to 2*pj*(1. - pj), where pj is the MAF of SNP j.\n    :param genmap_ne: The effective population size for the genetic map.\n    :param genmap_sample_size: The sample size used to estimate the genetic map.\n    :param shrinkage_cutoff: The cutoff value below which we assume that the shrinkage factor is zero.\n    :param phased_haplotype: A flag indicating whether the LD was calculated from phased haplotypes.\n    :param chunk_size: An optional parameter that sets the maximum number of rows processed simultaneously. The smaller\n    the `chunk_size`, the less memory requirements needed for this step.\n    \"\"\"\n\n    # The multiplicative term for the shrinkage factor\n    # The shrinkage factor is 4 * Ne * (rho_ij/100) / (2*m)\n    # where Ne is the effective population size and m is the sample size\n    # for the genetic map and rho_ij is the distance between SNPs i and j\n    # in centi Morgan.\n    # Therefore, the multiplicative term that we need to apply\n    # to the distance between SNPs is: 4*Ne/(200*m), which is equivalent to 0.02*Ne/m\n    # See also: https://github.com/stephenslab/rss/blob/master/misc/get_corr.R\n    # and Wen and Stephens (2010)\n\n    mult_term = .02*genmap_ne / genmap_sample_size\n\n    def harmonic_series_sum(n):\n        \"\"\"\n        A utility function to compute the sum of the harmonic series\n        found in Equation 2.8 in Wen and Stephens (2010)\n        Acknowledgement: https://stackoverflow.com/a/27683292\n        \"\"\"\n        from scipy.special import digamma\n        return digamma(n + 1) + np.euler_gamma\n\n    # Compute theta according to Eq. 2.8 in Wen and Stephens (2010)\n\n    h_sum = harmonic_series_sum(2*genmap_sample_size - 1)  # The sum of the harmonic series in Eq. 2.8\n    theta = (1. / h_sum) / (2. * genmap_sample_size + 1. / h_sum)  # The theta parameter (related to mutation rate)\n    theta_factor = (1. - theta)**2  # The theta factor that we'll multiply all elements of the covariance matrix with\n    theta_diag_factor = .5 * theta * (1. - .5 * theta)  # The theta factor for the diagonal elements\n\n    # Phased haplotype/unphased genotype multiplicative factor:\n    # Wen and Stephens (2010), Section 2.4\n    phased_mult = [.5, 1.][phased_haplotype]\n\n    # We need to turn the correlation matrix into a covariance matrix to\n    # apply the shrinkage factor. For this, we have to multiply each row\n    # by the product of standard deviations:\n    maf_sd = np.sqrt(phased_mult*maf_var)\n\n    # According to Eqs. 2.6 and 2.7 in Wen and Stephens (2010), the shrunk standard deviation should be:\n    shrunk_sd = np.sqrt(theta_factor*maf_var*phased_mult + theta_diag_factor)\n\n    global_indptr = ld_mat_obj.indptr\n\n    for chunk_idx in range(int(np.ceil(len(ld_mat_obj) / chunk_size))):\n\n        start_row = chunk_idx*chunk_size\n        end_row = min((chunk_idx+1)*chunk_size, len(ld_mat_obj))\n\n        # Load the subset of the LD matrix specified by chunk_size.\n        csr_mat = ld_mat_obj.load_data(\n            start_row=start_row,\n            end_row=end_row,\n            return_symmetric=False,\n            return_square=False,\n            keep_original_shape=True,\n            return_as_csr=True,\n            dtype=np.float32\n        )\n\n        # Get the relevant portion of indices and pointers from the CSR matrix:\n        indptr = global_indptr[start_row:end_row+1]\n\n        row_indices = np.concatenate([\n            (start_row + r_idx)*np.ones(indptr[r_idx+1] - indptr[r_idx], dtype=int)\n            for r_idx in range(len(indptr) - 1)\n        ])\n\n        # Compute the shrinkage factor for entries in the current block:\n        shrink_factor = np.exp(-mult_term*np.abs(cm_pos[csr_mat.indices] - cm_pos[row_indices]))\n        # Set shrinkage factors below the cutoff value to 0.:\n        shrink_factor[shrink_factor &lt; shrinkage_cutoff] = 0.\n        # Compute the theta multiplicative factor following Eq. 2.6 in Wen and Stephens (2010)\n        shrink_factor *= theta_factor\n\n        # The factor to convert the entries of the correlation matrix into corresponding covariances:\n        to_cov_factor = maf_sd[row_indices]*maf_sd[csr_mat.indices]\n\n        # Compute the new denominator for the Pearson correlation:\n        # The shrunk standard deviation of SNP j multiplied by the shrunk standard deviations of each neighbor:\n        shrunk_sd_prod = shrunk_sd[row_indices]*shrunk_sd[csr_mat.indices]\n\n        # Finally, compute the shrunk LD matrix entries:\n        csr_mat.data *= to_cov_factor*shrink_factor / shrunk_sd_prod\n\n        # Update the LD matrix object inplace:\n        ld_mat_obj.update_rows_inplace(csr_mat, start_row=start_row, end_row=end_row)\n\n    return ld_mat_obj\n</code></pre>"},{"location":"api/stats/score/utils/","title":"Utils","text":""},{"location":"api/stats/score/utils/#magenpy.stats.score.utils.score_plink2","title":"<code>score_plink2(genotype_matrix, betas, standardize_genotype=False, temp_dir='temp')</code>","text":"<p>Perform linear scoring using PLINK2. This function takes a genotype matrix object encapsulating and referencing plink BED files as well as a matrix of effect sizes (betas) and performs linear scoring of the form:</p> <p>y = X * betas</p> <p>This is useful for computing polygenic scores (PGS). The function supports a matrix of <code>beta</code> values, in which case the function returns a matrix of PGS values, one for each column of <code>beta</code>. For example, if there are 10 sets of betas, the function will compute 10 polygenic scores for each individual represented in the genotype matrix <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>An instance of <code>plinkBEDGenotypeMatrix</code>.</p> required <code>betas</code> <p>A matrix of effect sizes (betas).</p> required <code>standardize_genotype</code> <p>If True, standardize the genotype to have mean zero and unit variance before scoring.</p> <code>False</code> <code>temp_dir</code> <p>The directory where the temporary files will be stored.</p> <code>'temp'</code> <p>Returns:</p> Type Description <p>A numpy array of polygenic scores.</p> Source code in <code>magenpy/stats/score/utils.py</code> <pre><code>def score_plink2(genotype_matrix,\n                 betas,\n                 standardize_genotype=False,\n                 temp_dir='temp'):\n    \"\"\"\n    Perform linear scoring using PLINK2.\n    This function takes a genotype matrix object encapsulating and referencing\n    plink BED files as well as a matrix of effect sizes (betas) and performs\n    linear scoring of the form:\n\n    y = X * betas\n\n    This is useful for computing polygenic scores (PGS). The function supports\n    a matrix of `beta` values, in which case the function returns a matrix of\n    PGS values, one for each column of `beta`. For example, if there are 10 sets\n    of betas, the function will compute 10 polygenic scores for each individual represented\n    in the genotype matrix `X`.\n\n    :param genotype_matrix: An instance of `plinkBEDGenotypeMatrix`.\n    :param betas: A matrix of effect sizes (betas).\n    :param standardize_genotype: If True, standardize the genotype to have mean zero and unit variance\n    before scoring.\n    :param temp_dir: The directory where the temporary files will be stored.\n\n    :return: A numpy array of polygenic scores.\n\n    \"\"\"\n\n    from ...GenotypeMatrix import plinkBEDGenotypeMatrix\n    from ...utils.executors import plink2Executor\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink2 = plink2Executor()\n\n    try:\n        betas_shape = betas.shape[1]\n        if betas_shape == 1:\n            raise IndexError\n        score_col_nums = f\"--score-col-nums 3-{3 + betas_shape - 1}\"\n    except IndexError:\n        betas_shape = 1\n        betas = betas.reshape(-1, 1)\n        score_col_nums = \"--score-col-nums 3\"\n\n    # Create the samples file:\n\n    s_table = genotype_matrix.sample_table\n\n    keep_file = osp.join(temp_dir, 'samples.keep')\n    keep_table = s_table.get_individual_table()\n    keep_table.to_csv(keep_file, index=False, header=False, sep=\"\\t\")\n\n    eff_file = osp.join(temp_dir, 'variant_effect_size.txt')\n    df = genotype_matrix.get_snp_table(['SNP', 'A1'])\n\n    for i in range(betas_shape):\n        df['BETA' + str(i)] = betas[:, i]\n\n    # Remove any variants whose effect size is zero for all BETAs:\n    df = df.loc[df[['BETA' + str(i) for i in range(betas_shape)]].sum(axis=1) != 0]\n\n    # If none of the variants have an effect size, return zeros:\n    if len(df) == 0:\n        pgs = np.zeros((len(keep_table), betas_shape))\n    else:\n        # Standardize the genotype, if requested:\n        if standardize_genotype:\n            standardize_text = ' variance-standardize'\n        else:\n            standardize_text = ''\n\n        df.to_csv(eff_file, index=False, sep=\"\\t\")\n\n        output_file = osp.join(temp_dir, 'samples')\n\n        cmd = [\n            f\"--bfile {genotype_matrix.bed_file}\",\n            f\"--keep {keep_file}\",\n            f\"--score {eff_file} 1 2 header-read cols=+scoresums{standardize_text}\",\n            score_col_nums,\n            f\"--out {output_file}\",\n        ]\n\n        plink2.execute(cmd)\n\n        if not osp.isfile(output_file + '.sscore'):\n            raise FileNotFoundError\n\n        dtypes = {'FID': str, 'IID': str}\n        for i in range(betas_shape):\n            dtypes.update({'PRS' + str(i): np.float64})\n\n        chr_pgs = pd.read_csv(output_file + '.sscore',\n                              sep=r'\\s+',\n                              names=['FID', 'IID'] + ['PRS' + str(i) for i in range(betas_shape)],\n                              skiprows=1,\n                              usecols=[0, 1] + [4 + betas_shape + i for i in range(betas_shape)],\n                              dtype=dtypes)\n        chr_pgs = keep_table.astype({'FID': str, 'IID': str}).merge(chr_pgs)\n\n        pgs = chr_pgs[['PRS' + str(i) for i in range(betas_shape)]].values\n\n    if betas_shape == 1:\n        pgs = pgs.flatten()\n\n    return pgs\n</code></pre>"},{"location":"api/stats/transforms/genotype/","title":"Genotype","text":""},{"location":"api/stats/transforms/genotype/#magenpy.stats.transforms.genotype.standardize","title":"<code>standardize(g_mat)</code>","text":"<p>Standardize the genotype matrix, such that the columns (i.e. variants) have zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>g_mat</code> <p>A two-dimensional matrix (numpy, dask, xarray, etc.) where the rows are samples (individuals) and the columns are genetic variants.</p> required <p>Returns:</p> Type Description <p>The standardized genotype matrix.</p> Source code in <code>magenpy/stats/transforms/genotype.py</code> <pre><code>def standardize(g_mat):\n    \"\"\"\n    Standardize the genotype matrix, such that the columns (i.e. variants)\n    have zero mean and unit variance.\n    :param g_mat: A two-dimensional matrix (numpy, dask, xarray, etc.) where the rows are samples (individuals)\n    and the columns are genetic variants.\n\n    :return: The standardized genotype matrix.\n\n    \"\"\"\n\n    return (g_mat - g_mat.mean(axis=0)) / g_mat.std(axis=0)\n</code></pre>"},{"location":"api/stats/transforms/phenotype/","title":"Phenotype","text":""},{"location":"api/stats/transforms/phenotype/#magenpy.stats.transforms.phenotype.adjust_for_covariates","title":"<code>adjust_for_covariates(phenotype, covariates)</code>","text":"<p>This function takes a phenotype vector and a matrix of covariates and applies covariate correction on the phenotype. Concretely, this involves fitting a linear model where the response is the phenotype and the predictors are the covariates and then returning the residuals.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype</code> <p>A vector of continuous or quantitative phenotypes.</p> required <code>covariates</code> <p>A matrix where each row corresponds to an individual and each column corresponds to a covariate (e.g. age, sex, PCs, etc.)  :return: The residuals of the linear model fit.</p> required Source code in <code>magenpy/stats/transforms/phenotype.py</code> <pre><code>def adjust_for_covariates(phenotype, covariates):\n    \"\"\"\n    This function takes a phenotype vector and a matrix of covariates\n    and applies covariate correction on the phenotype. Concretely,\n    this involves fitting a linear model where the response is the\n    phenotype and the predictors are the covariates and then returning\n    the residuals.\n    :param phenotype: A vector of continuous or quantitative phenotypes.\n    :param covariates: A matrix where each row corresponds to an individual\n     and each column corresponds to a covariate (e.g. age, sex, PCs, etc.)\n\n     :return: The residuals of the linear model fit.\n    \"\"\"\n\n    import statsmodels.api as sm\n\n    return sm.OLS(phenotype, sm.add_constant(covariates)).fit().resid\n</code></pre>"},{"location":"api/stats/transforms/phenotype/#magenpy.stats.transforms.phenotype.chained_transform","title":"<code>chained_transform(sample_table, adjust_covariates=False, standardize_phenotype=False, rint_phenotype=False, outlier_sigma_threshold=None, transform_order=('standardize', 'covariate_adjust', 'rint', 'outlier_removal'))</code>","text":"<p>Apply a chain of transformations to the phenotype vector.</p> <p>Parameters:</p> Name Type Description Default <code>sample_table</code> <p>An instance of SampleTable that contains phenotype information and other covariates about the samples in the dataset.</p> required <code>adjust_covariates</code> <p>If true, regress out the covariates from the phenotype. By default, we regress out all the covariates present in the SampleTable.</p> <code>False</code> <code>standardize_phenotype</code> <p>If true, standardize the phenotype.</p> <code>False</code> <code>rint_phenotype</code> <p>If true, apply Rank-based inverse normal transform.</p> <code>False</code> <code>outlier_sigma_threshold</code> <p>The multiple of standard deviations or sigmas after which we consider the phenotypic value an outlier.</p> <code>None</code> <code>transform_order</code> <p>A tuple specifying the order in which to apply the transformations. By default, the order is standardize, covariate_adjust, rint, and outlier_removal.</p> <code>('standardize', 'covariate_adjust', 'rint', 'outlier_removal')</code> <p>Returns:</p> Type Description <p>The transformed phenotype vector and a boolean mask indicating the samples that were not removed.</p> Source code in <code>magenpy/stats/transforms/phenotype.py</code> <pre><code>def chained_transform(sample_table,\n                      adjust_covariates=False,\n                      standardize_phenotype=False,\n                      rint_phenotype=False,\n                      outlier_sigma_threshold=None,\n                      transform_order=('standardize', 'covariate_adjust', 'rint', 'outlier_removal')):\n    \"\"\"\n    Apply a chain of transformations to the phenotype vector.\n    :param sample_table: An instance of SampleTable that contains phenotype information and other\n    covariates about the samples in the dataset.\n    :param adjust_covariates: If true, regress out the covariates from the phenotype. By default, we regress out all\n    the covariates present in the SampleTable.\n    :param standardize_phenotype: If true, standardize the phenotype.\n    :param rint_phenotype: If true, apply Rank-based inverse normal transform.\n    :param outlier_sigma_threshold: The multiple of standard deviations or sigmas after\n    which we consider the phenotypic value an outlier.\n    :param transform_order: A tuple specifying the order in which to apply the transformations. By default,\n    the order is standardize, covariate_adjust, rint, and outlier_removal.\n\n    :return: The transformed phenotype vector and a boolean mask indicating the samples that were not removed.\n    \"\"\"\n\n    phenotype = sample_table.phenotype\n    mask = np.ones_like(phenotype, dtype=bool)\n\n    if sample_table.phenotype_likelihood != 'binomial':\n        for transform in transform_order:\n\n            if transform == 'standardize':\n                # Standardize the phenotype:\n                if standardize_phenotype:\n                    phenotype = standardize(phenotype)\n\n            elif transform == 'covariate_adjust':\n                # Adjust the phenotype for a set of covariates:\n                if adjust_covariates:\n                    phenotype = adjust_for_covariates(phenotype, sample_table.get_covariates_matrix()[mask, :])\n\n            elif transform == 'rint':\n                # Apply Rank-based inverse normal transform (RINT) to the phenotype:\n                if rint_phenotype:\n                    phenotype = rint(phenotype)\n\n            elif transform == 'outlier_removal':\n                # Remove outlier samples whose phenotypes are more than `threshold` standard deviations from the mean:\n                if outlier_sigma_threshold is not None:\n                    # Find outliers:\n                    mask = ~detect_outliers(phenotype, outlier_sigma_threshold)\n                    # Filter phenotype vector to exclude outliers:\n                    phenotype = phenotype[mask]\n\n    return phenotype, mask\n</code></pre>"},{"location":"api/stats/transforms/phenotype/#magenpy.stats.transforms.phenotype.detect_outliers","title":"<code>detect_outliers(phenotype, sigma_threshold=3, nan_policy='omit')</code>","text":"<p>Detect samples with outlier phenotype values. This function takes a vector of quantitative phenotypes, computes the z-score for every individual, and returns a boolean vector indicating whether individual i has phenotype value within the specified standard deviations <code>sigma_threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype</code> <p>A numpy vector of continuous or quantitative phenotypes.</p> required <code>sigma_threshold</code> <p>The multiple of standard deviations or sigmas after which we consider the phenotypic value an outlier. Default is 3.</p> <code>3</code> <code>nan_policy</code> <p>The policy to use when encountering NaN values in the phenotype vector. By default, we compute the z-scores ignoring NaN values.</p> <code>'omit'</code> <p>Returns:</p> Type Description <p>A boolean array indicating whether the phenotype value is an outlier (i.e. True indicates outlier).</p> Source code in <code>magenpy/stats/transforms/phenotype.py</code> <pre><code>def detect_outliers(phenotype, sigma_threshold=3, nan_policy='omit'):\n    \"\"\"\n    Detect samples with outlier phenotype values.\n    This function takes a vector of quantitative phenotypes,\n    computes the z-score for every individual, and returns a\n    boolean vector indicating whether individual i has phenotype value\n    within the specified standard deviations `sigma_threshold`.\n    :param phenotype: A numpy vector of continuous or quantitative phenotypes.\n    :param sigma_threshold: The multiple of standard deviations or sigmas after\n    which we consider the phenotypic value an outlier. Default is 3.\n    :param nan_policy: The policy to use when encountering NaN values in the phenotype vector.\n    By default, we compute the z-scores ignoring NaN values.\n\n    :return: A boolean array indicating whether the phenotype value is an outlier (i.e.\n    True indicates outlier).\n    \"\"\"\n    from scipy.stats import zscore\n    return np.abs(zscore(phenotype, nan_policy=nan_policy)) &gt; sigma_threshold\n</code></pre>"},{"location":"api/stats/transforms/phenotype/#magenpy.stats.transforms.phenotype.rint","title":"<code>rint(phenotype, offset=3.0 / 8)</code>","text":"<p>Apply Rank-based inverse normal transform on the phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>phenotype</code> <p>A vector of continuous or quantitative phenotypes.</p> required <code>offset</code> <p>The offset to use in the INT transformation (Blom's offset by default).</p> <code>3.0 / 8</code> <p>Returns:</p> Type Description <p>The RINT-transformed phenotype.</p> Source code in <code>magenpy/stats/transforms/phenotype.py</code> <pre><code>def rint(phenotype, offset=3./8):\n    \"\"\"\n    Apply Rank-based inverse normal transform on the phenotype.\n    :param phenotype: A vector of continuous or quantitative phenotypes.\n    :param offset: The offset to use in the INT transformation (Blom's offset by default).\n\n    :return: The RINT-transformed phenotype.\n    \"\"\"\n\n    from scipy.stats import rankdata, norm\n\n    ranked_pheno = rankdata(phenotype, method=\"average\")\n    return norm.ppf((ranked_pheno - offset) / (len(ranked_pheno) - 2 * offset + 1))\n</code></pre>"},{"location":"api/stats/transforms/phenotype/#magenpy.stats.transforms.phenotype.standardize","title":"<code>standardize(phenotype)</code>","text":"<p>Standardize the phenotype vector to have mean zero and unit variance</p> <p>Parameters:</p> Name Type Description Default <code>phenotype</code> <p>A numpy vector of continuous or quantitative phenotypes.</p> required <p>Returns:</p> Type Description <p>The standardized phenotype array.</p> Source code in <code>magenpy/stats/transforms/phenotype.py</code> <pre><code>def standardize(phenotype):\n    \"\"\"\n    Standardize the phenotype vector to have mean zero and unit variance\n    :param phenotype: A numpy vector of continuous or quantitative phenotypes.\n\n    :return: The standardized phenotype array.\n    \"\"\"\n    return (phenotype - phenotype.mean()) / phenotype.std()\n</code></pre>"},{"location":"api/stats/variant/utils/","title":"Utils","text":""},{"location":"api/stats/variant/utils/#magenpy.stats.variant.utils.compute_allele_frequency_plink2","title":"<code>compute_allele_frequency_plink2(genotype_matrix, temp_dir='temp')</code>","text":"<p>Compute the allele frequency for each SNP in the genotype matrix using PLINK2.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A GenotypeMatrix object.</p> required <code>temp_dir</code> <p>The temporary directory where to store intermediate files.</p> <code>'temp'</code> <p>Returns:</p> Type Description <p>A numpy array of allele frequencies.</p> Source code in <code>magenpy/stats/variant/utils.py</code> <pre><code>def compute_allele_frequency_plink2(genotype_matrix, temp_dir='temp'):\n    \"\"\"\n    Compute the allele frequency for each SNP in the genotype matrix using PLINK2.\n    :param genotype_matrix: A GenotypeMatrix object.\n    :param temp_dir: The temporary directory where to store intermediate files.\n\n    :return: A numpy array of allele frequencies.\n\n    \"\"\"\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink2 = plink2Executor()\n\n    s_table = genotype_matrix.sample_table\n\n    keep_file = osp.join(temp_dir, 'samples.keep')\n    keep_table = s_table.get_individual_table()\n    keep_table.to_csv(keep_file, index=False, header=False, sep=\"\\t\")\n\n    snp_keepfile = osp.join(temp_dir, f\"variants.keep\")\n    pd.DataFrame({'SNP': genotype_matrix.snps}).to_csv(\n        snp_keepfile, index=False, header=False\n    )\n\n    plink_output = osp.join(temp_dir, \"variants\")\n\n    cmd = [\n        f\"--bfile {genotype_matrix.bed_file}\",\n        f\"--keep {keep_file}\",\n        f\"--extract {snp_keepfile}\",\n        f\"--freq\",\n        f\"--out {plink_output}\",\n    ]\n\n    plink2.execute(cmd)\n\n    freq_df = pd.read_csv(plink_output + \".afreq\", sep=r'\\s+')\n    freq_df.rename(columns={'ID': 'SNP',\n                            'REF': 'A2',\n                            'ALT': 'A1', 'ALT1': 'A1',\n                            'ALT_FREQS': 'MAF', 'ALT1_FREQ': 'MAF'}, inplace=True)\n    merged_df = merge_snp_tables(genotype_matrix.get_snp_table(['SNP', 'A1', 'A2']), freq_df)\n\n    if len(merged_df) != genotype_matrix.n_snps:\n        raise ValueError(\"Length of allele frequency table does not match number of SNPs.\")\n\n    return merged_df['MAF'].values\n</code></pre>"},{"location":"api/stats/variant/utils/#magenpy.stats.variant.utils.compute_sample_size_per_snp_plink2","title":"<code>compute_sample_size_per_snp_plink2(genotype_matrix, temp_dir='temp')</code>","text":"<p>Compute the sample size per SNP in the genotype matrix using PLINK2.</p> <p>Parameters:</p> Name Type Description Default <code>genotype_matrix</code> <p>A GenotypeMatrix object.</p> required <code>temp_dir</code> <p>The temporary directory where to store intermediate files.</p> <code>'temp'</code> <p>Returns:</p> Type Description <p>A numpy array of sample sizes per SNP.</p> Source code in <code>magenpy/stats/variant/utils.py</code> <pre><code>def compute_sample_size_per_snp_plink2(genotype_matrix, temp_dir='temp'):\n    \"\"\"\n    Compute the sample size per SNP in the genotype matrix using PLINK2.\n    :param genotype_matrix: A GenotypeMatrix object.\n    :param temp_dir: The temporary directory where to store intermediate files.\n\n    :return: A numpy array of sample sizes per SNP.\n    \"\"\"\n\n    assert isinstance(genotype_matrix, plinkBEDGenotypeMatrix)\n\n    plink2 = plink2Executor()\n\n    s_table = genotype_matrix.sample_table\n\n    keep_file = osp.join(temp_dir, 'samples.keep')\n    keep_table = s_table.get_individual_table()\n    keep_table.to_csv(keep_file, index=False, header=False, sep=\"\\t\")\n\n    snp_keepfile = osp.join(temp_dir, f\"variants.keep\")\n    pd.DataFrame({'SNP': genotype_matrix.snps}).to_csv(\n        snp_keepfile, index=False, header=False\n    )\n\n    plink_output = osp.join(temp_dir, \"variants\")\n\n    cmd = [\n        f\"--bfile {genotype_matrix.bed_file}\",\n        f\"--keep {keep_file}\",\n        f\"--extract {snp_keepfile}\",\n        f\"--missing variant-only\",\n        f\"--out {plink_output}\",\n    ]\n\n    plink2.execute(cmd)\n\n    miss_df = pd.read_csv(plink_output + \".vmiss\", sep=r'\\s+')\n    miss_df = pd.DataFrame({'ID': genotype_matrix.snps}).merge(miss_df)\n\n    if len(miss_df) != genotype_matrix.n_snps:\n        raise ValueError(\"Length of missingness table does not match number of SNPs.\")\n\n    return (miss_df['OBS_CT'] - miss_df['MISSING_CT']).values\n</code></pre>"},{"location":"api/utils/compute_utils/","title":"Compute utils","text":""},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.detect_header_keywords","title":"<code>detect_header_keywords(fname, keywords)</code>","text":"<p>Detect if the first line of a file contains any of the provided keywords. This is used to check for whether headers are present in a file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <p>The fpath to the file</p> required <code>keywords</code> <p>A string or list of strings representing keywords to search for.</p> required <p>Returns:</p> Type Description <p>True if any of the keywords are found, False otherwise.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def detect_header_keywords(fname, keywords):\n    \"\"\"\n    Detect if the first line of a file contains any of the provided keywords.\n    This is used to check for whether headers are present in a file.\n\n    :param fname: The fpath to the file\n    :param keywords: A string or list of strings representing keywords to search for.\n\n    :return: True if any of the keywords are found, False otherwise.\n    \"\"\"\n\n    if isinstance(keywords, str):\n        keywords = [keywords]\n\n    with open(fname, 'r') as f:\n        line = f.readline().strip()\n\n    return any([kw in line for kw in keywords])\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.generate_overlapping_windows","title":"<code>generate_overlapping_windows(seq, window_size, step_size, min_window_size=1)</code>","text":"<p>Generate overlapping windows of a fixed size over a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <p>A numpy array of sorted values</p> required <code>window_size</code> <p>The size of each window.</p> required <code>step_size</code> <p>The step size between each window. If step_size &lt; window_size, windows will overlap.</p> required <code>min_window_size</code> <p>The minimum size of a window. Windows smaller than this size will be discarded.</p> <code>1</code> <p>Returns:</p> Type Description <p>A numpy array of start and end indices of each window.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def generate_overlapping_windows(seq, window_size, step_size, min_window_size=1):\n    \"\"\"\n    Generate overlapping windows of a fixed size over a sequence.\n\n    :param seq: A numpy array of sorted values\n    :param window_size: The size of each window.\n    :param step_size: The step size between each window. If step_size &lt; window_size, windows will overlap.\n    :param min_window_size: The minimum size of a window. Windows smaller than this size will be discarded.\n\n    :return: A numpy array of start and end indices of each window.\n    \"\"\"\n\n    # Calculate the start of each window\n    starts = np.arange(seq[0], seq[-1] - window_size, step_size)\n\n    # Find the indices where each window starts and ends\n    start_indices = np.searchsorted(seq, starts)\n    end_indices = np.searchsorted(seq, starts + window_size, side='right')\n\n    # Ensure that the last window is not truncated and split it if necessary:\n    if end_indices[-1] &lt; seq.shape[0]:\n        end_indices[-1] = seq.shape[0]\n\n    # Combine start and end indices:\n    block_iter = np.column_stack((start_indices, end_indices))\n    # Filter to keep only valid blocks:\n    block_iter = block_iter[block_iter[:, 0] + min_window_size &lt;= block_iter[:, 1], :]\n\n    return block_iter\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.generate_slice_dictionary","title":"<code>generate_slice_dictionary(vec)</code>","text":"<p>This utility function takes a sorted vector (e.g. numpy array), identifies the unique elements and generates a dictionary of slices delineating the start and end positions of each element in the vector.</p> <p>Parameters:</p> Name Type Description Default <code>vec</code> <p>A numpy array</p> required <p>Returns:</p> Type Description <p>A dictionary of slices</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def generate_slice_dictionary(vec):\n    \"\"\"\n    This utility function takes a sorted vector (e.g. numpy array),\n    identifies the unique elements and generates a dictionary of slices\n    delineating the start and end positions of each element in the vector.\n\n    :param vec: A numpy array\n    :return: A dictionary of slices\n    \"\"\"\n\n    vals, idx = np.unique(vec, return_index=True)\n    idx_sort = np.argsort(idx)\n\n    vals = vals[idx_sort]\n    idx = idx[idx_sort]\n\n    d = {}\n\n    for i in range(len(idx)):\n        try:\n            d[vals[i]] = slice(idx[i], idx[i + 1])\n        except IndexError:\n            d[vals[i]] = slice(idx[i], len(vec))\n\n    return d\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.intersect_arrays","title":"<code>intersect_arrays(arr1, arr2, return_index=False)</code>","text":"<p>This utility function takes two arrays and returns the shared elements (intersection) between them. If return_index is set to True, it returns the index of shared elements in the first array.</p> <p>Parameters:</p> Name Type Description Default <code>arr1</code> <p>The first array</p> required <code>arr2</code> <p>The second array</p> required <code>return_index</code> <p>Return the index of shared elements in the first array</p> <code>False</code> <p>Returns:</p> Type Description <p>A numpy array of shared elements or their indices</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def intersect_arrays(arr1, arr2, return_index=False):\n    \"\"\"\n    This utility function takes two arrays and returns the shared\n    elements (intersection) between them. If return_index is set to True,\n    it returns the index of shared elements in the first array.\n\n    :param arr1: The first array\n    :param arr2: The second array\n    :param return_index: Return the index of shared elements in the first array\n\n    :return: A numpy array of shared elements or their indices\n    \"\"\"\n\n    # NOTE: For best and consistent results, we cast all data types to `str`\n    # for now. May need a smarter solution in the future.\n    common_elements = pd.DataFrame({'ID': arr1}, dtype=str).reset_index().merge(\n        pd.DataFrame({'ID': arr2}, dtype=str)\n    )\n\n    if return_index:\n        return common_elements['index'].values\n    else:\n        return common_elements['ID'].values\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.intersect_multiple_arrays","title":"<code>intersect_multiple_arrays(arrays)</code>","text":"<p>This utility function takes multiple arrays and returns the shared elements (intersection) between them. This is done with the help of the <code>pd.Index.intersection</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <p>A list of numpy arrays (NOTE: Cannot contain missing values!).</p> required <p>Returns:</p> Type Description <p>A numpy array of shared elements.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def intersect_multiple_arrays(arrays):\n    \"\"\"\n    This utility function takes multiple arrays and returns the shared\n    elements (intersection) between them. This is done with the help of\n    the `pd.Index.intersection` method.\n\n    :param arrays: A list of numpy arrays (NOTE: Cannot contain missing values!).\n    :return: A numpy array of shared elements.\n    \"\"\"\n    common = pd.Index(arrays[0])\n    for arr in arrays[1:]:\n        common = common.intersection(arr)\n\n    return common.values\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.is_numeric","title":"<code>is_numeric(obj)</code>","text":"<p>Check if a python object is numeric. This function handles numpy arrays and scalars.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>A python object</p> required <p>Returns:</p> Type Description <p>True if the object is numeric, False otherwise.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def is_numeric(obj):\n    \"\"\"\n    Check if a python object is numeric. This function handles\n    numpy arrays and scalars.\n    :param obj: A python object\n    :return: True if the object is numeric, False otherwise.\n    \"\"\"\n    if isinstance(obj, np.ndarray):\n        return np.issubdtype(obj.dtype, np.number)\n    else:\n        return np.issubdtype(type(obj), np.number)\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.iterable","title":"<code>iterable(arg)</code>","text":"<p>Check if an object is iterable, but not a string.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <p>A python object.</p> required <p>Returns:</p> Type Description <p>True if the object is iterable, False otherwise.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def iterable(arg):\n    \"\"\"\n    Check if an object is iterable, but not a string.\n    :param arg: A python object.\n    :return: True if the object is iterable, False otherwise.\n    \"\"\"\n\n    import collections.abc\n\n    return (\n        isinstance(arg, collections.abc.Iterable)\n        and not isinstance(arg, str)\n    )\n</code></pre>"},{"location":"api/utils/compute_utils/#magenpy.utils.compute_utils.split_block_boundaries","title":"<code>split_block_boundaries(block_iter, max_block_size, mask=None)</code>","text":"<p>A utility function that takes a list of block boundaries and splits them into smaller blocks of a fixed size to satisfy the constraint that no block should exceed a certain size.</p> <p>Parameters:</p> Name Type Description Default <code>block_iter</code> <p>A list of block boundaries (start from 0 and end at the last element).</p> required <code>max_block_size</code> <p>The maximum size of a block.</p> required <code>mask</code> <p>A boolean mask to determine which elements are active and will be used in the block processing.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of block boundaries that satisfy the constraint.</p> Source code in <code>magenpy/utils/compute_utils.py</code> <pre><code>def split_block_boundaries(block_iter, max_block_size, mask=None):\n    \"\"\"\n    A utility function that takes a list of block boundaries and splits them\n    into smaller blocks of a fixed size to satisfy the constraint that no block\n    should exceed a certain size.\n\n    :param block_iter: A list of block boundaries (start from 0 and end at the last element).\n    :param max_block_size: The maximum size of a block.\n    :param mask: A boolean mask to determine which elements are active and will be used in\n    the block processing.\n\n    :return: A list of block boundaries that satisfy the constraint.\n    \"\"\"\n\n    new_block_iter = [0]\n\n    for bidx in range(len(block_iter) - 1):\n\n        # Get the block boundaries:\n        start = block_iter[bidx]\n        end = block_iter[bidx + 1]\n\n        # Determine the current block size:\n        if mask is not None:\n            original_block_size = mask[start:end].sum()\n        else:\n            original_block_size = end - start\n\n        if original_block_size &gt; max_block_size:\n            # Reverting back to define the blocks in terms of the original indices:\n            original_block_size = end - start\n            # Determine the number of blocks:\n            n_blocks = int(np.ceil(original_block_size / max_block_size))\n\n            new_block_iter += list(new_block_iter[-1] + np.linspace(0,\n                                                                    original_block_size,\n                                                                    n_blocks + 1, dtype=int)[1:])\n\n        else:\n            new_block_iter.append((end - start) + new_block_iter[-1])\n\n    return new_block_iter\n</code></pre>"},{"location":"api/utils/data_utils/","title":"Data utils","text":""},{"location":"api/utils/data_utils/#magenpy.utils.data_utils.lrld_path","title":"<code>lrld_path()</code>","text":"<p>The boundaries of Long Range LD (LRLD) regions derived from here:</p> <pre><code>https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n</code></pre> <p>Which is based on the work of</p> <p>Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\" Nature protocols 5.9 (2010): 1564-1573.</p> <p>Returns:</p> Type Description <p>The path of the attached BED file containing long-range linkage disequilibrium (LD) regions in the human genome. The coordinates are in hg19/GRCh37.</p> Source code in <code>magenpy/utils/data_utils.py</code> <pre><code>def lrld_path():\n    \"\"\"\n    The boundaries of Long Range LD (LRLD) regions derived from here:\n\n        https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)\n\n    Which is based on the work of\n\n    &gt; Anderson, Carl A., et al. \"Data quality control in genetic case-control association studies.\"\n    Nature protocols 5.9 (2010): 1564-1573.\n\n    :return: The path of the attached BED file containing long-range linkage disequilibrium\n    (LD) regions in the human genome. The coordinates are in hg19/GRCh37.\n    \"\"\"\n    return osp.join(osp.dirname(osp.dirname(__file__)), 'data/lrld_hg19_GRCh37.txt')\n</code></pre>"},{"location":"api/utils/data_utils/#magenpy.utils.data_utils.tgp_eur_data_path","title":"<code>tgp_eur_data_path()</code>","text":"<p>Returns:</p> Type Description <p>The path of the attached 1000G genotype data for European samples (N=378) and a subset of chromosome 22 (p=15938)</p> Source code in <code>magenpy/utils/data_utils.py</code> <pre><code>def tgp_eur_data_path():\n    \"\"\"\n    :return: The path of the attached 1000G genotype data for\n    European samples (N=378) and a subset of chromosome 22 (p=15938)\n    \"\"\"\n    return osp.join(osp.dirname(osp.dirname(__file__)), 'data/1000G_eur_chr22')\n</code></pre>"},{"location":"api/utils/data_utils/#magenpy.utils.data_utils.ukb_height_sumstats_path","title":"<code>ukb_height_sumstats_path()</code>","text":"<p>Returns:</p> Type Description <p>The path of the attached GWAS summary statistics file for standing height. The file contains summary statistics for HapMap3 variants on CHR22 and is a snapshot of the summary statistics published on the fastGWA database: https://yanglab.westlake.edu.cn/data/fastgwa_data/UKB/50.v1.1.fastGWA.gz</p> Source code in <code>magenpy/utils/data_utils.py</code> <pre><code>def ukb_height_sumstats_path():\n    \"\"\"\n    :return: The path of the attached GWAS summary statistics file\n    for standing height. The file contains summary statistics for\n    HapMap3 variants on CHR22 and is a snapshot of the summary statistics\n    published on the fastGWA database:\n    https://yanglab.westlake.edu.cn/data/fastgwa_data/UKB/50.v1.1.fastGWA.gz\n    \"\"\"\n    return osp.join(osp.dirname(osp.dirname(__file__)), 'data/ukb_height_chr22.fastGWA.gz')\n</code></pre>"},{"location":"api/utils/executors/","title":"Executors","text":""},{"location":"api/utils/executors/#magenpy.utils.executors.plink1Executor","title":"<code>plink1Executor</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for interfacing with the <code>plink1.9</code> command line tool.</p> Source code in <code>magenpy/utils/executors.py</code> <pre><code>class plink1Executor(object):\n    \"\"\"\n    A wrapper class for interfacing with the `plink1.9` command line tool.\n    \"\"\"\n\n    def __init__(self, threads=None, verbose=True):\n        \"\"\"\n        Initialize the plink1.9 executor\n        :param threads: The number of threads to use for computations. If None, the number of\n        threads will be set to 1.\n        :type threads: int or str\n        :param verbose: Whether to print the output of the command\n        :type verbose: bool\n        \"\"\"\n\n        self.threads = threads or 1\n\n        self.plink1_path = get_option('plink1.9_path')\n\n        if not is_cmd_tool(self.plink1_path):\n            raise Exception(f\"Did not find the executable for plink1.9 at: {self.plink1_path}\")\n\n        self.verbose = verbose\n\n    def execute(self, cmd):\n        \"\"\"\n        Execute a plink command\n        :param cmd: The flags to pass to plink. For example, ['--bfile', 'file', '--out', 'output']\n        :type cmd: list of strings\n        :raises: CalledProcessError if the command fails\n        \"\"\"\n\n        cmd = [self.plink1_path] + cmd + [f'--threads {self.threads}']\n\n        from subprocess import CalledProcessError\n\n        try:\n            run_shell_script(\" \".join(cmd))\n        except CalledProcessError as e:\n            if self.verbose:\n                print(\"Invocation of plink returned the following error message:\")\n                print(e.stderr.decode())\n            raise e\n</code></pre>"},{"location":"api/utils/executors/#magenpy.utils.executors.plink1Executor.__init__","title":"<code>__init__(threads=None, verbose=True)</code>","text":"<p>Initialize the plink1.9 executor</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int | str</code> <p>The number of threads to use for computations. If None, the number of threads will be set to 1.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the output of the command</p> <code>True</code> Source code in <code>magenpy/utils/executors.py</code> <pre><code>def __init__(self, threads=None, verbose=True):\n    \"\"\"\n    Initialize the plink1.9 executor\n    :param threads: The number of threads to use for computations. If None, the number of\n    threads will be set to 1.\n    :type threads: int or str\n    :param verbose: Whether to print the output of the command\n    :type verbose: bool\n    \"\"\"\n\n    self.threads = threads or 1\n\n    self.plink1_path = get_option('plink1.9_path')\n\n    if not is_cmd_tool(self.plink1_path):\n        raise Exception(f\"Did not find the executable for plink1.9 at: {self.plink1_path}\")\n\n    self.verbose = verbose\n</code></pre>"},{"location":"api/utils/executors/#magenpy.utils.executors.plink1Executor.execute","title":"<code>execute(cmd)</code>","text":"<p>Execute a plink command</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>list of strings</code> <p>The flags to pass to plink. For example, ['--bfile', 'file', '--out', 'output']</p> required Source code in <code>magenpy/utils/executors.py</code> <pre><code>def execute(self, cmd):\n    \"\"\"\n    Execute a plink command\n    :param cmd: The flags to pass to plink. For example, ['--bfile', 'file', '--out', 'output']\n    :type cmd: list of strings\n    :raises: CalledProcessError if the command fails\n    \"\"\"\n\n    cmd = [self.plink1_path] + cmd + [f'--threads {self.threads}']\n\n    from subprocess import CalledProcessError\n\n    try:\n        run_shell_script(\" \".join(cmd))\n    except CalledProcessError as e:\n        if self.verbose:\n            print(\"Invocation of plink returned the following error message:\")\n            print(e.stderr.decode())\n        raise e\n</code></pre>"},{"location":"api/utils/executors/#magenpy.utils.executors.plink2Executor","title":"<code>plink2Executor</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for interfacing with the <code>plink2</code> command line tool.</p> Source code in <code>magenpy/utils/executors.py</code> <pre><code>class plink2Executor(object):\n    \"\"\"\n    A wrapper class for interfacing with the `plink2` command line tool.\n    \"\"\"\n\n    def __init__(self, threads=None, verbose=True):\n        \"\"\"\n        Initialize the plink2 executor\n        :param threads: The number of threads to use for computations. If None, the number of\n        threads will be set to 1.\n        :type threads: int or str\n        :param verbose: Whether to print the output of the command\n        :type verbose: bool\n        \"\"\"\n\n        self.threads = threads or 1\n\n        self.plink2_path = get_option('plink2_path')\n\n        if not is_cmd_tool(self.plink2_path):\n            raise Exception(f\"Did not find the executable for plink2 at: {self.plink2_path}\")\n\n        self.verbose = verbose\n\n    def execute(self, cmd):\n        \"\"\"\n        Execute a `plink2` command\n        :param cmd: The flags to pass to plink2. For example, ['--bfile', 'file', '--out', 'output']\n        :type cmd: list of strings\n        :raises CalledProcessError: If the command returns a non-zero exit code\n        \"\"\"\n\n        cmd = [self.plink2_path] + cmd + [f'--threads {self.threads}']\n\n        from subprocess import CalledProcessError\n\n        try:\n            run_shell_script(\" \".join(cmd))\n        except CalledProcessError as e:\n\n            if self.verbose:\n                print(\"Invocation of plink2 returned the following error message:\")\n                print(e.stderr.decode())\n\n            raise e\n</code></pre>"},{"location":"api/utils/executors/#magenpy.utils.executors.plink2Executor.__init__","title":"<code>__init__(threads=None, verbose=True)</code>","text":"<p>Initialize the plink2 executor</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int | str</code> <p>The number of threads to use for computations. If None, the number of threads will be set to 1.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the output of the command</p> <code>True</code> Source code in <code>magenpy/utils/executors.py</code> <pre><code>def __init__(self, threads=None, verbose=True):\n    \"\"\"\n    Initialize the plink2 executor\n    :param threads: The number of threads to use for computations. If None, the number of\n    threads will be set to 1.\n    :type threads: int or str\n    :param verbose: Whether to print the output of the command\n    :type verbose: bool\n    \"\"\"\n\n    self.threads = threads or 1\n\n    self.plink2_path = get_option('plink2_path')\n\n    if not is_cmd_tool(self.plink2_path):\n        raise Exception(f\"Did not find the executable for plink2 at: {self.plink2_path}\")\n\n    self.verbose = verbose\n</code></pre>"},{"location":"api/utils/executors/#magenpy.utils.executors.plink2Executor.execute","title":"<code>execute(cmd)</code>","text":"<p>Execute a <code>plink2</code> command</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>list of strings</code> <p>The flags to pass to plink2. For example, ['--bfile', 'file', '--out', 'output']</p> required <p>Raises:</p> Type Description <code>CalledProcessError</code> <p>If the command returns a non-zero exit code</p> Source code in <code>magenpy/utils/executors.py</code> <pre><code>def execute(self, cmd):\n    \"\"\"\n    Execute a `plink2` command\n    :param cmd: The flags to pass to plink2. For example, ['--bfile', 'file', '--out', 'output']\n    :type cmd: list of strings\n    :raises CalledProcessError: If the command returns a non-zero exit code\n    \"\"\"\n\n    cmd = [self.plink2_path] + cmd + [f'--threads {self.threads}']\n\n    from subprocess import CalledProcessError\n\n    try:\n        run_shell_script(\" \".join(cmd))\n    except CalledProcessError as e:\n\n        if self.verbose:\n            print(\"Invocation of plink2 returned the following error message:\")\n            print(e.stderr.decode())\n\n        raise e\n</code></pre>"},{"location":"api/utils/model_utils/","title":"Model utils","text":""},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.dequantize","title":"<code>dequantize(ints, float_dtype=np.float32)</code>","text":"<p>Dequantize integers to the specified floating point type. NOTE: Assumes original floats are in the range [-1, 1].</p> <p>Parameters:</p> Name Type Description Default <code>ints</code> <p>A numpy array of integers</p> required <code>float_dtype</code> <p>The floating point data type to dequantize the integers to.</p> <code>float32</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def dequantize(ints, float_dtype=np.float32):\n    \"\"\"\n    Dequantize integers to the specified floating point type.\n    NOTE: Assumes original floats are in the range [-1, 1].\n    :param ints: A numpy array of integers\n    :param float_dtype: The floating point data type to dequantize the integers to.\n    \"\"\"\n\n    # Infer the boundaries from the integer type\n    info = np.iinfo(ints.dtype)\n\n    dq = ints.astype(float_dtype)\n    dq /= info.max  # in-place multiplication\n\n    return dq\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.get_shared_distance_matrix","title":"<code>get_shared_distance_matrix(tree, tips=None)</code>","text":"<p>This function takes a Biopython tree and returns the shared distance matrix, i.e. for a pair of clades or populations, time to most recent common ancestor of the pair minus the time of the most recent common ancestor (MRCA).</p> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def get_shared_distance_matrix(tree, tips=None):\n    \"\"\"\n    This function takes a Biopython tree and returns the\n    shared distance matrix, i.e. for a pair of clades or populations,\n    time to most recent common ancestor of the pair minus the time of\n    the most recent common ancestor (MRCA).\n    \"\"\"\n\n    tips = tree.get_terminals() if tips is None else tips\n    n_tips = len(tips)  # Number of terminal species\n    sdist_matrix = np.zeros((n_tips, n_tips))  # Shared distance matrix\n\n    for i in range(n_tips):\n        for j in range(i, n_tips):\n            if i == j:\n                sdist_matrix[i, j] = tree.distance(tree.root, tips[i])\n            else:\n                mrca = tree.common_ancestor(tips[i], tips[j])\n                sdist_matrix[i, j] = sdist_matrix[j, i] = tree.distance(tree.root, mrca)\n\n    return sdist_matrix\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.identify_mismatched_snps","title":"<code>identify_mismatched_snps(gdl, chrom=None, n_iter=10, G=100, p_dentist_threshold=5e-08, p_gwas_threshold=0.01, rsq_threshold=0.95, max_removed_per_iter=0.005)</code>","text":"<p>This function implements a simple quality control procedures that checks that the GWAS summary statistics (Z-scores) are consistent with the LD reference panel. This is done using a simplified version of the framework outlined in the DENTIST paper:</p> <p>Improved analyses of GWAS summary statistics by reducing data heterogeneity and errors Chen et al. 2021</p> <p>Compared to DENTIST, the simplifications we make are:     -   For each SNP, we sample one neighboring SNP at a time and compute the T statistic         using that neighbor's information. The benefit of this is that we don't need to         invert any matrices, so it's a fast operation to run.     -   To arrive at a more robust estimate, we sample up to <code>k</code> neighbors and average         the T-statistic across those <code>k</code> neighbors.</p> <p>NOTE: May need to re-implement this to apply some of the constraints genome-wide rather than on a per-chromosome basis.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>A <code>GWADataLoader</code> object</p> required <code>chrom</code> <p>Perform checking only on chromosome <code>chrom</code></p> <code>None</code> <code>n_iter</code> <p>Number of iterations</p> <code>10</code> <code>G</code> <p>The number of neighboring SNPs to sample (default: 100)</p> <code>100</code> <code>p_dentist_threshold</code> <p>The Bonferroni-corrected P-value threshold (default: 5e-8)</p> <code>5e-08</code> <code>p_gwas_threshold</code> <p>The nominal GWAS P-value threshold for partitioning variants (default: 1e-2)</p> <code>0.01</code> <code>rsq_threshold</code> <p>The R^2 threshold to select neighbors (neighbor's squared correlation coefficient must be less than specified threshold).</p> <code>0.95</code> <code>max_removed_per_iter</code> <p>The maximum proportion of variants removed in each iteration</p> <code>0.005</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def identify_mismatched_snps(gdl,\n                             chrom=None,\n                             n_iter=10,\n                             G=100,\n                             p_dentist_threshold=5e-8,\n                             p_gwas_threshold=1e-2,\n                             rsq_threshold=.95,\n                             max_removed_per_iter=.005):\n    \"\"\"\n    This function implements a simple quality control procedures\n    that checks that the GWAS summary statistics (Z-scores)\n    are consistent with the LD reference panel. This is done\n    using a simplified version of the framework outlined in the DENTIST paper:\n\n    Improved analyses of GWAS summary statistics by reducing data heterogeneity and errors\n    Chen et al. 2021\n\n    Compared to DENTIST, the simplifications we make are:\n        -   For each SNP, we sample one neighboring SNP at a time and compute the T statistic\n            using that neighbor's information. The benefit of this is that we don't need to\n            invert any matrices, so it's a fast operation to run.\n        -   To arrive at a more robust estimate, we sample up to `k` neighbors and average\n            the T-statistic across those `k` neighbors.\n\n    NOTE: May need to re-implement this to apply some of the constraints genome-wide\n    rather than on a per-chromosome basis.\n\n    :param gdl: A `GWADataLoader` object\n    :param chrom: Perform checking only on chromosome `chrom`\n    :param n_iter: Number of iterations\n    :param G: The number of neighboring SNPs to sample (default: 100)\n    :param p_dentist_threshold: The Bonferroni-corrected P-value threshold (default: 5e-8)\n    :param p_gwas_threshold: The nominal GWAS P-value threshold for partitioning variants (default: 1e-2)\n    :param rsq_threshold: The R^2 threshold to select neighbors (neighbor's squared\n    correlation coefficient must be less than specified threshold).\n    :param max_removed_per_iter: The maximum proportion of variants removed in each iteration\n    \"\"\"\n\n    # Import required modules / functions:\n    from scipy import stats\n\n    # Data preparation:\n    if chrom is None:\n        chromosomes = gdl.chromosomes\n    else:\n        chromosomes = [chrom]\n\n    shapes = gdl.shapes\n    mismatched_dict = {c: np.repeat(False, gdl.shapes[c])\n                       for c in chromosomes}\n\n    p_gwas_above_thres = {c: gdl.sumstats_table[c].p_value &gt; p_gwas_threshold for c in chromosomes}\n    gwas_thres_size = {c: p.sum() for c, p in p_gwas_above_thres.items()}\n    converged = {c: False for c in chromosomes}\n\n    for j in tqdm(range(n_iter),\n                  total=n_iter,\n                  desc=\"Identifying mismatched SNPs...\"):\n\n        for chrom in chromosomes:\n\n            if converged[chrom]:\n                continue\n\n            ld_bounds = gdl.ld[chrom].get_masked_boundaries()\n            z = gdl.z_scores[chrom]  # Obtain the z-scores\n            t = np.zeros_like(z)\n\n            # Loop over the LD matrix:\n            for i, r in enumerate(gdl.ld[chrom]):\n\n                # If the number of neighbors is less than 10, skip...\n                if mismatched_dict[chrom][i] or len(r) &lt; 10:\n                    continue\n\n                start_idx = ld_bounds[0, i]\n                # Select neighbors randomly\n                # Note: We are excluding neighbors whose squared correlation coefficient\n                # is greater than pre-specified threshold:\n                p = (np.array(r)**2 &lt; rsq_threshold).astype(float)\n                p /= p.sum()\n\n                neighbor_idx = np.random.choice(len(r), p=p, size=G)\n                neighbor_r = np.array(r)[neighbor_idx]\n\n                # Predict the z-score of snp i, given the z-scores of its neighbors:\n                pred_z = neighbor_r*z[start_idx + neighbor_idx]\n\n                # Compute the Td statistic for each neighbor and average:\n                t[i] = ((z[i] - pred_z) ** 2 / (1. - neighbor_r**2)).mean()\n\n            # Compute the DENTIST p-value assuming a Chi-Square distribution with 1 dof.\n            dentist_pval = 1. - stats.chi2.cdf(t, 1)\n            # Use a Bonferroni correction to select mismatched SNPs:\n            mismatched_snps = dentist_pval &lt; p_dentist_threshold\n\n            if mismatched_snps.sum() &lt; 1:\n                # If no new mismatched SNPs are identified, stop iterating...\n                converged[chrom] = True\n            elif j == n_iter - 1:\n                # If this is the last iteration, take all identified SNPs\n                mismatched_dict[chrom] = (mismatched_dict[chrom] | mismatched_snps)\n            else:\n\n                # Otherwise, we will perform the iterative filtering procedure\n                # by splitting variants based on their GWAS p-values:\n\n                # (1) Group S1: SNPs to remove from P_GWAS &gt; threshold:\n                mismatch_above_thres = mismatched_snps &amp; p_gwas_above_thres[chrom]\n                n_mismatch_above_thres = mismatch_above_thres.sum()\n                prop_mismatch_above_thres = n_mismatch_above_thres / gwas_thres_size[chrom]\n\n                if n_mismatch_above_thres &lt; 1:\n                    # If no mismatches are detected above the threshold, filter\n                    # the mismatches below the threshold and continue...\n                    mismatched_dict[chrom] = (mismatched_dict[chrom] | mismatched_snps)\n                    continue\n\n                # Sort the DENTIST p-values by index:\n                sort_d_pval_idx = np.argsort(dentist_pval)\n\n                if prop_mismatch_above_thres &gt; max_removed_per_iter:\n                    idx_to_keep = sort_d_pval_idx[mismatch_above_thres][\n                                  int(gwas_thres_size[chrom]*max_removed_per_iter):]\n                    mismatch_above_thres[idx_to_keep] = False\n\n                # (2) Group S2: SNPs to remove from P_GWAS &lt; threshold\n\n                # Find mismatched variants below the threshold:\n                mismatch_below_thres = mismatched_snps &amp; (~p_gwas_above_thres[chrom])\n                n_mismatch_below_thres = mismatch_below_thres.sum()\n                prop_mismatch_below_thres = n_mismatch_below_thres / (shapes[chrom] - gwas_thres_size[chrom])\n\n                # For the mismatched variants below the threshold,\n                # we remove the same proportion as the variants above the threshold:\n                prop_keep_below_thres = min(max_removed_per_iter, prop_mismatch_above_thres)\n\n                if prop_mismatch_below_thres &gt; prop_keep_below_thres:\n                    idx_to_keep = sort_d_pval_idx[mismatch_below_thres][\n                                  int((shapes[chrom] - gwas_thres_size[chrom]) * prop_keep_below_thres):\n                                  ]\n                    mismatch_below_thres[idx_to_keep] = False\n\n                # Update the number of variants above the threshold:\n                gwas_thres_size[chrom] -= mismatch_above_thres.sum()\n\n                # Update the mismatched dictionary:\n                mismatched_dict[chrom] = (mismatched_dict[chrom] | mismatch_below_thres | mismatch_above_thres)\n\n    return mismatched_dict\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.map_variants_to_genomic_blocks","title":"<code>map_variants_to_genomic_blocks(variant_table, block_table, variant_pos_col='POS', block_boundary_cols=('block_start', 'block_end'), filter_unmatched=False)</code>","text":"<p>Merge a variant table with a genomic block table. This function assumes that the variant table includes a column with the positions of the variants <code>POS</code> and the block table contains the start and end positions of each block.</p> <p>!!! warning      This function assumes that the tables contains data for a single chromosome only.</p> <p>Parameters:</p> Name Type Description Default <code>variant_table</code> <p>A pandas dataframe with variant information</p> required <code>block_table</code> <p>A pandas dataframe with block information</p> required <code>variant_pos_col</code> <p>The name of the column in the variant table that contains the positions. By default, this is set to <code>POS</code>.</p> <code>'POS'</code> <code>block_boundary_cols</code> <p>A tuple of two strings specifying the column names in the block table that contain the start and end positions of the blocks. By default, this is set to <code>('block_start', 'block_end')</code>.</p> <code>('block_start', 'block_end')</code> <code>filter_unmatched</code> <p>If True, filter out variants that were not matched to a block.</p> <code>False</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def map_variants_to_genomic_blocks(variant_table,\n                                   block_table,\n                                   variant_pos_col='POS',\n                                   block_boundary_cols=('block_start', 'block_end'),\n                                   filter_unmatched=False):\n    \"\"\"\n    Merge a variant table with a genomic block table. This function assumes that the\n    variant table includes a column with the positions of the variants `POS` and the block table\n    contains the start and end positions of each block.\n\n     !!! warning\n         This function assumes that the tables contains data for a single chromosome only.\n\n    :param variant_table: A pandas dataframe with variant information\n    :param block_table: A pandas dataframe with block information\n    :param variant_pos_col: The name of the column in the variant table that contains the positions. By default,\n    this is set to `POS`.\n    :param block_boundary_cols: A tuple of two strings specifying the column names in the block table that contain\n    the start and end positions of the blocks. By default, this is set to `('block_start', 'block_end')`.\n    :param filter_unmatched: If True, filter out variants that were not matched to a block.\n    \"\"\"\n\n    # Sanity checks:\n    assert variant_pos_col in variant_table.columns\n    assert all([col in block_table.columns for col in block_boundary_cols])\n\n    # Sort the variant table by position:\n    variant_table.sort_values(variant_pos_col, inplace=True)\n\n    # Merge the two dataframes to assign each SNP to its corresponding block:\n    merged_df = pd.merge_asof(variant_table, block_table,\n                              left_on=variant_pos_col, right_on=block_boundary_cols[0],\n                              direction='backward')\n\n    if filter_unmatched:\n        # Filter merged_df to only include variants that were matched properly with a block:\n        merged_df = merged_df.loc[(merged_df[variant_pos_col] &gt;= merged_df[block_boundary_cols[0]]) &amp;\n                                  (merged_df[variant_pos_col] &lt; merged_df[block_boundary_cols[1]])]\n\n    return merged_df\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.match_chromosomes","title":"<code>match_chromosomes(chrom_1, chrom_2, check_patterns=('chr_', 'chr:', 'chr'), return_both=False)</code>","text":"<p>Given two lists of chromosome IDs, this function returns the chromosomes that are common to both lists. By default, the returned chromosomes follow the data type and order of the first list. If <code>return_both</code> is set to True, the function returns the common chromosomes in both lists.</p> <p>The function also accounts for common ways to encode chromosomes, such as chr18, chr_18, 18, etc.</p> <p>Parameters:</p> Name Type Description Default <code>chrom_1</code> <p>A list or numpy array of chromosome IDs</p> required <code>chrom_2</code> <p>A list or numpy array of chromosome IDs</p> required <code>check_patterns</code> <p>A list of patterns to check for and replace in the chromosome IDs</p> <code>('chr_', 'chr:', 'chr')</code> <code>return_both</code> <p>If True, return the common chromosomes in both lists</p> <code>False</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def match_chromosomes(chrom_1, chrom_2, check_patterns=('chr_', 'chr:', 'chr'), return_both=False):\n    \"\"\"\n    Given two lists of chromosome IDs, this function returns the\n    chromosomes that are common to both lists. By default, the returned chromosomes\n    follow the data type and order of the first list. If `return_both` is set to True,\n    the function returns the common chromosomes in both lists.\n\n    The function also accounts for common ways to encode chromosomes, such as\n    chr18, chr_18, 18, etc.\n\n    :param chrom_1: A list or numpy array of chromosome IDs\n    :param chrom_2: A list or numpy array of chromosome IDs\n    :param check_patterns: A list of patterns to check for and replace in the chromosome IDs\n    :param return_both: If True, return the common chromosomes in both lists\n    \"\"\"\n\n    chrom_1 = np.array(list(chrom_1))\n    chrom_2 = np.array(list(chrom_2))\n\n    # First, convert the data types to strings:\n    chr1_str = chrom_1.astype(str)\n    chr2_str = chrom_2.astype(str)\n\n    _, chr1_idx, chr2_idx = np.intersect1d(chr1_str, chr2_str, return_indices=True)\n\n    if len(chr1_idx) &lt; 1:\n        # Replace patterns\n        for pattern in check_patterns:\n            chr1_str = np.char.replace(chr1_str, pattern, '')\n            chr2_str = np.char.replace(chr2_str, pattern, '')\n\n        _, chr1_idx, chr2_idx = np.intersect1d(chr1_str, chr2_str, return_indices=True)\n\n    if len(chr1_idx) &lt; 1:\n        if return_both:\n            return [], []\n        else:\n            return []\n    else:\n        if return_both:\n            return chrom_1[chr1_idx], chrom_2[chr2_idx]\n        else:\n            return chrom_1[chr1_idx]\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.merge_snp_tables","title":"<code>merge_snp_tables(ref_table, alt_table, how='inner', on='auto', signed_statistics=('BETA', 'STD_BETA', 'Z'), drop_duplicates=True, correct_flips=True, return_ref_indices=False, return_alt_indices=False)</code>","text":"<p>This function takes a reference SNP table with at least 3 columns ('SNP', 'A1', <code>A2</code>) and matches it with an alternative table that also has these 3 columns defined. In the most recent implementation, we allow users to merge on any set of columns that they wish by specifying the <code>on</code> parameter. For example, instead of <code>SNP</code>, the user can join the SNP tables on <code>CHR</code> and <code>POS</code>, the chromosome number and base pair position of the SNP.</p> <p>The manner in which the join operation takes place depends on the <code>how</code> argument. Currently, the function supports <code>inner</code> and <code>left</code> joins.</p> <p>The function removes duplicates if <code>drop_duplicates</code> parameter is set to True</p> <p>If <code>correct_flips</code> is set to True, the function will correct summary statistics in the alternative table <code>alt_table</code> (e.g. BETA, MAF) based whether the A1 alleles agree between the two tables.</p> <p>Parameters:</p> Name Type Description Default <code>ref_table</code> <p>The reference table (pandas dataframe).</p> required <code>alt_table</code> <p>The alternative table (pandas dataframe)</p> required <code>how</code> <p><code>inner</code> or <code>left</code></p> <code>'inner'</code> <code>on</code> <p>Which columns to use as anchors when merging. By default, we automatically infer which columns to use, but the user can specify this directly. When <code>on='auto'</code>, we try to use <code>SNP</code> (i.e. rsID) if available. If not, we use <code>['CHR', 'POS']</code>. If neither are available, we raise a ValueError.</p> <code>'auto'</code> <code>signed_statistics</code> <p>The columns with signed statistics to flip if <code>correct_flips</code> is set to True.</p> <code>('BETA', 'STD_BETA', 'Z')</code> <code>drop_duplicates</code> <p>Drop duplicate SNPs</p> <code>True</code> <code>correct_flips</code> <p>Correct SNP summary statistics that depend on status of alternative allele</p> <code>True</code> <code>return_ref_indices</code> <p>Return the indices of the remaining entries in the reference table before merging.</p> <code>False</code> <code>return_alt_indices</code> <p>Return the indices of the remaining entries in the alternative table before merging.</p> <code>False</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def merge_snp_tables(ref_table,\n                     alt_table,\n                     how='inner',\n                     on='auto',\n                     signed_statistics=('BETA', 'STD_BETA', 'Z'),\n                     drop_duplicates=True,\n                     correct_flips=True,\n                     return_ref_indices=False,\n                     return_alt_indices=False):\n    \"\"\"\n    This function takes a reference SNP table with at least 3 columns ('SNP', 'A1', `A2`)\n    and matches it with an alternative table that also has these 3 columns defined. In the most recent\n    implementation, we allow users to merge on any set of columns that they wish by specifying the `on`\n    parameter. For example, instead of `SNP`, the user can join the SNP tables on `CHR` and `POS`, the\n    chromosome number and base pair position of the SNP.\n\n    The manner in which the join operation takes place depends on the `how` argument.\n    Currently, the function supports `inner` and `left` joins.\n\n    The function removes duplicates if `drop_duplicates` parameter is set to True\n\n    If `correct_flips` is set to True, the function will correct summary statistics in\n    the alternative table `alt_table` (e.g. BETA, MAF) based whether the A1 alleles agree between the two tables.\n\n    :param ref_table: The reference table (pandas dataframe).\n    :param alt_table: The alternative table (pandas dataframe)\n    :param how: `inner` or `left`\n    :param on: Which columns to use as anchors when merging. By default, we automatically infer which columns\n    to use, but the user can specify this directly. When `on='auto'`, we try to use `SNP` (i.e. rsID) if available.\n    If not, we use `['CHR', 'POS']`. If neither are available, we raise a ValueError.\n    :param signed_statistics: The columns with signed statistics to flip if `correct_flips` is set to True.\n    :param drop_duplicates: Drop duplicate SNPs\n    :param correct_flips: Correct SNP summary statistics that depend on status of alternative allele\n    :param return_ref_indices: Return the indices of the remaining entries in the reference table before merging.\n    :param return_alt_indices: Return the indices of the remaining entries in the alternative table before merging.\n    \"\"\"\n\n    # Sanity checking steps:\n    assert how in ('left', 'inner')\n    for tab in (ref_table, alt_table):\n        assert isinstance(tab, pd.DataFrame)\n        if not all([col in tab.columns for col in ('A1', 'A2')]):\n            raise ValueError(\"To merge SNP tables, we require that the columns `A1` and `A2` are present.\")\n\n    # Determine which columns to merge on:\n    if on == 'auto':\n        # Check that the `SNP` column is present in both tables:\n        if all(['SNP' in tab.columns for tab in (ref_table, alt_table)]):\n            on = ['SNP']\n        # Check that the `CHR`, `POS` columns are present in both tables:\n        elif all([col in tab.columns for col in ('CHR', 'POS') for tab in (ref_table, alt_table)]):\n            on = ['CHR', 'POS']\n        else:\n            raise ValueError(\"Cannot merge SNP tables without specifying which columns to merge on.\")\n    elif isinstance(on, str):\n        on = [on]\n\n    ref_include = on + ['A1', 'A2']\n\n    if return_ref_indices:\n        ref_table.reset_index(inplace=True, names='REF_IDX')\n        ref_include += ['REF_IDX']\n    if return_alt_indices:\n        alt_table.reset_index(inplace=True, names='ALT_IDX')\n\n    merged_table = ref_table[ref_include].merge(alt_table, how=how, on=on)\n\n    if drop_duplicates:\n        merged_table.drop_duplicates(inplace=True, subset=on)\n\n    if how == 'left':\n        merged_table['A1_y'] = merged_table['A1_y'].fillna(merged_table['A1_x'])\n        merged_table['A2_y'] = merged_table['A2_y'].fillna(merged_table['A2_x'])\n\n    # Assign A1 to be the one derived from the reference table:\n    merged_table['A1'] = merged_table['A1_x']\n    merged_table['A2'] = merged_table['A2_x']\n\n    # Detect cases where the correct allele is specified in both tables:\n    matching_allele = np.all(merged_table[['A1_x', 'A2_x']].values == merged_table[['A1_y', 'A2_y']].values, axis=1)\n\n    # Detect cases where the effect and reference alleles are flipped:\n    flip = np.all(merged_table[['A2_x', 'A1_x']].values == merged_table[['A1_y', 'A2_y']].values, axis=1)\n\n    if how == 'inner':\n        # Variants to keep:\n        if correct_flips:\n            keep_snps = matching_allele | flip\n        else:\n            keep_snps = matching_allele\n\n        # Keep only SNPs with matching alleles or SNPs with flipped alleles:\n        merged_table = merged_table.loc[keep_snps, ]\n        flip = flip[keep_snps]\n\n    if correct_flips:\n\n        flip = flip.astype(int)\n        num_flips = flip.sum()\n\n        if num_flips &gt; 0:\n\n            # If the user provided a single signed statistic as a string, convert to list first:\n            if isinstance(signed_statistics, str):\n                signed_statistics = [signed_statistics]\n\n            # Loop over the signed statistics and correct them:\n            for s_stat in signed_statistics:\n                if s_stat in merged_table:\n                    merged_table[s_stat] = (-2. * flip + 1.) * merged_table[s_stat]\n\n            # Correct MAF:\n            if 'MAF' in merged_table:\n                merged_table['MAF'] = np.abs(flip - merged_table['MAF'])\n\n    merged_table.drop(['A1_x', 'A1_y', 'A2_x', 'A2_y'], axis=1, inplace=True)\n\n    return merged_table\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.multinomial_rvs","title":"<code>multinomial_rvs(n, p)</code>","text":"<p>Copied from Warren Weckesser: https://stackoverflow.com/a/55830796</p> <p>Sample from the multinomial distribution with multiple p vectors.</p> <ul> <li>n must be a scalar.</li> <li>p must an n-dimensional numpy array, n &gt;= 1.  The last axis of p   holds the sequence of probabilities for a multinomial distribution.</li> </ul> <p>The return value has the same shape as p.</p> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def multinomial_rvs(n, p):\n    \"\"\"\n    Copied from Warren Weckesser:\n    https://stackoverflow.com/a/55830796\n\n    Sample from the multinomial distribution with multiple p vectors.\n\n    * n must be a scalar.\n    * p must an n-dimensional numpy array, n &gt;= 1.  The last axis of p\n      holds the sequence of probabilities for a multinomial distribution.\n\n    The return value has the same shape as p.\n    \"\"\"\n    count = np.full(p.shape[:-1], n)\n    out = np.zeros(p.shape, dtype=int)\n    ps = p.cumsum(axis=-1)\n    # Conditional probabilities\n    with np.errstate(divide='ignore', invalid='ignore'):\n        condp = p / ps\n    condp[np.isnan(condp)] = 0.0\n    for i in range(p.shape[-1]-1, 0, -1):\n        binsample = np.random.binomial(count, condp[..., i])\n        out[..., i] = binsample\n        count -= binsample\n    out[..., 0] = count\n    return out\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.quantize","title":"<code>quantize(floats, int_dtype=np.int8)</code>","text":"<p>Quantize floating point numbers to the specified integer type. NOTE: Assumes that the floats are in the range [-1, 1].</p> <p>Parameters:</p> Name Type Description Default <code>floats</code> <p>A numpy array of floats</p> required <code>int_dtype</code> <p>The integer type to quantize to.</p> <code>int8</code> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def quantize(floats, int_dtype=np.int8):\n    \"\"\"\n    Quantize floating point numbers to the specified integer type.\n    NOTE: Assumes that the floats are in the range [-1, 1].\n    :param floats: A numpy array of floats\n    :param int_dtype: The integer type to quantize to.\n    \"\"\"\n\n    # Infer the boundaries from the integer type\n    info = np.iinfo(int_dtype)\n\n    # NOTE: We add 1 to the info.min here to force the zero point to be exactly at 0.\n    # See discussions on Scale Quantization Mapping.\n\n    # Use as much in-place operations as possible\n    # (Currently, we copy the data twice)\n    scaled_floats = floats*info.max\n    np.round(scaled_floats, out=scaled_floats)\n    np.clip(scaled_floats, info.min + 1, info.max, out=scaled_floats)\n\n    return scaled_floats.astype(int_dtype)\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.sumstats_train_test_split","title":"<code>sumstats_train_test_split(gdl, prop_train=0.9, **kwargs)</code>","text":"<p>Perform a train-test split on the GWAS summary statistics data. This function implemented the PUMAS procedure described in</p> <p>Zhao, Z., Yi, Y., Song, J. et al. PUMAS: fine-tuning polygenic risk scores with GWAS summary statistics. Genome Biol 22, 257 (2021). https://doi.org/10.1186/s13059-021-02479-9</p> <p>Specifically, the function takes harmonized LD and summary statistics data (in the form of a <code>GWADataLoader</code> object) and samples the marginal beta values for the training set, conditional on the LD matrix and the proportion of training data specified by <code>prop_train</code>.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>A <code>GWADataLoader</code> object containing the harmonized GWAS summary statistics and LD data.</p> required <code>prop_train</code> <p>The proportion of training data to sample.</p> <code>0.9</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>multivariate_normal_conditional_sampling</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A dictionary with the sampled beta values for the training and test sets.</p> Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def sumstats_train_test_split(gdl, prop_train=0.9, **kwargs):\n    \"\"\"\n    Perform a train-test split on the GWAS summary statistics data.\n    This function implemented the PUMAS procedure described in\n\n    &gt; Zhao, Z., Yi, Y., Song, J. et al. PUMAS: fine-tuning polygenic risk scores with GWAS summary statistics.\n    Genome Biol 22, 257 (2021). https://doi.org/10.1186/s13059-021-02479-9\n\n    Specifically, the function takes harmonized LD and summary statistics data (in the form of a\n    `GWADataLoader` object) and samples the marginal beta values for the training set, conditional\n    on the LD matrix and the proportion of training data specified by `prop_train`.\n\n    :param gdl: A `GWADataLoader` object containing the harmonized GWAS summary statistics and LD data.\n    :param prop_train: The proportion of training data to sample.\n    :param kwargs: Additional keyword arguments to pass to the\n    `multivariate_normal_conditional_sampling` function.\n\n    :return: A dictionary with the sampled beta values for the training and test sets.\n    \"\"\"\n\n    # Sanity checks:\n    assert 0. &lt; prop_train &lt; 1., \"The proportion of training data must be between 0 and 1.\"\n    assert gdl.sumstats_table is not None, \"The GWADataLoader object must have summary statistics initialized.\"\n    assert gdl.ld is not None, \"The GWADataLoader object must have LD matrices initialized.\"\n\n    from ..stats.ld.utils import multivariate_normal_conditional_sampling\n\n    prop_test = 1. - prop_train\n\n    results = {}\n\n    for chrom in gdl.chromosomes:\n\n        assert gdl.ld[chrom].n_snps == gdl.sumstats_table[chrom].n_snps, (\n            \"The number of SNPs in the LD matrix and the summary statistics table must match. Invoke the \"\n            \"`harmonize_data` method on the GWADataLoader object to ensure that the data is harmonized.\"\n        )\n\n        n_per_snp = gdl.sumstats_table[chrom].n_per_snp\n        n = n_per_snp.max()  # Get the GWAS sample size\n\n        # The covariance scale is computed based on the proportion of training data:\n        cov_scale = 1. / (prop_train * n) - (1. / n)\n        # Use the standardized marginal beta as the mean:\n        mean = gdl.sumstats_table[chrom].standardized_marginal_beta\n\n        # Sample the training beta values:\n        sampled_train_beta = multivariate_normal_conditional_sampling(\n            gdl.ld[chrom],\n            mean=mean,\n            cov_scale=cov_scale,\n            **kwargs\n        )\n\n        # Calculate the test beta values:\n        sampled_test_beta = mean * (1. / prop_test) - sampled_train_beta * (prop_train / prop_test)\n\n        # Store the results:\n        results[chrom] = {\n            'train_beta': sampled_train_beta,\n            'train_n': n_per_snp * prop_train,\n            'test_beta': sampled_test_beta,\n            'test_n': n_per_snp * prop_test\n        }\n\n    return results\n</code></pre>"},{"location":"api/utils/model_utils/#magenpy.utils.model_utils.tree_to_rho","title":"<code>tree_to_rho(tree, min_corr)</code>","text":"<p>This function takes a Biopython tree and a minimum correlation parameter and returns the correlation matrix for the effect sizes across populations.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <p>a Biopython Phylo object</p> required <code>min_corr</code> <p>minimum correlation</p> required <p>Returns:</p> Type Description Source code in <code>magenpy/utils/model_utils.py</code> <pre><code>def tree_to_rho(tree, min_corr):\n    \"\"\"\n    This function takes a Biopython tree and a minimum correlation\n    parameter and returns the correlation matrix for the effect sizes\n    across populations.\n\n    :param tree: a Biopython Phylo object\n    :param min_corr: minimum correlation\n    :return:\n    \"\"\"\n\n    max_depth = max(tree.depths().values())\n    tree.root.branch_length = min_corr*max_depth / (1. - min_corr)\n    max_depth = max(tree.depths().values())\n\n    for c in tree.find_clades():\n        c.branch_length /= max_depth\n\n    return tree.root.branch_length + get_shared_distance_matrix(tree)\n</code></pre>"},{"location":"api/utils/system_utils/","title":"System utils","text":""},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler","title":"<code>PeakMemoryProfiler</code>","text":"<p>A context manager that monitors and tracks the peak memory usage of a process (and optionally its children) over a period of time. The memory usage can be reported in various units (bytes, MB, or GB).</p> <p>Example:</p> <pre><code>with PeakMemoryProfiler() as profiler:\n    # Code block to monitor memory usage\n    ...\n</code></pre> <p>Class Attributes:</p> <p>Attributes:</p> Name Type Description <code>pid</code> <p>The PID of the process being monitored. Defaults to the current process.</p> <code>interval</code> <p>Time interval (in seconds) between memory checks. Defaults to 0.1.</p> <code>include_children</code> <p>Whether memory usage from child processes is included. Defaults to True.</p> <code>unit</code> <p>The unit used to report memory usage (either 'bytes', 'MB', or 'GB'). Defaults to 'MB'.</p> <code>max_memory</code> <p>The peak memory usage observed during the monitoring period.</p> <code>monitoring_thread</code> <p>Thread used for monitoring memory usage.</p> <code>_stop_monitoring</code> <p>Event used to signal when to stop monitoring.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>class PeakMemoryProfiler:\n    \"\"\"\n    A context manager that monitors and tracks the peak memory usage of a process\n    (and optionally its children) over a period of time. The memory usage can be\n    reported in various units (bytes, MB, or GB).\n\n    Example:\n\n    ```\n    with PeakMemoryProfiler() as profiler:\n        # Code block to monitor memory usage\n        ...\n    ```\n\n    Class Attributes:\n    :ivar pid: The PID of the process being monitored. Defaults to the current process.\n    :ivar interval: Time interval (in seconds) between memory checks. Defaults to 0.1.\n    :ivar include_children: Whether memory usage from child processes is included. Defaults to True.\n    :ivar unit: The unit used to report memory usage (either 'bytes', 'MB', or 'GB'). Defaults to 'MB'.\n    :ivar max_memory: The peak memory usage observed during the monitoring period.\n    :ivar monitoring_thread: Thread used for monitoring memory usage.\n    :ivar _stop_monitoring: Event used to signal when to stop monitoring.\n    \"\"\"\n\n    def __init__(self, pid=None, interval=0.1, include_children=True, unit=\"MB\"):\n        \"\"\"\n        Initializes the PeakMemoryProfiler instance with the provided parameters.\n\n        :param pid: The PID of the process to monitor. Defaults to None (current process).\n        :param interval: The interval (in seconds) between memory checks. Defaults to 0.1.\n        :param include_children: Whether to include memory usage from child processes. Defaults to True.\n        :param unit: The unit in which to report memory usage. Options are 'bytes', 'MB', or 'GB'. Defaults to 'MB'.\n        \"\"\"\n        self.pid = pid or psutil.Process().pid  # Default to current process if no PID is provided\n        self.interval = interval\n        self.include_children = include_children\n        self.unit = unit\n        self.max_memory = 0\n        self.monitoring_thread = None\n        self._stop_monitoring = threading.Event()\n\n    def __enter__(self):\n        \"\"\"\n        Starts monitoring memory usage when entering the context block.\n\n        :return: Returns the instance of PeakMemoryProfiler, so that we can access peak memory later.\n        \"\"\"\n        self.process = psutil.Process(self.pid)\n        self.max_memory = 0\n        self._stop_monitoring.clear()  # Clear the stop flag to begin monitoring\n        self.monitoring_thread = threading.Thread(target=self._monitor_memory)\n        self.monitoring_thread.start()\n        return self  # Return the instance so that the caller can access max_memory\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"\n        Stops the memory monitoring when exiting the context block.\n\n        :param exc_type: The exception type if an exception was raised in the block.\n        :param exc_value: The exception instance if an exception was raised.\n        :param traceback: The traceback object if an exception was raised.\n        \"\"\"\n        self._stop_monitoring.set()  # Signal the thread to stop monitoring\n        self.monitoring_thread.join()  # Wait for the monitoring thread to finish\n\n    def get_curr_memory(self):\n        \"\"\"\n        Get the current memory usage of the monitored process and its children.\n\n        :return: The current memory usage in the specified unit (bytes, MB, or GB).\n        :rtype: float\n        \"\"\"\n\n        memory = self.process.memory_info().rss\n\n        if self.include_children:\n            # Include memory usage of child processes recursively\n            for child in self.process.children(recursive=True):\n                try:\n                    memory += child.memory_info().rss\n                except (psutil.NoSuchProcess, psutil.AccessDenied):\n                    continue\n\n        if self.unit == \"MB\":\n            return memory / (1024 ** 2)  # Convert to MB\n        elif self.unit == \"GB\":\n            return memory / (1024 ** 3)  # Convert to GB\n        else:\n            return memory  # Return in bytes if no conversion is requested\n\n    def _monitor_memory(self):\n        \"\"\"\n        Monitors the memory usage of the process and its children continuously\n        until the monitoring is stopped.\n\n        This method runs in a separate thread and updates the peak memory usage\n        as long as the monitoring flag is not set.\n        \"\"\"\n        while not self._stop_monitoring.is_set():\n            try:\n                curr_memory = self.get_curr_memory()\n\n                # Update max memory if a new peak is found\n                self.max_memory = max(self.max_memory, curr_memory)\n                time.sleep(self.interval)\n            except psutil.NoSuchProcess:\n                break  # Process no longer exists, stop monitoring\n\n    def get_peak_memory(self):\n        \"\"\"\n        Get the peak memory usage observed during the monitoring period.\n\n        :return: The peak memory usage in the specified unit (bytes, MB, or GB).\n        :rtype: float\n        \"\"\"\n        return self.max_memory\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler.__enter__","title":"<code>__enter__()</code>","text":"<p>Starts monitoring memory usage when entering the context block.</p> <p>Returns:</p> Type Description <p>Returns the instance of PeakMemoryProfiler, so that we can access peak memory later.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    Starts monitoring memory usage when entering the context block.\n\n    :return: Returns the instance of PeakMemoryProfiler, so that we can access peak memory later.\n    \"\"\"\n    self.process = psutil.Process(self.pid)\n    self.max_memory = 0\n    self._stop_monitoring.clear()  # Clear the stop flag to begin monitoring\n    self.monitoring_thread = threading.Thread(target=self._monitor_memory)\n    self.monitoring_thread.start()\n    return self  # Return the instance so that the caller can access max_memory\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Stops the memory monitoring when exiting the context block.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <p>The exception type if an exception was raised in the block.</p> required <code>exc_value</code> <p>The exception instance if an exception was raised.</p> required <code>traceback</code> <p>The traceback object if an exception was raised.</p> required Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"\n    Stops the memory monitoring when exiting the context block.\n\n    :param exc_type: The exception type if an exception was raised in the block.\n    :param exc_value: The exception instance if an exception was raised.\n    :param traceback: The traceback object if an exception was raised.\n    \"\"\"\n    self._stop_monitoring.set()  # Signal the thread to stop monitoring\n    self.monitoring_thread.join()  # Wait for the monitoring thread to finish\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler.__init__","title":"<code>__init__(pid=None, interval=0.1, include_children=True, unit='MB')</code>","text":"<p>Initializes the PeakMemoryProfiler instance with the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <p>The PID of the process to monitor. Defaults to None (current process).</p> <code>None</code> <code>interval</code> <p>The interval (in seconds) between memory checks. Defaults to 0.1.</p> <code>0.1</code> <code>include_children</code> <p>Whether to include memory usage from child processes. Defaults to True.</p> <code>True</code> <code>unit</code> <p>The unit in which to report memory usage. Options are 'bytes', 'MB', or 'GB'. Defaults to 'MB'.</p> <code>'MB'</code> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def __init__(self, pid=None, interval=0.1, include_children=True, unit=\"MB\"):\n    \"\"\"\n    Initializes the PeakMemoryProfiler instance with the provided parameters.\n\n    :param pid: The PID of the process to monitor. Defaults to None (current process).\n    :param interval: The interval (in seconds) between memory checks. Defaults to 0.1.\n    :param include_children: Whether to include memory usage from child processes. Defaults to True.\n    :param unit: The unit in which to report memory usage. Options are 'bytes', 'MB', or 'GB'. Defaults to 'MB'.\n    \"\"\"\n    self.pid = pid or psutil.Process().pid  # Default to current process if no PID is provided\n    self.interval = interval\n    self.include_children = include_children\n    self.unit = unit\n    self.max_memory = 0\n    self.monitoring_thread = None\n    self._stop_monitoring = threading.Event()\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler.get_curr_memory","title":"<code>get_curr_memory()</code>","text":"<p>Get the current memory usage of the monitored process and its children.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current memory usage in the specified unit (bytes, MB, or GB).</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def get_curr_memory(self):\n    \"\"\"\n    Get the current memory usage of the monitored process and its children.\n\n    :return: The current memory usage in the specified unit (bytes, MB, or GB).\n    :rtype: float\n    \"\"\"\n\n    memory = self.process.memory_info().rss\n\n    if self.include_children:\n        # Include memory usage of child processes recursively\n        for child in self.process.children(recursive=True):\n            try:\n                memory += child.memory_info().rss\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                continue\n\n    if self.unit == \"MB\":\n        return memory / (1024 ** 2)  # Convert to MB\n    elif self.unit == \"GB\":\n        return memory / (1024 ** 3)  # Convert to GB\n    else:\n        return memory  # Return in bytes if no conversion is requested\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.PeakMemoryProfiler.get_peak_memory","title":"<code>get_peak_memory()</code>","text":"<p>Get the peak memory usage observed during the monitoring period.</p> <p>Returns:</p> Type Description <code>float</code> <p>The peak memory usage in the specified unit (bytes, MB, or GB).</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def get_peak_memory(self):\n    \"\"\"\n    Get the peak memory usage observed during the monitoring period.\n\n    :return: The peak memory usage in the specified unit (bytes, MB, or GB).\n    :rtype: float\n    \"\"\"\n    return self.max_memory\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.available_cpu","title":"<code>available_cpu()</code>","text":"<p>Returns:</p> Type Description <p>The number of available cores on the system minus 1.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def available_cpu():\n    \"\"\"\n    :return: The number of available cores on the system minus 1.\n    \"\"\"\n    return psutil.cpu_count() - 1\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.delete_temp_files","title":"<code>delete_temp_files(prefix)</code>","text":"<p>Delete temporary files with the given <code>prefix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <p>A string with the prefix of the temporary files to delete.</p> required Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def delete_temp_files(prefix):\n    \"\"\"\n    Delete temporary files with the given `prefix`.\n    :param prefix: A string with the prefix of the temporary files to delete.\n    \"\"\"\n    for f in glob.glob(f\"{prefix}*\"):\n        try:\n            os.remove(f)\n        except (OSError, FileNotFoundError):\n            continue\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.get_filenames","title":"<code>get_filenames(path, extension=None)</code>","text":"<p>Obtain valid and full path names given the provided <code>path</code> or prefix and extensions.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A string with the path prefix or full path.</p> required <code>extension</code> <p>The extension for the class of files to search for.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of strings with the full paths of the files/folders.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def get_filenames(path, extension=None):\n    \"\"\"\n    Obtain valid and full path names given the provided `path` or prefix and extensions.\n\n    :param path: A string with the path prefix or full path.\n    :param extension: The extension for the class of files to search for.\n\n    :return: A list of strings with the full paths of the files/folders.\n    \"\"\"\n\n    if osp.isdir(path):\n        if extension:\n            if osp.isfile(osp.join(path, extension)):\n                return [path]\n            else:\n                return [f for f in glob.glob(osp.join(path, '*/'))\n                        if extension in f or osp.isfile(osp.join(f, extension))]\n        else:\n            return glob.glob(osp.join(path, '*'))\n    else:\n        if extension is None:\n            return glob.glob(path + '*')\n        elif extension in path:\n            return glob.glob(path)\n        elif osp.isfile(path + extension):\n            return [path + extension]\n        else:\n            return (\n                    glob.glob(osp.join(path, '*' + extension + '*')) +\n                    glob.glob(osp.join(path, extension + '*')) +\n                    glob.glob(path + '*' + extension + '*')\n            )\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.get_memory_usage","title":"<code>get_memory_usage()</code>","text":"<p>Returns:</p> Type Description <p>The current memory usage of the running process in Mega Bytes (MB)</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def get_memory_usage():\n    \"\"\"\n    :return: The current memory usage of the running process in Mega Bytes (MB)\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    mem_info = process.memory_info()\n    return mem_info.rss / (1024 ** 2)\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.glob_s3_path","title":"<code>glob_s3_path(path)</code>","text":"<p>Get the list of files/folders in the provided AWS S3 path. This works with wildcards.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A string with the S3 path to list files/folders from.</p> required <p>Returns:</p> Type Description <p>A list of strings with the full paths of the files/folders.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def glob_s3_path(path):\n    \"\"\"\n    Get the list of files/folders in the provided AWS S3 path. This works with wildcards.\n\n    :param path: A string with the S3 path to list files/folders from.\n    :return: A list of strings with the full paths of the files/folders.\n    \"\"\"\n\n    import s3fs\n    s3 = s3fs.S3FileSystem(anon=True)\n\n    return s3.glob(path)\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.is_cmd_tool","title":"<code>is_cmd_tool(name)</code>","text":"<p>Check whether <code>name</code> is on PATH and marked as executable. From: https://stackoverflow.com/a/34177358</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>A string with the name of the command-line tool.</p> required <p>Returns:</p> Type Description <p>True if the command-line tool is available, False otherwise.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def is_cmd_tool(name):\n    \"\"\"\n    Check whether `name` is on PATH and marked as executable.\n    From: https://stackoverflow.com/a/34177358\n    :param name: A string with the name of the command-line tool.\n    :return: True if the command-line tool is available, False otherwise.\n    \"\"\"\n    from shutil import which\n\n    return which(name) is not None\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.is_path_writable","title":"<code>is_path_writable(path)</code>","text":"<p>Check whether the user has write-access to the provided <code>path</code>. This function supports checking for nested directories (i.e., we iterate upwards until finding a parent directory that currently exists, and we check the write-access of that directory).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A string with the path to check.</p> required <p>Returns:</p> Type Description <p>True if the path is writable, False otherwise.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def is_path_writable(path):\n    \"\"\"\n    Check whether the user has write-access to the provided `path`.\n    This function supports checking for nested directories (i.e.,\n    we iterate upwards until finding a parent directory that currently\n    exists, and we check the write-access of that directory).\n\n    :param path: A string with the path to check.\n\n    :return: True if the path is writable, False otherwise.\n    \"\"\"\n\n    # Get the absolute path first:\n    path = osp.abspath(path)\n\n    while True:\n\n        if osp.exists(path):\n            return os.access(path, os.W_OK)\n        else:\n            path = osp.dirname(path)\n            if path == '/' or len(path) == 0:\n                return False\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.makedir","title":"<code>makedir(dirs)</code>","text":"<p>Create directories on the filesystem, recursively.</p> <p>Parameters:</p> Name Type Description Default <code>dirs</code> <p>A string or list of strings with the paths to create.</p> required Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def makedir(dirs):\n    \"\"\"\n    Create directories on the filesystem, recursively.\n    :param dirs: A string or list of strings with the paths to create.\n    :raises: OSError if it fails to create the directory structure.\n    \"\"\"\n\n    if isinstance(dirs, str):\n        dirs = [dirs]\n\n    for dir_l in dirs:\n\n        if len(dir_l) &lt; 1:\n            continue\n\n        try:\n            os.makedirs(dir_l)\n        except OSError as e:\n            import errno\n            if e.errno != errno.EEXIST:\n                raise\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.run_shell_script","title":"<code>run_shell_script(cmd)</code>","text":"<p>Run the shell script given the command prompt in <code>cmd</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <p>A string with the shell command to run.</p> required <p>Returns:</p> Type Description <p>The result of the shell command.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def run_shell_script(cmd):\n    \"\"\"\n    Run the shell script given the command prompt in `cmd`.\n    :param cmd: A string with the shell command to run.\n\n    :return: The result of the shell command.\n    :raises: subprocess.CalledProcessError if the shell command fails.\n    \"\"\"\n\n    result = subprocess.run(cmd, shell=True, capture_output=True, check=True)\n\n    if result.stderr:\n        raise subprocess.CalledProcessError(\n            returncode=result.returncode,\n            cmd=result.args,\n            stderr=result.stderr\n        )\n\n    return result\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.setup_logger","title":"<code>setup_logger(loggers=None, modules=None, log_file=None, log_format=None, log_level='WARNING', clear_file=False)</code>","text":"<p>Set up the logger with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>loggers</code> <p>A list of logger instances to apply the logger configurations to.</p> <code>None</code> <code>modules</code> <p>A list of modules to apply the logger configurations to. This allows for setting up different logger configs for different modules.</p> <code>None</code> <code>log_file</code> <p>A string with the path to the log file.</p> <code>None</code> <code>log_format</code> <p>A string with the format of the log messages.</p> <code>None</code> <code>log_level</code> <p>A string with the logging level.</p> <code>'WARNING'</code> <code>clear_file</code> <p>A boolean flag to clear the log file before writing.</p> <code>False</code> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def setup_logger(loggers=None,\n                 modules=None,\n                 log_file=None,\n                 log_format=None,\n                 log_level='WARNING',\n                 clear_file=False):\n    \"\"\"\n    Set up the logger with the provided configuration.\n\n    :param loggers: A list of logger instances to apply the logger configurations to.\n    :param modules: A list of modules to apply the logger configurations to. This allows\n    for setting up different logger configs for different modules.\n    :param log_file: A string with the path to the log file.\n    :param log_format: A string with the format of the log messages.\n    :param log_level: A string with the logging level.\n    :param clear_file: A boolean flag to clear the log file before writing.\n    \"\"\"\n\n    # ------------------ Sanity checks ------------------\n    assert log_level in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], \\\n        f\"Invalid log level: {log_level}\"\n\n    assert loggers is not None or modules is not None, \\\n        \"Either `loggers` or `modules` must be provided!\"\n\n    loggers = loggers or []\n\n    # ------------------ Set up the configurations ------------------\n\n    import logging\n\n    log_level = logging.getLevelName(log_level)\n    handlers = []\n\n    # Create a file handler:\n    if log_file is not None:\n        assert is_path_writable(log_file), f\"Cannot write to {log_file}!\"\n        file_handler = logging.FileHandler(log_file, mode='w' if clear_file else 'a')\n        file_handler.setLevel(logging.getLevelName(log_level))\n        handlers.append(file_handler)\n\n    # Create a console handler:\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.getLevelName(log_level))\n    handlers.append(console_handler)\n\n    # Set up the formatter:\n    log_format = log_format or '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    formatter = logging.Formatter(log_format)\n\n    # Add the formatter to the handlers and add the handlers to the logger:\n    for handler in handlers:\n        handler.setFormatter(formatter)\n\n        for lgr in loggers:\n            lgr.setLevel(log_level)\n            lgr.addHandler(handler)\n\n        if modules is not None:\n            for lgr_name, lgr in logging.root.manager.loggerDict.items():\n                if any([m in lgr_name for m in modules]) and isinstance(lgr, logging.Logger):\n                    lgr.setLevel(log_level)\n                    lgr.addHandler(handler)\n</code></pre>"},{"location":"api/utils/system_utils/#magenpy.utils.system_utils.valid_url","title":"<code>valid_url(path)</code>","text":"<p>Check whether the provided <code>path</code> is a valid URL.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A string with the URL to check.</p> required <p>Returns:</p> Type Description <p>True if the URL is valid, False otherwise.</p> Source code in <code>magenpy/utils/system_utils.py</code> <pre><code>def valid_url(path):\n    \"\"\"\n    Check whether the provided `path` is a valid URL.\n    :param path: A string with the URL to check.\n    :return: True if the URL is valid, False otherwise.\n    \"\"\"\n\n    import urllib.request\n\n    try:\n        with urllib.request.urlopen(path) as response:\n            v_url = response.getcode() == 200  # Check if the response status is OK (HTTP 200)\n        return v_url\n    except Exception:\n        return False\n</code></pre>"},{"location":"commandline/magenpy_ld/","title":"magenpy_ld","text":""},{"location":"commandline/magenpy_ld/#compute-linkage-disequilibrium-ld-matrices-magenpy_ld","title":"Compute Linkage-Disequilibrium (LD) matrices (<code>magenpy_ld</code>)","text":"<p>The <code>magenpy_ld</code> script is used to compute Linkage-Disequilibrium (LD) matrices, which record the  pairwise SNP-by-SNP correlations from a sample of genotype data stored in <code>plink</code>'s BED format. The script  offers an interface to compute LD matrices by simply specifying the path to the genotype files, the type of LD  estimator to use, the subset of variants or samples to keep, and the output directory.</p> <p>A full listing of the options available for the <code>magenpy_ld</code> script can be found by running the  following command in your terminal:</p> <pre><code>magenpy_ld -h\n</code></pre> <p>Which outputs the following help message:</p> <pre><code>        **********************************************                            \n         _ __ ___   __ _  __ _  ___ _ __  _ __  _   _ \n        | '_ ` _ \\ / _` |/ _` |/ _ \\ '_ \\| '_ \\| | | |\n        | | | | | | (_| | (_| |  __/ | | | |_) | |_| |\n        |_| |_| |_|\\__,_|\\__, |\\___|_| |_| .__/ \\__, |\n                         |___/           |_|    |___/\n        Modeling and Analysis of Genetics data in python\n        Version: 0.1.4 | Release date: June 2024\n        Author: Shadi Zabad, McGill University\n        **********************************************\n        &lt; Compute LD matrix and store in Zarr format &gt;\n\nusage: magenpy_ld [-h] [--estimator {shrinkage,block,windowed,sample}] --bfile BED_FILE [--keep KEEP_FILE] [--extract EXTRACT_FILE]\n                  [--backend {xarray,plink}] [--temp-dir TEMP_DIR] --output-dir OUTPUT_DIR [--min-maf MIN_MAF] [--min-mac MIN_MAC]\n                  [--genome-build GENOME_BUILD] [--metadata METADATA] [--storage-dtype {float64,float32,int16,int8}]\n                  [--compute-spectral-properties] [--compressor {lz4,zlib,zstd,gzip}] [--compression-level COMPRESSION_LEVEL]\n                  [--ld-window LD_WINDOW] [--ld-window-kb LD_WINDOW_KB] [--ld-window-cm LD_WINDOW_CM] [--ld-blocks LD_BLOCKS]\n                  [--genmap-Ne GENMAP_NE] [--genmap-sample-size GENMAP_SS] [--shrinkage-cutoff SHRINK_CUTOFF]\n\nCommandline arguments for LD matrix computation and storage\n\noptions:\n  -h, --help            show this help message and exit\n  --estimator {shrinkage,block,windowed,sample}\n                        The LD estimator (windowed, shrinkage, block, sample)\n  --bfile BED_FILE      The path to a plink BED file.\n  --keep KEEP_FILE      A plink-style keep file to select a subset of individuals to compute the LD matrices.\n  --extract EXTRACT_FILE\n                        A plink-style extract file to select a subset of SNPs to compute the LD matrix for.\n  --backend {xarray,plink}\n                        The backend software used to compute the Linkage-Disequilibrium between variants.\n  --temp-dir TEMP_DIR   The temporary directory where we store intermediate files.\n  --output-dir OUTPUT_DIR\n                        The output directory where the Zarr formatted LD matrices will be stored.\n  --min-maf MIN_MAF     The minimum minor allele frequency for variants included in the LD matrix.\n  --min-mac MIN_MAC     The minimum minor allele count for variants included in the LD matrix.\n  --genome-build GENOME_BUILD\n                        The genome build for the genotype data (recommend storing as metadata).\n  --metadata METADATA   A comma-separated string with metadata keys and values. This is used to store information about the genotype data\n                        from which the LD matrix was computed, such as the biobank/samples, cohort characteristics (e.g. ancestry), etc.\n                        Keys and values should be separated by =, such that inputs are in the form of:--metadata\n                        Biobank=UKB,Ancestry=EUR,Date=April2024\n  --storage-dtype {float64,float32,int16,int8}\n                        The data type for the entries of the LD matrix.\n  --compute-spectral-properties\n                        Compute and store the spectral properties of the LD matrix (e.g. eigenvalues, eigenvectors).\n  --compressor {lz4,zlib,zstd,gzip}\n                        The compressor name or compression algorithm to use for the LD matrix.\n  --compression-level COMPRESSION_LEVEL\n                        The compression level to use for the entries of the LD matrix (1-9).\n  --ld-window LD_WINDOW\n                        Maximum number of neighboring SNPs to consider when computing LD.\n  --ld-window-kb LD_WINDOW_KB\n                        Maximum distance (in kilobases) between pairs of variants when computing LD.\n  --ld-window-cm LD_WINDOW_CM\n                        Maximum distance (in centi Morgan) between pairs of variants when computing LD.\n  --ld-blocks LD_BLOCKS\n                        Path to the file with the LD block boundaries, in LDetect format (e.g. chr start stop, tab-separated)\n  --genmap-Ne GENMAP_NE\n                        The effective population size for the population from which the genetic map was derived.\n  --genmap-sample-size GENMAP_SS\n                        The sample size for the dataset used to infer the genetic map.\n  --shrinkage-cutoff SHRINK_CUTOFF\n                        The cutoff value below which we assume that the correlation between variants is zero.\n</code></pre>"},{"location":"commandline/magenpy_simulate/","title":"magenpy_simulate","text":""},{"location":"commandline/magenpy_simulate/#simulate-complex-traits-with-varying-genetic-architectures-magenpy_simulate","title":"Simulate complex traits with varying genetic architectures (<code>magenpy_simulate</code>)","text":"<p>The <code>magenpy_simulate</code> script is used to facilitate simulating complex traits with a variety of  genetic architectures, given a set of genotypes stored in <code>plink</code>'s BED file format. The script  takes as input the path to the genotype data, the type of trait to simulate, the parameters of  the genetic architecture (e.g. polygenicity, heritability, effect sizes), and the output directory  where the simulated phenotypes will be stored.</p> <p>A full listing of the options available for the <code>magenpy_simulate</code> script can be found by running the  following command in your terminal:</p> <pre><code>magenpy_simulate -h\n</code></pre> <p>Which outputs the following help message:</p> <pre><code>        ********************************************************                            \n             _ __ ___   __ _  __ _  ___ _ __  _ __  _   _ \n            | '_ ` _ \\ / _` |/ _` |/ _ \\ '_ \\| '_ \\| | | |\n            | | | | | | (_| | (_| |  __/ | | | |_) | |_| |\n            |_| |_| |_|\\__,_|\\__, |\\___|_| |_| .__/ \\__, |\n                             |___/           |_|    |___/\n            Modeling and Analysis of Genetics data in python\n            Version: 0.1.4 | Release date: June 2024\n            Author: Shadi Zabad, McGill University\n        ********************************************************\n        &lt; Simulate complex quantitative or case-control traits &gt;\n\nusage: magenpy_simulate [-h] --bfile BED_FILE [--keep KEEP_FILE] [--extract EXTRACT_FILE] [--backend {plink,xarray}] [--temp-dir TEMP_DIR]\n                        --output-file OUTPUT_FILE [--output-simulated-beta] [--min-maf MIN_MAF] [--min-mac MIN_MAC] --h2 H2\n                        [--mix-prop MIX_PROP] [--prop-causal PROP_CAUSAL] [--var-mult VAR_MULT]\n                        [--phenotype-likelihood {gaussian,binomial}] [--prevalence PREVALENCE] [--seed SEED]\n\nCommandline arguments for the complex trait simulator\n\noptions:\n  -h, --help            show this help message and exit\n  --bfile BED_FILE      The BED files containing the genotype data. You may use a wildcard here (e.g. \"data/chr_*.bed\")\n  --keep KEEP_FILE      A plink-style keep file to select a subset of individuals for simulation.\n  --extract EXTRACT_FILE\n                        A plink-style extract file to select a subset of SNPs for simulation.\n  --backend {plink,xarray}\n                        The backend software used for the computation.\n  --temp-dir TEMP_DIR   The temporary directory where we store intermediate files.\n  --output-file OUTPUT_FILE\n                        The path where the simulated phenotype will be stored (no extension needed).\n  --output-simulated-beta\n                        Output a table with the true simulated effect size for each variant.\n  --min-maf MIN_MAF     The minimum minor allele frequency for variants included in the simulation.\n  --min-mac MIN_MAC     The minimum minor allele count for variants included in the simulation.\n  --h2 H2               Trait heritability. Ranges between 0. and 1., inclusive.\n  --mix-prop MIX_PROP   Mixing proportions for the mixture density (comma separated). For example, for the spike-and-slab mixture density,\n                        with the proportion of causal variants set to 0.1, you can specify: \"--mix-prop 0.9,0.1 --var-mult 0,1\".\n  --prop-causal PROP_CAUSAL, -p PROP_CAUSAL\n                        The proportion of causal variants in the simulation. See --mix-prop for more complex architectures specification.\n  --var-mult VAR_MULT, -d VAR_MULT\n                        Multipliers on the variance for each mixture component.\n  --phenotype-likelihood {gaussian,binomial}\n                        The likelihood for the simulated trait: gaussian (e.g. quantitative) or binomial (e.g. case-control).\n  --prevalence PREVALENCE\n                        The prevalence of cases (or proportion of positives) for binary traits. Ranges between 0. and 1.\n  --seed SEED           The random seed to use for the random number generator.\n</code></pre>"},{"location":"commandline/overview/","title":"Overview","text":"<p>In addition to the python package interface, users may also opt to use some of <code>magenpy</code>'s functionalities  via commandline scripts. The commandline interface is limited at this point to mainly simulating complex traits  and computing LD matrices.</p> <p>When you install <code>magenpy</code> using <code>pip</code>, the commandline scripts are automatically installed on your system and  are available for use. The available scripts are:</p> <ul> <li> <p><code>magenpy_ld</code>: This script is used to compute LD matrices from genotype data in <code>plink</code> BED format.      The script provides a variety of options for the user to customize the LD computation process, including the      choice of LD estimator, storage and compression options, etc.</p> </li> <li> <p><code>magenpy_simulate</code>: This script is used to simulate complex traits with a variety of genetic      architectures. The script provides a variety of options for the user to customize the simulation process,      including the choice of genetic architecture, the proportion of causal variants, the effect sizes, etc.</p> </li> </ul>"}]}